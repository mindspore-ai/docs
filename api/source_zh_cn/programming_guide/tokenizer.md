# åˆ†è¯å™¨

<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [åˆ†è¯å™¨](#åˆ†è¯å™¨)
    - [æ¦‚è¿°](#æ¦‚è¿°)
    - [MindSporeåˆ†è¯å™¨](#mindsporeåˆ†è¯å™¨)

<!-- /TOC -->

<a href="https://gitee.com/mindspore/docs/blob/master/api/source_zh_cn/programming_guide/tokenizer.md" target="_blank"><img src="../_static/logo_source.png"></a>

## æ¦‚è¿°

åˆ†è¯å°±æ˜¯å°†è¿ç»­çš„å­—åºåˆ—æŒ‰ç…§ä¸€å®šçš„è§„èŒƒé‡æ–°ç»„åˆæˆè¯åºåˆ—çš„è¿‡ç¨‹ï¼Œåˆç†çš„è¿›è¡Œåˆ†è¯æœ‰åŠ©äºè¯­ä¹‰çš„ç†è§£ã€‚

MindSporeæä¾›äº†å¤šç§ç”¨é€”çš„åˆ†è¯å™¨ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·é«˜æ€§èƒ½åœ°å¤„ç†æ–‡æœ¬ï¼Œç”¨æˆ·å¯ä»¥æ„å»ºè‡ªå·±çš„å­—å…¸ï¼Œä½¿ç”¨é€‚å½“çš„æ ‡è®°å™¨å°†å¥å­æ‹†åˆ†ä¸ºä¸åŒçš„æ ‡è®°ï¼Œå¹¶é€šè¿‡æŸ¥æ‰¾æ“ä½œè·å–å­—å…¸ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚

MindSporeç›®å‰æä¾›çš„åˆ†è¯å™¨å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚æ­¤å¤–ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å®ç°è‡ªå®šä¹‰çš„åˆ†è¯å™¨ã€‚

| åˆ†è¯å™¨ | åˆ†è¯å™¨è¯´æ˜ |
| -- | -- |
| BasicTokenizer | æ ¹æ®æŒ‡å®šè§„åˆ™å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |
| BertTokenizer | ç”¨äºå¤„ç†Bertæ–‡æœ¬æ•°æ®çš„åˆ†è¯å™¨ã€‚ |
| JiebaTokenizer | åŸºäºå­—å…¸çš„ä¸­æ–‡å­—ç¬¦ä¸²åˆ†è¯å™¨ã€‚ |
| RegexTokenizer | æ ¹æ®æŒ‡å®šæ­£åˆ™è¡¨è¾¾å¼å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |
| SentencePieceTokenizer | åŸºäºSentencePieceå¼€æºå·¥å…·åŒ…è¿›è¡Œåˆ†è¯ã€‚ |
| UnicodeCharTokenizer | å°†æ ‡é‡æ–‡æœ¬æ•°æ®åˆ†è¯ä¸ºUnicodeå­—ç¬¦ã€‚ |
| UnicodeScriptTokenizer | æ ¹æ®Unicodeè¾¹ç•Œå¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |
| WhitespaceTokenizer | æ ¹æ®ç©ºæ ¼ç¬¦å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |
| WordpieceTokenizer | æ ¹æ®å•è¯é›†å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |

æ›´å¤šåˆ†è¯å™¨çš„è¯¦ç»†è¯´æ˜ï¼Œå¯ä»¥å‚è§[APIæ–‡æ¡£](https://www.mindspore.cn/api/zh-CN/master/api/python/mindspore/mindspore.dataset.text.html)ã€‚

## MindSporeåˆ†è¯å™¨

### BasicTokenizer

`BasicTokenizer`æ˜¯é€šè¿‡å¤§å°å†™æŠ˜å ã€ç¼–ç ç»Ÿä¸€ã€å»é™¤é‡éŸ³ç¬¦ï¼ŒæŒ‰ç…§æ­£åˆ™åŒ¹é…æ¨¡å¼æ¥åˆ†è¯çš„ã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

# æ„å»ºè¾“å…¥çš„æ•°æ®åˆ—è¡¨
input_list = ["Welcome to BeijingåŒ—äº¬æ¬¢è¿æ‚¨", "é•·é¢¨ç ´æµªæœƒæœ‰æ™‚ï¼Œç›´æ›é›²å¸†æ¿Ÿæ»„æµ·","ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»",
                "æ˜æœï¼ˆ1368â€”1644å¹´ï¼‰å’Œæ¸…æœï¼ˆ1644â€”1911å¹´ï¼‰ï¼Œæ˜¯ä¸­å›½å°å»ºç‹æœå²ä¸Šæœ€åä¸¤ä¸ªæœä»£",
                "æ˜ä»£ï¼ˆ1368-1644ï¼‰ã¨æ¸…ä»£ï¼ˆ1644-1911ï¼‰ã¯ã€ä¸­å›½ã®å°å»ºç‹æœã®æ­´å²ã«ãŠã‘ã‚‹æœ€å¾Œã®2ã¤ã®ç‹æœã§ã—ãŸ",
                "ëª…ë‚˜ë¼ (1368-1644)ì™€ ì²­ë‚˜ë¼ (1644-1911)ëŠ” ì¤‘êµ­ ë´‰ê±´ ì™•ì¡°ì˜ ì—­ì‚¬ì—ì„œ ë§ˆì§€ë§‰ ë‘ ì™•ì¡°ì˜€ë‹¤"]

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenize----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹å‰çš„æ•°æ®
for data in dataset.create_dict_iterator():
    print(text.to_str(data['text']))

#æ‰“å°åˆ†è¯åçš„æ•°æ®è¾“å‡º
print("------------------------after tokenize-----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹åçš„æ•°æ®
# BasicTokenizerä¸ºåˆ†è¯çš„å‡½æ•°
basic_tokenizer = text.BasicTokenizer()

dataset = dataset.map(operations=basic_tokenizer)

for i in dataset.create_dict_iterator(num_epochs=1):
    token = text.to_str(i['text'])
    print(token)
```

```
------------------------before tokenize----------------------------
Welcome to BeijingåŒ—äº¬æ¬¢è¿æ‚¨
é•·é¢¨ç ´æµªæœƒæœ‰æ™‚ï¼Œç›´æ›é›²å¸†æ¿Ÿæ»„æµ·
ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»
æ˜æœï¼ˆ1368â€”1644å¹´ï¼‰å’Œæ¸…æœï¼ˆ1644â€”1911å¹´ï¼‰ï¼Œæ˜¯ä¸­å›½å°å»ºç‹æœå²ä¸Šæœ€åä¸¤ä¸ªæœä»£
æ˜ä»£ï¼ˆ1368-1644ï¼‰ã¨æ¸…ä»£ï¼ˆ1644-1911ï¼‰ã¯ã€ä¸­å›½ã®å°å»ºç‹æœã®æ­´å²ã«ãŠã‘ã‚‹æœ€å¾Œã®2ã¤ã®ç‹æœã§ã—ãŸ
ëª…ë‚˜ë¼ (1368-1644)ì™€ ì²­ë‚˜ë¼ (1644-1911)ëŠ” ì¤‘êµ­ ë´‰ê±´ ì™•ì¡°ì˜ ì—­ì‚¬ì—ì„œ ë§ˆì§€ë§‰ ë‘ ì™•ì¡°ì˜€ë‹¤
------------------------after tokenize-----------------------------
['Welcome' 'to' 'Beijing' 'åŒ—' 'äº¬' 'æ¬¢' 'è¿' 'æ‚¨']
['é•·' 'é¢¨' 'ç ´' 'æµª' 'æœƒ' 'æœ‰' 'æ™‚' 'ï¼Œ' 'ç›´' 'æ›' 'é›²' 'å¸†' 'æ¿Ÿ' 'æ»„' 'æµ·']
['ğŸ˜€' 'å˜¿' 'å˜¿' 'ğŸ˜ƒ' 'å“ˆ' 'å“ˆ' 'ğŸ˜„' 'å¤§' 'ç¬‘' 'ğŸ˜' 'å˜»' 'å˜»']
['æ˜' 'æœ' 'ï¼ˆ' '1368' 'â€”' '1644' 'å¹´' 'ï¼‰' 'å’Œ' 'æ¸…' 'æœ' 'ï¼ˆ' '1644' 'â€”' '1911' 'å¹´' 'ï¼‰' 'ï¼Œ' 'æ˜¯' 'ä¸­' 'å›½' 'å°' 'å»º' 'ç‹' 'æœ' 'å²' 'ä¸Š' 'æœ€' 'å' 'ä¸¤' 'ä¸ª' 'æœ' 'ä»£']
['æ˜' 'ä»£' 'ï¼ˆ' '1368' '-' '1644' 'ï¼‰' 'ã¨' 'æ¸…' 'ä»£' 'ï¼ˆ' '1644' '-' '1911' 'ï¼‰' 'ã¯' 'ã€' 'ä¸­' 'å›½' 'ã®' 'å°' 'å»º' 'ç‹' 'æœ' 'ã®' 'æ­´' 'å²' 'ã«ãŠã‘ã‚‹' 'æœ€' 'å¾Œ' 'ã®2ã¤ã®' 'ç‹' 'æœ' 'ã§ã—ãŸ']
['ëª…ë‚˜ë¼' '(' '1368' '-' '1644' ')' 'ì™€' 'ì²­ë‚˜ë¼' '(' '1644' '-' '1911' ')' 'ëŠ”' 'ì¤‘êµ­' 'ë´‰ê±´' 'ì™•ì¡°ì˜' 'ì—­ì‚¬ì—ì„œ' 'ë§ˆì§€ë§‰' 'ë‘' 'ì™•ì¡°ì˜€ë‹¤']
```

### BertTokenizer

`BertTokenizer`æ˜¯é€šè¿‡è°ƒç”¨`BasicTokenizer`å’Œ`WordpieceTokenizer`æ¥è¿›è¡Œåˆ†è¯çš„ã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

# æ„å»ºè¾“å…¥çš„æ•°æ®åˆ—è¡¨
input_list = ["åºŠå‰æ˜æœˆå…‰", "ç–‘æ˜¯åœ°ä¸Šéœœ", "ä¸¾å¤´æœ›æ˜æœˆ", "ä½å¤´æ€æ•…ä¹¡", "I am making small mistakes during working hours",
                "ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»", "ç¹é«”å­—"]

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenize----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹å‰çš„æ•°æ®
for data in dataset.create_dict_iterator():
    print(text.to_str(data['text']))

# å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹çš„å•è¯ã€‚
vocab_list = [
  "åºŠ", "å‰", "æ˜", "æœˆ", "å…‰", "ç–‘", "æ˜¯", "åœ°", "ä¸Š", "éœœ", "ä¸¾", "å¤´", "æœ›", "ä½", "æ€", "æ•…", "ä¹¡",
  "ç¹", "é«”", "å­—", "å˜¿", "å“ˆ", "å¤§", "ç¬‘", "å˜»", "i", "am", "mak", "make", "small", "mistake",
  "##s", "during", "work", "##ing", "hour", "ğŸ˜€", "ğŸ˜ƒ", "ğŸ˜„", "ğŸ˜", "+", "/", "-", "=", "12",
  "28", "40", "16", " ", "I", "[CLS]", "[SEP]", "[UNK]", "[PAD]", "[MASK]", "[unused1]", "[unused10]"]

# ä»å•è¯åˆ—è¡¨ä¸­æ„å»ºä¸€ä¸ªvocabå¯¹è±¡
vocab = text.Vocab.from_list(vocab_list)

# è¾“å‡ºåˆ†è¯ä¹‹åçš„æ•°æ®
# BertTokenizerä¸ºåˆ†è¯çš„å‡½æ•°
tokenizer_op = text.BertTokenizer(vocab=vocab)

#æ‰“å°åˆ†è¯åçš„æ•°æ®è¾“å‡º
print("------------------------after tokenize-----------------------------")

dataset = dataset.map(operations=tokenizer_op)

for i in dataset.create_dict_iterator(num_epochs=1):
    token = text.to_str(i['text'])
    print(token)
```

```
------------------------before tokenize----------------------------
åºŠå‰æ˜æœˆå…‰
ç–‘æ˜¯åœ°ä¸Šéœœ
ä¸¾å¤´æœ›æ˜æœˆ
ä½å¤´æ€æ•…ä¹¡
I am making small mistakes during working hours
ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»
ç¹é«”å­—
------------------------after tokenize-----------------------------
['åºŠ' 'å‰' 'æ˜' 'æœˆ' 'å…‰']
['ç–‘' 'æ˜¯' 'åœ°' 'ä¸Š' 'éœœ']
['ä¸¾' 'å¤´' 'æœ›' 'æ˜' 'æœˆ']
['ä½' 'å¤´' 'æ€' 'æ•…' 'ä¹¡']
['i' 'am' 'mak' '##ing' 'small' 'mistake' '##s' 'during' 'work' '##ing' 'hour' '##s']
['ğŸ˜€' 'å˜¿' 'å˜¿' 'ğŸ˜ƒ' 'å“ˆ' 'å“ˆ' 'ğŸ˜„' 'å¤§' 'ç¬‘' 'ğŸ˜' 'å˜»' 'å˜»']
['ç¹' 'é«”' 'å­—']
```

### JiebaTokenizer

`JiebaTokenizer`æ˜¯åŸºäºjiebaçš„ä¸­æ–‡åˆ†è¯ã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

# æ„å»ºè¾“å…¥çš„æ•°æ®åˆ—è¡¨
input_list = ["åºŠå‰æ˜æœˆå…‰", "ç–‘æ˜¯åœ°ä¸Šéœœ", "ä¸¾å¤´æœ›æ˜æœˆ", "ä½å¤´æ€æ•…ä¹¡", "I am making small mistakes during working hours",
                "ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»", "ç¹é«”å­—"]

# å­—å…¸æ–‡ä»¶ç”±HMMSegmentç®—æ³•å’ŒMPSegmentç®—æ³•ä½¿ç”¨ï¼Œè¯¥å­—å…¸å¯åœ¨cppjiebaçš„å®˜æ–¹ç½‘ç«™ä¸Šè·å¾—ã€‚
HMM_FILE = "hmm_model.utf8"
MP_FILE = "jieba.dict.utf8"

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenize----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹å‰çš„æ•°æ®
for data in dataset.create_dict_iterator():
    print(text.to_str(data['text']))

tokenizer_op = text.JiebaTokenizer(HMM_FILE, MP_FILE)

#æ‰“å°åˆ†è¯åçš„æ•°æ®è¾“å‡º
print("------------------------after tokenize-----------------------------")

dataset = dataset.map(operations=jieba_op, input_columns=["text"], num_parallel_workers=1)

for i in dataset.create_dict_iterator(num_epochs=1):
    token = text.to_str(i['text'])
    print(token)
```

```
------------------------before tokenize----------------------------
ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§
------------------------after tokenize-----------------------------
['ä»Šå¤©å¤©æ°”' 'å¤ªå¥½äº†' 'æˆ‘ä»¬' 'ä¸€èµ·' 'å»' 'å¤–é¢' 'ç©å§']
```

### RegexTokenizer

`RegexTokenizer`æ˜¯é€šæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¨¡å¼æ¥è¿›è¡Œåˆ†è¯çš„ã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

# æ„å»ºè¾“å…¥çš„æ•°æ®åˆ—è¡¨
input_list = ["Welcome to Shenzhen!"]

# åŸå§‹å­—ç¬¦ä¸²å°†ç”±åŒ¹é…çš„å…ƒç´ åˆ†éš”ã€‚
delim_pattern = "\\s+"

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenize----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹å‰çš„æ•°æ®
for data in dataset.create_dict_iterator():
    print(text.to_str(data['text']))

tokenizer_op = text.RegexTokenizer(delim_pattern)

#æ‰“å°åˆ†è¯åçš„æ•°æ®è¾“å‡º
print("------------------------after tokenize-----------------------------")

dataset = dataset.map(operations=tokenizer_op)

for i in dataset.create_dict_iterator(num_epochs=1):
    token = text.to_str(i['text']).tolist()
    print(token)
```

```
------------------------before tokenize----------------------------
Welcome to Shenzhen!
------------------------after tokenize-----------------------------
['Welcome', 'to', 'Shenzhen!']
```

### SentencePieceTokenizer

`SentencePieceTokenizer`æ˜¯åŸºäºSentencePieceè¿™ä¸ªå¼€æºçš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

# æ„å»ºè¾“å…¥çš„æ•°æ®åˆ—è¡¨
input_list = ["I saw a girl with a telescope."]

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenize----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹å‰çš„æ•°æ®
for data in dataset.create_dict_iterator():
    print(text.to_str(data['text']))

# ä»æ–‡ä»¶æ•°æ®ä¸­æ„å»ºä¸€ä¸ªvocabå¯¹è±¡
vocab = text.SentencePieceVocab.from_file([VOCAB_FILE], 5000, 0.9995, SentencePieceModel.UNIGRAM, {})
tokenizer_op = text.SentencePieceTokenizer(vocab, out_type=SPieceTokenizerOutType.STRING)

#æ‰“å°åˆ†è¯åçš„æ•°æ®è¾“å‡º
print("------------------------after tokenize-----------------------------")

dataset = dataset.map(operations=tokenizer_op)

for i in dataset.create_dict_iterator(num_epochs=1):
    token = text.to_str(i['text'])
    print(token)
```

```
------------------------before tokenize----------------------------
I saw a girl with a telescope.
------------------------after tokenize-----------------------------
['â–I' 'â–sa' 'w' 'â–a' 'â–girl' 'â–with' 'â–a' 'â–te' 'les' 'co' 'pe' '.']
```

### UnicodeCharTokenizer

`UnicodeCharTokenizer`æ˜¯æ ¹æ®Unicodeå­—ç¬¦é›†æ¥åˆ†è¯çš„ã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

# æ„å»ºè¾“å…¥çš„æ•°æ®åˆ—è¡¨
input_list = ["Welcome to Beijing!", "åŒ—äº¬æ¬¢è¿æ‚¨ï¼", "æˆ‘å–œæ¬¢English!"]

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenize----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹å‰çš„æ•°æ®
for data in dataset.create_dict_iterator():
    print(text.to_str(data['text']))

tokenizer_op = text.UnicodeCharTokenizer()

#æ‰“å°åˆ†è¯åçš„æ•°æ®è¾“å‡º
print("------------------------after tokenize-----------------------------")

dataset = dataset.map(operations=tokenizer_op)

for i in dataset.create_dict_iterator(num_epochs=1):
    token = text.to_str(i['text']).tolist()
    print(token)
```

```
------------------------before tokenize----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenize-----------------------------
['W', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'B', 'e', 'i', 'j', 'i', 'n', 'g', '!']
['åŒ—', 'äº¬', 'æ¬¢', 'è¿', 'æ‚¨', 'ï¼']
['æˆ‘', 'å–œ', 'æ¬¢', 'E', 'n', 'g', 'l', 'i', 's', 'h', '!']
```

### UnicodeScriptTokenizer

`UnicodeScriptTokenizer`æ˜¯æ ¹æ®ä¸åŒçš„Unicodeçš„è¾¹ç•Œæ¥è¿›è¡Œåˆ†è¯çš„ã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

# æ„å»ºè¾“å…¥çš„æ•°æ®åˆ—è¡¨
input_list = ["Welcome to Beijing!", "åŒ—äº¬æ¬¢è¿æ‚¨ï¼", "æˆ‘å–œæ¬¢English!"]

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenize----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹å‰çš„æ•°æ®
for data in dataset.create_dict_iterator():
    print(text.to_str(data['text']))

tokenizer_op = text.UnicodeScriptTokenizer()

#æ‰“å°åˆ†è¯åçš„æ•°æ®è¾“å‡º
print("------------------------after tokenize-----------------------------")

dataset = dataset.map(operations=tokenizer_op)

for i in dataset.create_dict_iterator(num_epochs=1):
    token = text.to_str(i['text']).tolist()
    print(token)      
```

```
------------------------before tokenize----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenize-----------------------------
['Welcome', 'to', 'Beijing', '!']
['åŒ—äº¬æ¬¢è¿æ‚¨', 'ï¼']
['æˆ‘å–œæ¬¢', 'English', '!']
```

### WhitespaceTokenizer

`WhitespaceTokenizer`æ˜¯æ ¹æ®ç©ºæ ¼æ¥è¿›è¡Œåˆ†è¯çš„ã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

# æ„å»ºè¾“å…¥çš„æ•°æ®åˆ—è¡¨
input_list = ["Welcome to Beijing!", "åŒ—äº¬æ¬¢è¿æ‚¨ï¼", "æˆ‘å–œæ¬¢English!"]

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenize----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹å‰çš„æ•°æ®
for data in dataset.create_dict_iterator():
    print(text.to_str(data['text']))

tokenizer_op = text.WhitespaceTokenizer()

#æ‰“å°åˆ†è¯åçš„æ•°æ®è¾“å‡º
print("------------------------after tokenize-----------------------------")

dataset = dataset.map(operations=tokenizer_op)

for i in dataset.create_dict_iterator(num_epochs=1):
    token = text.to_str(i['text']).tolist()
    print(token)
```

```
>> Tokenize Result
------------------------before tokenize----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenize-----------------------------
['Welcome', 'to', 'Beijing!']
['åŒ—äº¬æ¬¢è¿æ‚¨ï¼']
['æˆ‘å–œæ¬¢English!']
```

### WordpieceTokenizer

`WordpieceTokenizer`æ˜¯åŸºäºå•è¯é›†æ¥åˆ’åˆ†çš„ï¼Œå•è¯é›†é‡Œæ²¡æœ‰çš„ï¼Œä½†æ˜¯æœ‰ç»„åˆçš„ä¹Ÿä¼šåˆ’åˆ†å‡ºæ¥ã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

# æ„å»ºè¾“å…¥çš„æ•°æ®åˆ—è¡¨
input_list = ["my", "favorite", "book", "is", "love", "during", "the", "cholera", "era", "what", "æˆ‘", "æœ€", "å–œ", "æ¬¢", "çš„", "ä¹¦", "æ˜¯", "éœ", "ä¹±", "æ—¶", "æœŸ", "çš„", "çˆ±", "æƒ…", "æ‚¨"]

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenize----------------------------")

# è¾“å‡ºåˆ†è¯ä¹‹å‰çš„æ•°æ®
for data in dataset.create_dict_iterator():
    print(text.to_str(data['text']))

#æ‰“å°åˆ†è¯åçš„æ•°æ®è¾“å‡º
print("------------------------after tokenize-----------------------------")

# ä»å•è¯åˆ—è¡¨ä¸­æ„å»ºä¸€ä¸ªvocabå¯¹è±¡
vocab = text.Vocab.from_list(vocab_list)

# è¾“å‡ºåˆ†è¯ä¹‹åçš„æ•°æ®
# BasicTokenizerä¸ºåˆ†è¯çš„å‡½æ•°
tokenizer_op = text.WordpieceTokenizer(vocab=vocab)

dataset = dataset.map(operations=tokenizer_op)

for i in dataset.create_dict_iterator(num_epochs=1):
    token = text.to_str(i['text'])
    print(token)
```

```
------------------------before tokenize----------------------------
my
favorite
book
is
love
during
the
cholera
era
what
æˆ‘
æœ€
å–œ
æ¬¢
çš„
ä¹¦
æ˜¯
éœ
ä¹±
æ—¶
æœŸ
çš„
çˆ±
æƒ…
æ‚¨
------------------------after tokenize-----------------------------
['my']
['favor' '##ite']
['book']
['is']
['love']
['dur' '##ing']
['the']
['cholera']
['era']
['[UNK]']
['æˆ‘']
['æœ€']
['å–œ']
['æ¬¢']
['çš„']
['ä¹¦']
['æ˜¯']
['éœ']
['ä¹±']
['æ—¶']
['æœŸ']
['çš„']
['çˆ±']
['æƒ…']
['[UNK]']
```
