# API Updates

Compared with the previous version, the added, deleted and supported platforms change information of `mindspore.nn` operators in MindSpore, is shown in the following table.

|API|Status|Description|Support Platform|Class
|:----|:----|:----|:----|:----
|[mindspore.nn.MicroBatchInterleaved](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.MicroBatchInterleaved.html#mindspore.nn.MicroBatchInterleaved)|New|Wrap the network with Batch Size.|r1.6: Ascend/GPU|Wrapper Functions
|[mindspore.nn.PipelineCell](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.PipelineCell.html#mindspore.nn.PipelineCell)|Changed|Wrap the network with Micro Batch.|r1.5: To Be Developed => r1.6: Ascend/GPU|Wrapper Functions
|[mindspore.nn.Unfold](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.Unfold.html#mindspore.nn.Unfold)|Changed|Extracts patches from images.|r1.5: Ascend => r1.6: Ascend/GPU|Utilities
|[mindspore.nn.GRU](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.GRU.html#mindspore.nn.GRU)|Changed|Stacked GRU (Gated Recurrent Unit) layers.|r1.5: Ascend/GPU => r1.6: Ascend/GPU/CPU|Recurrent Layers
|[mindspore.nn.RNNCell](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.RNNCell.html#mindspore.nn.RNNCell)|Changed|An Elman RNN cell with tanh or ReLU non-linearity.|r1.5: Ascend/GPU => r1.6: Ascend/GPU/CPU|Recurrent Layers
|[mindspore.nn.LSTMCell](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.LSTMCell.html#mindspore.nn.LSTMCell)|Changed|A LSTM (Long Short-Term Memory) cell.|r1.5: GPU/CPU => r1.6: Ascend/GPU/CPU|Recurrent Layers
|[mindspore.nn.RNN](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.RNN.html#mindspore.nn.RNN)|Changed|Stacked Elman RNN layers.|r1.5: Ascend/GPU => r1.6: Ascend/GPU/CPU|Recurrent Layers
|[mindspore.nn.GRUCell](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.GRUCell.html#mindspore.nn.GRUCell)|Changed|A GRU(Gated Recurrent Unit) cell.|r1.5: Ascend/GPU => r1.6: Ascend/GPU/CPU|Recurrent Layers
|[mindspore.nn.LSTM](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.LSTM.html#mindspore.nn.LSTM)|Changed|Stacked LSTM (Long Short-Term Memory) layers.|r1.5: Ascend/GPU => r1.6: Ascend/GPU/CPU|Recurrent Layers
|[mindspore.nn.Rprop](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.Rprop.html#mindspore.nn.Rprop)|New|Implements Resilient backpropagation.|r1.6: Ascend/GPU/CPU|Optimizer Functions
|[mindspore.nn.ASGD](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.ASGD.html#mindspore.nn.ASGD)|New|Implements Average Stochastic Gradient Descent.|r1.6: Ascend/GPU/CPU|Optimizer Functions
|[mindspore.nn.Lamb](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.Lamb.html#mindspore.nn.Lamb)|Changed|An optimizer that implements the Lamb(Layer-wise Adaptive Moments optimizer for Batching training) algorithm.|r1.5: Ascend/GPU/CPU => r1.6: Ascend/GPU|Optimizer Functions
|[mindspore.nn.LARS](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.LARS.html#mindspore.nn.LARS)|Changed|Implements the LARS algorithm with LARSUpdate Operator.|r1.5: Ascend/CPU => r1.6: Ascend|Optimizer Functions
|[mindspore.nn.FTRL](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.FTRL.html#mindspore.nn.FTRL)|Changed|Implements the FTRL algorithm with ApplyFtrl Operator.|r1.5: Ascend/GPU/CPU => r1.6: Ascend/GPU|Optimizer Functions
|[mindspore.nn.LazyAdam](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.LazyAdam.html#mindspore.nn.LazyAdam)|Changed|r1.5: This optimizer will apply a lazy adam algorithm when gradient is sparse. => r1.6: Updates gradients by the Adaptive Moment Estimation (Adam) algorithm.|r1.5: Ascend/GPU => r1.6: Ascend/GPU/CPU|Optimizer Functions
|[mindspore.nn.GlobalBatchNorm](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.GlobalBatchNorm.html#mindspore.nn.GlobalBatchNorm)|Changed|The GlobalBatchNorm interface is deprecated, please use the  mindspore.nn.SyncBatchNorm  instead.|r1.5: Ascend => r1.6: deprecated|Normalization Layers
|[mindspore.nn.CELU](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.CELU.html#mindspore.nn.CELU)|New|Continuously differentiable exponential linear units activation function.|r1.6: Ascend|Non-linear Activations
|[mindspore.nn.LogSigmoid](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.LogSigmoid.html#mindspore.nn.LogSigmoid)|Changed|Logsigmoid activation function.|r1.5: Ascend/GPU/CPU => r1.6: Ascend/GPU|Non-linear Activations
|[mindspore.nn.MatMul](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.MatMul.html#mindspore.nn.MatMul)|Changed|The nn.MatMul interface is deprecated, please use the  mindspore.ops.matmul  instead.|r1.5: Ascend/GPU/CPU => r1.6: deprecated|Math Functions
|mindspore.nn.MAELoss|Deleted|MAELoss creates a criterion to measure the average absolute error between  \(x\)  and  \(y\)  element-wise, where  \(x\)  is the input and  \(y\)  is the labels.|r1.5: Ascend/GPU/CPU|Loss Functions
|[mindspore.nn.Jvp](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.Jvp.html#mindspore.nn.Jvp)|Changed|Compute the jacobian-vector-product of the given fn.|r1.5: To Be Developed => r1.6: Ascend/GPU/CPU|Gradient
|[mindspore.nn.Vjp](https://www.mindspore.cn/api/docs/zh-CN/r1.6/api_python/nn/mindspore.nn.Vjp.html#mindspore.nn.Vjp)|Changed|Computes the dot product between a vector  v  and the Jacobian of the given fn at the point given by the inputs.|r1.5: To Be Developed => r1.6: Ascend/GPU/CPU|Gradient
|mindspore.nn.GraphKernel|Deleted|Base class for GraphKernel.|r1.5: Ascend/GPU/CPU|Cell