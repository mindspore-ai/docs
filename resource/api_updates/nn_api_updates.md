# API Updates

Compared with the previous version, the added, deleted and supported platforms change information of `mindspore.nn` operators in MindSpore, is shown in the following table.

|API|Status|Description|Support Platform|Class
|:----|:----|:----|:----|:----
[mindspore.nn.GraphKernel](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.GraphKernel.html#mindspore.nn.GraphKernel)|Deleted|Base class for GraphKernel cell that can be compiled into a fused kernel automatically when enable_graph_kernel in context is set to True.|Ascend/GPU/CPU|Cell
[mindspore.nn.GraphCell](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.GraphCell.html#mindspore.nn.GraphCell)|New|Base class for running the graph loaded from a MindIR.|r1.5: Ascend/GPU/CPU|Cell
[mindspore.nn.Jvp](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.Jvp.html#mindspore.nn.Jvp)|New|Compute the jacobian-vector-product of the given network.|r1.5: To Be Developed|Gradient
[mindspore.nn.Vjp](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.Vjp.html#mindspore.nn.Vjp)|New|Computes the dot product between a vector  v  and the Jacobian of the given network at the point given by the inputs.|r1.5: To Be Developed|Gradient
[mindspore.nn.SoftMarginLoss](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.SoftMarginLoss.html#mindspore.nn.SoftMarginLoss)|New|A loss class for two-class classification problems.|r1.5: Ascend|Loss Functions
[mindspore.nn.DiGamma](https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.DiGamma.html#mindspore.nn.DiGamma)|Deleted|Calculates Digamma using Lanczos’ approximation referring to “A Precision Approximation of the Gamma Function”.|Ascend/GPU|Math Functions
[mindspore.nn.IGamma](https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.IGamma.html#mindspore.nn.IGamma)|Deleted|Calculates lower regularized incomplete Gamma function.|Ascend/GPU|Math Functions
[mindspore.nn.LBeta](https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.LBeta.html#mindspore.nn.LBeta)|Deleted|This method avoids the numeric cancellation by explicitly decomposing lgamma into the Stirling approximation and an explicit log_gamma_correction, and cancelling the large terms from the Striling analytically.|Ascend/GPU|Math Functions
[mindspore.nn.LGamma](https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.LGamma.html#mindspore.nn.LGamma)|Deleted|Calculates LGamma using Lanczos’ approximation referring to “A Precision Approximation of the Gamma Function”.|Ascend/GPU|Math Functions
[mindspore.nn.MatDet](https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.MatDet.html#mindspore.nn.MatDet)|Deleted|Calculates the determinant of Positive-Definite Hermitian matrix using Cholesky decomposition.|GPU|Math Functions
[mindspore.nn.MatInverse](https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.MatInverse.html#mindspore.nn.MatInverse)|Deleted|Calculates the inverse of Positive-Definite Hermitian matrix using Cholesky decomposition.|GPU|Math Functions
[mindspore.nn.HShrink](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.HShrink.html#mindspore.nn.HShrink)|New|Applies the hard shrinkage function element-wise, each element complies the follow function:|r1.5: Ascend|Non-linear Activations
[mindspore.nn.SoftShrink](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.SoftShrink.html#mindspore.nn.SoftShrink)|New|Applies the soft shrinkage function elementwise.|r1.5: Ascend|Non-linear Activations
[mindspore.nn.GRU](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.GRU.html#mindspore.nn.GRU)|New|Stacked GRU (Gated Recurrent Unit) layers.|r1.5: Ascend/GPU|Recurrent Layers
[mindspore.nn.GRUCell](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.GRUCell.html#mindspore.nn.GRUCell)|New|A GRU(Gated Recurrent Unit) cell.|r1.5: Ascend/GPU|Recurrent Layers
[mindspore.nn.RNN](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.RNN.html#mindspore.nn.RNN)|New|Stacked Elman RNN layers.|r1.5: Ascend/GPU|Recurrent Layers
[mindspore.nn.RNNCell](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.RNNCell.html#mindspore.nn.RNNCell)|New|An Elman RNN cell with tanh or ReLU non-linearity.|r1.5: Ascend/GPU|Recurrent Layers
[mindspore.nn.Conv2dThor](https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.Conv2dThor.html#mindspore.nn.Conv2dThor)|Deleted|2D convolution layer and saving the information needed for THOR.|Ascend/GPU|Thor Layers
[mindspore.nn.DenseThor](https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.DenseThor.html#mindspore.nn.DenseThor)|Deleted|The dense connected layer and saving the information needed for THOR.|Ascend/GPU|Thor Layers
[mindspore.nn.EmbeddingThor](https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.EmbeddingThor.html#mindspore.nn.EmbeddingThor)|Deleted|A simple lookup table that stores embeddings of a fixed dictionary and size and saving the information needed for THOR.|Ascend/GPU|Thor Layers
[mindspore.nn.Roll](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/nn/mindspore.nn.Roll.html#mindspore.nn.Roll)|New|Rolls the elements of a tensor along an axis.|r1.5: Ascend|Utilities
