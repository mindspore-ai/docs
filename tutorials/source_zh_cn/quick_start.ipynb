{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://gitee.com/mindspore/docs/blob/master/tutorials/training/source_zh_cn/quick_start.ipynb\" target=\"_blank\"><img src=\"../_static/logo_source.png\"></a>\n",
    "\n",
    "# 快速入门\n",
    "\n",
    "本节贯穿MindSpore的基础功能，实现深度学习中的常见任务，请参考各节链接进行更加深入的学习。\n",
    "\n",
    "## 配置运行信息\n",
    "\n",
    "MindSpore通过`context.set_context`来配置运行需要的信息，譬如运行模式、后端信息、硬件等信息。\n",
    "\n",
    "导入`context`模块，配置运行需要的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from mindspore import context\n",
    "\n",
    "parser = argparse.ArgumentParser(description='MindSpore LeNet Example')\n",
    "parser.add_argument('--device_target', type=str, default=\"CPU\", choices=['Ascend', 'GPU', 'CPU'])\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在样例中，我们配置样例运行使用图模式。根据实际情况配置硬件信息，譬如代码运行在Ascend AI处理器上，则`--device_target`选择`Ascend`，代码运行在CPU、GPU同理。详细参数说明，请参见[context.set_context](https://www.mindspore.cn/doc/api_python/zh-CN/master/mindspore/mindspore.context.html)接口说明。\n",
    "\n",
    "## 数据处理\n",
    "\n",
    "数据集对于模型训练非常重要，好的数据集可以有效提高训练精度和效率。\n",
    "MindSpore提供了用于数据处理的API模块 `mindspore.dataset` ，用于存储样本和标签。在加载数据集前，我们通常会对数据集进行一些处理，`mindspore.dataset`也集成了常见的数据处理方法。\n",
    "\n",
    "### 定义数据集及数据操作\n",
    "\n",
    "首先导入MindSpore中`mindspore.dataset`和其他相应的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore import dtype as mstype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集处理主要分为四个步骤：\n",
    "\n",
    "1. 定义函数`create_dataset`来创建数据集\n",
    "2. 定义需要进行的数据增强和处理操作，为之后进行map映射做准备\n",
    "3. 使用map映射函数，将数据操作应用到数据集\n",
    "4. 进行数据shuffle、batch操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    # 定义数据集\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # 定义所需要的操作的map映射\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)\n",
    "    rescale_op = CV.Rescale(rescale, shift)\n",
    "    hwc2chw_op = CV.HWC2CHW()\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "    \n",
    "    # 使用map映射函数，将数据操作应用到数据集\n",
    "    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=resize_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_nml_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=hwc2chw_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    \n",
    "    # 进行shuffle、batch操作\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return mnist_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "其中，  \n",
    "`batch_size`：每组包含的数据个数，现设置每组包含32个数据。\n",
    "\n",
    "> MindSpore支持进行多种数据处理和增强的操作，具体可以参考[数据处理](https://www.mindspore.cn/doc/programming_guide/zh-CN/master/pipeline.html)和[数据增强](https://www.mindspore.cn/doc/programming_guide/zh-CN/master/augmentation.html)章节。\n",
    "\n",
    "## 创建模型\n",
    "\n",
    "使用MindSpore定义神经网络需要继承`mindspore.nn.Cell`。`Cell`是所有神经网络（如`Conv2d-relu-softmax`等）的基类。\n",
    "\n",
    "神经网络的各层需要预先在`__init__`方法中定义，然后通过定义`construct`方法来完成神经网络的前向构造。按照LeNet的网络结构，定义网络各层如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import Normal\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"\n",
    "    Lenet网络结构\n",
    "    \"\"\"\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # 定义所需要的运算\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 使用定义好的运算构建前向网络\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 实例化网络\n",
    "net = LeNet5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">阅读更多有关[在MindSpore中构建神经网络](https://www.mindspore.cn/tutorial/training/zh-CN/master/use/defining_the_network.html)的信息。\n",
    "\n",
    "## 优化模型参数\n",
    "\n",
    "要训练神经网络模型，需要定义损失函数和优化器。\n",
    "\n",
    "MindSpore支持的损失函数有`SoftmaxCrossEntropyWithLogits`、`L1Loss`、`MSELoss`等。这里使用交叉熵损失函数`SoftmaxCrossEntropyWithLogits`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">阅读更多有关[在MindSpore中使用损失函数](https://www.mindspore.cn/tutorial/training/zh-CN/master/quick_start/quick_start/xxx.html)的信息。\n",
    "\n",
    "MindSpore支持的优化器有`Adam`、`AdamWeightDecay`、`Momentum`等。这里使用`Momentum`优化器为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "net_opt = nn.Momentum(net.trainable_params(), learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">阅读更多有关[在MindSpore中使用优化器](https://www.mindspore.cn/tutorial/training/zh-CN/master/quick_start/quick_start/xxxx.html)的信息。\n",
    "\n",
    "## 训练及保存模型\n",
    "\n",
    "MindSpore提供了回调callback机制，可以在训练过程中执行自定义逻辑，这里以使用框架提供的`ModelCheckpoint`为例。\n",
    "`ModelCheckpoint`可以保存网络模型和参数，以便进行后续的fine-tuning（微调）操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig\n",
    "# 设置模型保存参数\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "# 应用模型保存参数\n",
    "ckpoint = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通过MindSpore提供的`model.train`接口可以方便地进行网络的训练，`LossMonitor`可以监控训练过程中`loss`值的变化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模型训练需要的库\n",
    "from mindspore.nn import Accuracy\n",
    "from mindspore.train.callback import LossMonitor\n",
    "from mindspore import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(args, model, epoch_size, data_path, repeat_size, ckpoint_cb, sink_mode):\n",
    "    \"\"\"定义训练的方法\"\"\"\n",
    "    # 加载训练数据集\n",
    "    ds_train = create_dataset(os.path.join(data_path, \"train\"), 32, repeat_size)\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor()], dataset_sink_mode=sink_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "其中，`dataset_sink_mode`用于控制数据是否下沉，数据下沉是指数据通过通道直接传送到Device上，可以加快训练速度，`dataset_sink_mode`为True表示数据下沉，否则为非下沉。\n",
    "\n",
    "通过模型运行测试数据集得到的结果，验证模型的泛化能力。\n",
    "\n",
    "1. 使用`model.eval`接口读入测试数据集。\n",
    "2. 使用保存后的模型参数进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(network, model, data_path):\n",
    "    \"\"\"定义验证的方法\"\"\"\n",
    "    ds_eval = create_dataset(os.path.join(data_path, \"test\"))\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
    "    print(\"{}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "这里把`train_epoch`设置为1，对数据集进行1个迭代的训练。在`train_net`和 `test_net`方法中，我们加载了之前下载的训练数据集，`mnist_path`是MNIST数据集路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 2.3025858\n",
      "epoch: 1 step: 2, loss is 2.30255\n",
      "epoch: 1 step: 3, loss is 2.302762\n",
      "epoch: 1 step: 4, loss is 2.3028076\n",
      "epoch: 1 step: 5, loss is 2.303102\n",
      "epoch: 1 step: 6, loss is 2.3031402\n",
      "epoch: 1 step: 7, loss is 2.3031652\n",
      "epoch: 1 step: 8, loss is 2.3039827\n",
      "epoch: 1 step: 9, loss is 2.3025923\n",
      "epoch: 1 step: 10, loss is 2.3016698\n",
      "epoch: 1 step: 11, loss is 2.3035183\n",
      "epoch: 1 step: 12, loss is 2.3031225\n",
      "epoch: 1 step: 13, loss is 2.3033147\n",
      "epoch: 1 step: 14, loss is 2.3009682\n",
      "epoch: 1 step: 15, loss is 2.301793\n",
      "epoch: 1 step: 16, loss is 2.303956\n",
      "epoch: 1 step: 17, loss is 2.3025527\n",
      "epoch: 1 step: 18, loss is 2.3027444\n",
      "epoch: 1 step: 19, loss is 2.3023093\n",
      "epoch: 1 step: 20, loss is 2.304097\n",
      "epoch: 1 step: 21, loss is 2.3056555\n",
      "epoch: 1 step: 22, loss is 2.3039343\n",
      "epoch: 1 step: 23, loss is 2.3043733\n",
      "epoch: 1 step: 24, loss is 2.301777\n",
      "epoch: 1 step: 25, loss is 2.3042703\n",
      "epoch: 1 step: 26, loss is 2.3033063\n",
      "epoch: 1 step: 27, loss is 2.3034954\n",
      "epoch: 1 step: 28, loss is 2.3030577\n",
      "epoch: 1 step: 29, loss is 2.300377\n",
      "epoch: 1 step: 30, loss is 2.2983584\n",
      "epoch: 1 step: 31, loss is 2.3049655\n",
      "epoch: 1 step: 32, loss is 2.3058405\n",
      "epoch: 1 step: 33, loss is 2.2975075\n",
      "epoch: 1 step: 34, loss is 2.3027284\n",
      "epoch: 1 step: 35, loss is 2.305198\n",
      "epoch: 1 step: 36, loss is 2.3053954\n",
      "epoch: 1 step: 37, loss is 2.296161\n",
      "epoch: 1 step: 38, loss is 2.2995348\n",
      "epoch: 1 step: 39, loss is 2.2982144\n",
      "epoch: 1 step: 40, loss is 2.302027\n",
      "epoch: 1 step: 41, loss is 2.3011124\n",
      "epoch: 1 step: 42, loss is 2.3037162\n",
      "epoch: 1 step: 43, loss is 2.3045733\n",
      "epoch: 1 step: 44, loss is 2.3000305\n",
      "epoch: 1 step: 45, loss is 2.3093655\n",
      "epoch: 1 step: 46, loss is 2.3028831\n",
      "epoch: 1 step: 47, loss is 2.2976224\n",
      "epoch: 1 step: 48, loss is 2.2998724\n",
      "epoch: 1 step: 49, loss is 2.3008893\n",
      "epoch: 1 step: 50, loss is 2.3073916\n",
      "epoch: 1 step: 51, loss is 2.3078518\n",
      "epoch: 1 step: 52, loss is 2.2984557\n",
      "epoch: 1 step: 53, loss is 2.29991\n",
      "epoch: 1 step: 54, loss is 2.30254\n",
      "epoch: 1 step: 55, loss is 2.3042998\n",
      "epoch: 1 step: 56, loss is 2.296676\n",
      "epoch: 1 step: 57, loss is 2.3043752\n",
      "epoch: 1 step: 58, loss is 2.3048666\n",
      "epoch: 1 step: 59, loss is 2.307372\n",
      "epoch: 1 step: 60, loss is 2.301609\n",
      "epoch: 1 step: 61, loss is 2.2969868\n",
      "epoch: 1 step: 62, loss is 2.2976537\n",
      "epoch: 1 step: 63, loss is 2.3092468\n",
      "epoch: 1 step: 64, loss is 2.300659\n",
      "epoch: 1 step: 65, loss is 2.3054438\n",
      "epoch: 1 step: 66, loss is 2.3086076\n",
      "epoch: 1 step: 67, loss is 2.295754\n",
      "epoch: 1 step: 68, loss is 2.3161159\n",
      "epoch: 1 step: 69, loss is 2.2925642\n",
      "epoch: 1 step: 70, loss is 2.3025558\n",
      "epoch: 1 step: 71, loss is 2.3002918\n",
      "epoch: 1 step: 72, loss is 2.2994952\n",
      "epoch: 1 step: 73, loss is 2.294921\n",
      "epoch: 1 step: 74, loss is 2.2972221\n",
      "epoch: 1 step: 75, loss is 2.3008256\n",
      "epoch: 1 step: 76, loss is 2.3000803\n",
      "epoch: 1 step: 77, loss is 2.299416\n",
      "epoch: 1 step: 78, loss is 2.3001297\n",
      "epoch: 1 step: 79, loss is 2.2965164\n",
      "epoch: 1 step: 80, loss is 2.2903893\n",
      "epoch: 1 step: 81, loss is 2.3046622\n",
      "epoch: 1 step: 82, loss is 2.3087904\n",
      "epoch: 1 step: 83, loss is 2.2922451\n",
      "epoch: 1 step: 84, loss is 2.2959957\n",
      "epoch: 1 step: 85, loss is 2.29487\n",
      "epoch: 1 step: 86, loss is 2.285842\n",
      "epoch: 1 step: 87, loss is 2.299424\n",
      "epoch: 1 step: 88, loss is 2.3046198\n",
      "epoch: 1 step: 89, loss is 2.2921565\n",
      "epoch: 1 step: 90, loss is 2.307233\n",
      "epoch: 1 step: 91, loss is 2.2961679\n",
      "epoch: 1 step: 92, loss is 2.3055344\n",
      "epoch: 1 step: 93, loss is 2.2868583\n",
      "epoch: 1 step: 94, loss is 2.3098297\n",
      "epoch: 1 step: 95, loss is 2.3084893\n",
      "epoch: 1 step: 96, loss is 2.3025663\n",
      "epoch: 1 step: 97, loss is 2.2963886\n",
      "epoch: 1 step: 98, loss is 2.300643\n",
      "epoch: 1 step: 99, loss is 2.3111596\n",
      "epoch: 1 step: 100, loss is 2.3048756\n",
      "epoch: 1 step: 101, loss is 2.3031857\n",
      "epoch: 1 step: 102, loss is 2.3016825\n",
      "epoch: 1 step: 103, loss is 2.2704651\n",
      "epoch: 1 step: 104, loss is 2.2880363\n",
      "epoch: 1 step: 105, loss is 2.300616\n",
      "epoch: 1 step: 106, loss is 2.3140268\n",
      "epoch: 1 step: 107, loss is 2.2919204\n",
      "epoch: 1 step: 108, loss is 2.3232658\n",
      "epoch: 1 step: 109, loss is 2.2887075\n",
      "epoch: 1 step: 110, loss is 2.3116915\n",
      "epoch: 1 step: 111, loss is 2.3107538\n",
      "epoch: 1 step: 112, loss is 2.3147812\n",
      "epoch: 1 step: 113, loss is 2.309417\n",
      "epoch: 1 step: 114, loss is 2.3087165\n",
      "epoch: 1 step: 115, loss is 2.2979228\n",
      "epoch: 1 step: 116, loss is 2.287122\n",
      "epoch: 1 step: 117, loss is 2.3001866\n",
      "epoch: 1 step: 118, loss is 2.3047256\n",
      "epoch: 1 step: 119, loss is 2.3194954\n",
      "epoch: 1 step: 120, loss is 2.308643\n",
      "epoch: 1 step: 121, loss is 2.2984548\n",
      "epoch: 1 step: 122, loss is 2.3161027\n",
      "epoch: 1 step: 123, loss is 2.3200936\n",
      "epoch: 1 step: 124, loss is 2.3030376\n",
      "epoch: 1 step: 125, loss is 2.2778022\n",
      "epoch: 1 step: 126, loss is 2.3116713\n",
      "epoch: 1 step: 127, loss is 2.3087807\n",
      "epoch: 1 step: 128, loss is 2.2995136\n",
      "epoch: 1 step: 129, loss is 2.297295\n",
      "epoch: 1 step: 130, loss is 2.287146\n",
      "epoch: 1 step: 131, loss is 2.3028524\n",
      "epoch: 1 step: 132, loss is 2.301208\n",
      "epoch: 1 step: 133, loss is 2.2862966\n",
      "epoch: 1 step: 134, loss is 2.2883592\n",
      "epoch: 1 step: 135, loss is 2.289201\n",
      "epoch: 1 step: 136, loss is 2.3059738\n",
      "epoch: 1 step: 137, loss is 2.28712\n",
      "epoch: 1 step: 138, loss is 2.3114552\n",
      "epoch: 1 step: 139, loss is 2.3139205\n",
      "epoch: 1 step: 140, loss is 2.3086073\n",
      "epoch: 1 step: 141, loss is 2.309448\n",
      "epoch: 1 step: 142, loss is 2.3194766\n",
      "epoch: 1 step: 143, loss is 2.2954357\n",
      "epoch: 1 step: 144, loss is 2.3050392\n",
      "epoch: 1 step: 145, loss is 2.3258283\n",
      "epoch: 1 step: 146, loss is 2.315812\n",
      "epoch: 1 step: 147, loss is 2.3050458\n",
      "epoch: 1 step: 148, loss is 2.2942538\n",
      "epoch: 1 step: 149, loss is 2.3049583\n",
      "epoch: 1 step: 150, loss is 2.2972503\n",
      "epoch: 1 step: 151, loss is 2.304506\n",
      "epoch: 1 step: 152, loss is 2.3040426\n",
      "epoch: 1 step: 153, loss is 2.3147824\n",
      "epoch: 1 step: 154, loss is 2.2884388\n",
      "epoch: 1 step: 155, loss is 2.3014705\n",
      "epoch: 1 step: 156, loss is 2.2953453\n",
      "epoch: 1 step: 157, loss is 2.2861798\n",
      "epoch: 1 step: 158, loss is 2.294113\n",
      "epoch: 1 step: 159, loss is 2.3193743\n",
      "epoch: 1 step: 160, loss is 2.2933507\n",
      "epoch: 1 step: 161, loss is 2.3052204\n",
      "epoch: 1 step: 162, loss is 2.3108938\n",
      "epoch: 1 step: 163, loss is 2.3125868\n",
      "epoch: 1 step: 164, loss is 2.314479\n",
      "epoch: 1 step: 165, loss is 2.316732\n",
      "epoch: 1 step: 166, loss is 2.3218884\n",
      "epoch: 1 step: 167, loss is 2.3157275\n",
      "epoch: 1 step: 168, loss is 2.3077562\n",
      "epoch: 1 step: 169, loss is 2.2893107\n",
      "epoch: 1 step: 170, loss is 2.2928445\n",
      "epoch: 1 step: 171, loss is 2.3039155\n",
      "epoch: 1 step: 172, loss is 2.2834163\n",
      "epoch: 1 step: 173, loss is 2.303961\n",
      "epoch: 1 step: 174, loss is 2.3125634\n",
      "epoch: 1 step: 175, loss is 2.307097\n",
      "epoch: 1 step: 176, loss is 2.3090475\n",
      "epoch: 1 step: 177, loss is 2.3044825\n",
      "epoch: 1 step: 178, loss is 2.302985\n",
      "epoch: 1 step: 179, loss is 2.2973747\n",
      "epoch: 1 step: 180, loss is 2.2907667\n",
      "epoch: 1 step: 181, loss is 2.3148563\n",
      "epoch: 1 step: 182, loss is 2.300355\n",
      "epoch: 1 step: 183, loss is 2.2960157\n",
      "epoch: 1 step: 184, loss is 2.280629\n",
      "epoch: 1 step: 185, loss is 2.301179\n",
      "epoch: 1 step: 186, loss is 2.2920034\n",
      "epoch: 1 step: 187, loss is 2.3057506\n",
      "epoch: 1 step: 188, loss is 2.3168392\n",
      "epoch: 1 step: 189, loss is 2.2915769\n",
      "epoch: 1 step: 190, loss is 2.2980912\n",
      "epoch: 1 step: 191, loss is 2.2944906\n",
      "epoch: 1 step: 192, loss is 2.2983065\n",
      "epoch: 1 step: 193, loss is 2.2781568\n",
      "epoch: 1 step: 194, loss is 2.3041725\n",
      "epoch: 1 step: 195, loss is 2.3052092\n",
      "epoch: 1 step: 196, loss is 2.2879267\n",
      "epoch: 1 step: 197, loss is 2.301817\n",
      "epoch: 1 step: 198, loss is 2.3028243\n",
      "epoch: 1 step: 199, loss is 2.292534\n",
      "epoch: 1 step: 200, loss is 2.2866395\n",
      "epoch: 1 step: 201, loss is 2.3069615\n",
      "epoch: 1 step: 202, loss is 2.3129961\n",
      "epoch: 1 step: 203, loss is 2.289147\n",
      "epoch: 1 step: 204, loss is 2.2987685\n",
      "epoch: 1 step: 205, loss is 2.3014243\n",
      "epoch: 1 step: 206, loss is 2.2824533\n",
      "epoch: 1 step: 207, loss is 2.2975452\n",
      "epoch: 1 step: 208, loss is 2.2931564\n",
      "epoch: 1 step: 209, loss is 2.3153932\n",
      "epoch: 1 step: 210, loss is 2.3020208\n",
      "epoch: 1 step: 211, loss is 2.2988198\n",
      "epoch: 1 step: 212, loss is 2.3201306\n",
      "epoch: 1 step: 213, loss is 2.2966464\n",
      "epoch: 1 step: 214, loss is 2.3225656\n",
      "epoch: 1 step: 215, loss is 2.3085978\n",
      "epoch: 1 step: 216, loss is 2.2974842\n",
      "epoch: 1 step: 217, loss is 2.3246074\n",
      "epoch: 1 step: 218, loss is 2.290768\n",
      "epoch: 1 step: 219, loss is 2.3018022\n",
      "epoch: 1 step: 220, loss is 2.295109\n",
      "epoch: 1 step: 221, loss is 2.2967095\n",
      "epoch: 1 step: 222, loss is 2.317628\n",
      "epoch: 1 step: 223, loss is 2.3007588\n",
      "epoch: 1 step: 224, loss is 2.3013434\n",
      "epoch: 1 step: 225, loss is 2.2976794\n",
      "epoch: 1 step: 226, loss is 2.31335\n",
      "epoch: 1 step: 227, loss is 2.2867563\n",
      "epoch: 1 step: 228, loss is 2.3076358\n",
      "epoch: 1 step: 229, loss is 2.2895715\n",
      "epoch: 1 step: 230, loss is 2.3109891\n",
      "epoch: 1 step: 231, loss is 2.2972043\n",
      "epoch: 1 step: 232, loss is 2.3055325\n",
      "epoch: 1 step: 233, loss is 2.2918973\n",
      "epoch: 1 step: 234, loss is 2.3101032\n",
      "epoch: 1 step: 235, loss is 2.3194158\n",
      "epoch: 1 step: 236, loss is 2.2776687\n",
      "epoch: 1 step: 237, loss is 2.3248413\n",
      "epoch: 1 step: 238, loss is 2.305055\n",
      "epoch: 1 step: 239, loss is 2.3188782\n",
      "epoch: 1 step: 240, loss is 2.2975564\n",
      "epoch: 1 step: 241, loss is 2.2961278\n",
      "epoch: 1 step: 242, loss is 2.2882915\n",
      "epoch: 1 step: 243, loss is 2.3015876\n",
      "epoch: 1 step: 244, loss is 2.3005059\n",
      "epoch: 1 step: 245, loss is 2.3160567\n",
      "epoch: 1 step: 246, loss is 2.3035705\n",
      "epoch: 1 step: 247, loss is 2.3042362\n",
      "epoch: 1 step: 248, loss is 2.3137956\n",
      "epoch: 1 step: 249, loss is 2.3056262\n",
      "epoch: 1 step: 250, loss is 2.31653\n",
      "epoch: 1 step: 251, loss is 2.3091545\n",
      "epoch: 1 step: 252, loss is 2.2966807\n",
      "epoch: 1 step: 253, loss is 2.3051949\n",
      "epoch: 1 step: 254, loss is 2.2977107\n",
      "epoch: 1 step: 255, loss is 2.303083\n",
      "epoch: 1 step: 256, loss is 2.3031323\n",
      "epoch: 1 step: 257, loss is 2.2921364\n",
      "epoch: 1 step: 258, loss is 2.3170192\n",
      "epoch: 1 step: 259, loss is 2.2890036\n",
      "epoch: 1 step: 260, loss is 2.3068285\n",
      "epoch: 1 step: 261, loss is 2.2976713\n",
      "epoch: 1 step: 262, loss is 2.3042803\n",
      "epoch: 1 step: 263, loss is 2.3166358\n",
      "epoch: 1 step: 264, loss is 2.2911632\n",
      "epoch: 1 step: 265, loss is 2.2994983\n",
      "epoch: 1 step: 266, loss is 2.3020418\n",
      "epoch: 1 step: 267, loss is 2.3100162\n",
      "epoch: 1 step: 268, loss is 2.2941966\n",
      "epoch: 1 step: 269, loss is 2.2988465\n",
      "epoch: 1 step: 270, loss is 2.2999537\n",
      "epoch: 1 step: 271, loss is 2.33163\n",
      "epoch: 1 step: 272, loss is 2.2999287\n",
      "epoch: 1 step: 273, loss is 2.286343\n",
      "epoch: 1 step: 274, loss is 2.3063116\n",
      "epoch: 1 step: 275, loss is 2.299879\n",
      "epoch: 1 step: 276, loss is 2.2961729\n",
      "epoch: 1 step: 277, loss is 2.304388\n",
      "epoch: 1 step: 278, loss is 2.2895079\n",
      "epoch: 1 step: 279, loss is 2.3002825\n",
      "epoch: 1 step: 280, loss is 2.3091757\n",
      "epoch: 1 step: 281, loss is 2.317237\n",
      "epoch: 1 step: 282, loss is 2.3188536\n",
      "epoch: 1 step: 283, loss is 2.3076863\n",
      "epoch: 1 step: 284, loss is 2.2962883\n",
      "epoch: 1 step: 285, loss is 2.3008397\n",
      "epoch: 1 step: 286, loss is 2.3119795\n",
      "epoch: 1 step: 287, loss is 2.2977953\n",
      "epoch: 1 step: 288, loss is 2.3054585\n",
      "epoch: 1 step: 289, loss is 2.2991478\n",
      "epoch: 1 step: 290, loss is 2.3040848\n",
      "epoch: 1 step: 291, loss is 2.3151703\n",
      "epoch: 1 step: 292, loss is 2.3004456\n",
      "epoch: 1 step: 293, loss is 2.302075\n",
      "epoch: 1 step: 294, loss is 2.2967856\n",
      "epoch: 1 step: 295, loss is 2.3045878\n",
      "epoch: 1 step: 296, loss is 2.2917902\n",
      "epoch: 1 step: 297, loss is 2.2964926\n",
      "epoch: 1 step: 298, loss is 2.3058846\n",
      "epoch: 1 step: 299, loss is 2.306472\n",
      "epoch: 1 step: 300, loss is 2.2908955\n",
      "epoch: 1 step: 301, loss is 2.300238\n",
      "epoch: 1 step: 302, loss is 2.2996395\n",
      "epoch: 1 step: 303, loss is 2.3159273\n",
      "epoch: 1 step: 304, loss is 2.2956617\n",
      "epoch: 1 step: 305, loss is 2.2955072\n",
      "epoch: 1 step: 306, loss is 2.3170266\n",
      "epoch: 1 step: 307, loss is 2.301391\n",
      "epoch: 1 step: 308, loss is 2.3056319\n",
      "epoch: 1 step: 309, loss is 2.3112514\n",
      "epoch: 1 step: 310, loss is 2.3093317\n",
      "epoch: 1 step: 311, loss is 2.2943337\n",
      "epoch: 1 step: 312, loss is 2.3061056\n",
      "epoch: 1 step: 313, loss is 2.3025858\n",
      "epoch: 1 step: 314, loss is 2.29452\n",
      "epoch: 1 step: 315, loss is 2.2983801\n",
      "epoch: 1 step: 316, loss is 2.3149648\n",
      "epoch: 1 step: 317, loss is 2.2999566\n",
      "epoch: 1 step: 318, loss is 2.3129067\n",
      "epoch: 1 step: 319, loss is 2.295619\n",
      "epoch: 1 step: 320, loss is 2.308366\n",
      "epoch: 1 step: 321, loss is 2.2930322\n",
      "epoch: 1 step: 322, loss is 2.3035035\n",
      "epoch: 1 step: 323, loss is 2.3081822\n",
      "epoch: 1 step: 324, loss is 2.3098862\n",
      "epoch: 1 step: 325, loss is 2.2934248\n",
      "epoch: 1 step: 326, loss is 2.3123703\n",
      "epoch: 1 step: 327, loss is 2.31404\n",
      "epoch: 1 step: 328, loss is 2.3093796\n",
      "epoch: 1 step: 329, loss is 2.303097\n",
      "epoch: 1 step: 330, loss is 2.3160045\n",
      "epoch: 1 step: 331, loss is 2.2986891\n",
      "epoch: 1 step: 332, loss is 2.2977912\n",
      "epoch: 1 step: 333, loss is 2.2838976\n",
      "epoch: 1 step: 334, loss is 2.2981067\n",
      "epoch: 1 step: 335, loss is 2.3076649\n",
      "epoch: 1 step: 336, loss is 2.3099103\n",
      "epoch: 1 step: 337, loss is 2.2959254\n",
      "epoch: 1 step: 338, loss is 2.3054848\n",
      "epoch: 1 step: 339, loss is 2.2972684\n",
      "epoch: 1 step: 340, loss is 2.3032503\n",
      "epoch: 1 step: 341, loss is 2.2956674\n",
      "epoch: 1 step: 342, loss is 2.304305\n",
      "epoch: 1 step: 343, loss is 2.2987432\n",
      "epoch: 1 step: 344, loss is 2.3010833\n",
      "epoch: 1 step: 345, loss is 2.2986445\n",
      "epoch: 1 step: 346, loss is 2.2909749\n",
      "epoch: 1 step: 347, loss is 2.301358\n",
      "epoch: 1 step: 348, loss is 2.294965\n",
      "epoch: 1 step: 349, loss is 2.2890942\n",
      "epoch: 1 step: 350, loss is 2.300417\n",
      "epoch: 1 step: 351, loss is 2.2984035\n",
      "epoch: 1 step: 352, loss is 2.3039143\n",
      "epoch: 1 step: 353, loss is 2.300588\n",
      "epoch: 1 step: 354, loss is 2.3098598\n",
      "epoch: 1 step: 355, loss is 2.2901716\n",
      "epoch: 1 step: 356, loss is 2.3117805\n",
      "epoch: 1 step: 357, loss is 2.2993941\n",
      "epoch: 1 step: 358, loss is 2.2895699\n",
      "epoch: 1 step: 359, loss is 2.3056383\n",
      "epoch: 1 step: 360, loss is 2.3058345\n",
      "epoch: 1 step: 361, loss is 2.2951508\n",
      "epoch: 1 step: 362, loss is 2.3096294\n",
      "epoch: 1 step: 363, loss is 2.309071\n",
      "epoch: 1 step: 364, loss is 2.2995682\n",
      "epoch: 1 step: 365, loss is 2.304543\n",
      "epoch: 1 step: 366, loss is 2.2921276\n",
      "epoch: 1 step: 367, loss is 2.278173\n",
      "epoch: 1 step: 368, loss is 2.3074312\n",
      "epoch: 1 step: 369, loss is 2.2980187\n",
      "epoch: 1 step: 370, loss is 2.287474\n",
      "epoch: 1 step: 371, loss is 2.3004222\n",
      "epoch: 1 step: 372, loss is 2.2968884\n",
      "epoch: 1 step: 373, loss is 2.2895947\n",
      "epoch: 1 step: 374, loss is 2.3151963\n",
      "epoch: 1 step: 375, loss is 2.311313\n",
      "epoch: 1 step: 376, loss is 2.2842364\n",
      "epoch: 1 step: 377, loss is 2.2864509\n",
      "epoch: 1 step: 378, loss is 2.2903063\n",
      "epoch: 1 step: 379, loss is 2.2906952\n",
      "epoch: 1 step: 380, loss is 2.2905247\n",
      "epoch: 1 step: 381, loss is 2.3033602\n",
      "epoch: 1 step: 382, loss is 2.291298\n",
      "epoch: 1 step: 383, loss is 2.2804513\n",
      "epoch: 1 step: 384, loss is 2.2947369\n",
      "epoch: 1 step: 385, loss is 2.2965956\n",
      "epoch: 1 step: 386, loss is 2.3018649\n",
      "epoch: 1 step: 387, loss is 2.2876778\n",
      "epoch: 1 step: 388, loss is 2.2937076\n",
      "epoch: 1 step: 389, loss is 2.309733\n",
      "epoch: 1 step: 390, loss is 2.295897\n",
      "epoch: 1 step: 391, loss is 2.314374\n",
      "epoch: 1 step: 392, loss is 2.2931876\n",
      "epoch: 1 step: 393, loss is 2.2905035\n",
      "epoch: 1 step: 394, loss is 2.3041878\n",
      "epoch: 1 step: 395, loss is 2.2722988\n",
      "epoch: 1 step: 396, loss is 2.3204057\n",
      "epoch: 1 step: 397, loss is 2.3252747\n",
      "epoch: 1 step: 398, loss is 2.2705636\n",
      "epoch: 1 step: 399, loss is 2.2964752\n",
      "epoch: 1 step: 400, loss is 2.314135\n",
      "epoch: 1 step: 401, loss is 2.3104374\n",
      "epoch: 1 step: 402, loss is 2.293943\n",
      "epoch: 1 step: 403, loss is 2.2964287\n",
      "epoch: 1 step: 404, loss is 2.2816334\n",
      "epoch: 1 step: 405, loss is 2.3353257\n",
      "epoch: 1 step: 406, loss is 2.3227978\n",
      "epoch: 1 step: 407, loss is 2.315639\n",
      "epoch: 1 step: 408, loss is 2.309644\n",
      "epoch: 1 step: 409, loss is 2.3178208\n",
      "epoch: 1 step: 410, loss is 2.2953875\n",
      "epoch: 1 step: 411, loss is 2.3129005\n",
      "epoch: 1 step: 412, loss is 2.2835023\n",
      "epoch: 1 step: 413, loss is 2.2948177\n",
      "epoch: 1 step: 414, loss is 2.3042295\n",
      "epoch: 1 step: 415, loss is 2.306868\n",
      "epoch: 1 step: 416, loss is 2.3137279\n",
      "epoch: 1 step: 417, loss is 2.2985804\n",
      "epoch: 1 step: 418, loss is 2.3011305\n",
      "epoch: 1 step: 419, loss is 2.288144\n",
      "epoch: 1 step: 420, loss is 2.2992759\n",
      "epoch: 1 step: 421, loss is 2.2970521\n",
      "epoch: 1 step: 422, loss is 2.2937891\n",
      "epoch: 1 step: 423, loss is 2.2980042\n",
      "epoch: 1 step: 424, loss is 2.3016348\n",
      "epoch: 1 step: 425, loss is 2.2985117\n",
      "epoch: 1 step: 426, loss is 2.2905328\n",
      "epoch: 1 step: 427, loss is 2.3128355\n",
      "epoch: 1 step: 428, loss is 2.2983458\n",
      "epoch: 1 step: 429, loss is 2.2938282\n",
      "epoch: 1 step: 430, loss is 2.2977543\n",
      "epoch: 1 step: 431, loss is 2.3189306\n",
      "epoch: 1 step: 432, loss is 2.2977874\n",
      "epoch: 1 step: 433, loss is 2.309886\n",
      "epoch: 1 step: 434, loss is 2.2873733\n",
      "epoch: 1 step: 435, loss is 2.3147132\n",
      "epoch: 1 step: 436, loss is 2.2812068\n",
      "epoch: 1 step: 437, loss is 2.3153474\n",
      "epoch: 1 step: 438, loss is 2.2946033\n",
      "epoch: 1 step: 439, loss is 2.3287454\n",
      "epoch: 1 step: 440, loss is 2.3007674\n",
      "epoch: 1 step: 441, loss is 2.2721858\n",
      "epoch: 1 step: 442, loss is 2.2958782\n",
      "epoch: 1 step: 443, loss is 2.3013859\n",
      "epoch: 1 step: 444, loss is 2.3278441\n",
      "epoch: 1 step: 445, loss is 2.3212304\n",
      "epoch: 1 step: 446, loss is 2.298533\n",
      "epoch: 1 step: 447, loss is 2.280128\n",
      "epoch: 1 step: 448, loss is 2.3189034\n",
      "epoch: 1 step: 449, loss is 2.2978103\n",
      "epoch: 1 step: 450, loss is 2.287748\n",
      "epoch: 1 step: 451, loss is 2.3023548\n",
      "epoch: 1 step: 452, loss is 2.272197\n",
      "epoch: 1 step: 453, loss is 2.2968855\n",
      "epoch: 1 step: 454, loss is 2.3007367\n",
      "epoch: 1 step: 455, loss is 2.3008106\n",
      "epoch: 1 step: 456, loss is 2.302368\n",
      "epoch: 1 step: 457, loss is 2.3018358\n",
      "epoch: 1 step: 458, loss is 2.3192396\n",
      "epoch: 1 step: 459, loss is 2.3074272\n",
      "epoch: 1 step: 460, loss is 2.282678\n",
      "epoch: 1 step: 461, loss is 2.2941585\n",
      "epoch: 1 step: 462, loss is 2.2986133\n",
      "epoch: 1 step: 463, loss is 2.316976\n",
      "epoch: 1 step: 464, loss is 2.3246431\n",
      "epoch: 1 step: 465, loss is 2.3181717\n",
      "epoch: 1 step: 466, loss is 2.3003895\n",
      "epoch: 1 step: 467, loss is 2.2892008\n",
      "epoch: 1 step: 468, loss is 2.324767\n",
      "epoch: 1 step: 469, loss is 2.301185\n",
      "epoch: 1 step: 470, loss is 2.2887878\n",
      "epoch: 1 step: 471, loss is 2.3038971\n",
      "epoch: 1 step: 472, loss is 2.3016577\n",
      "epoch: 1 step: 473, loss is 2.3131876\n",
      "epoch: 1 step: 474, loss is 2.2838097\n",
      "epoch: 1 step: 475, loss is 2.295851\n",
      "epoch: 1 step: 476, loss is 2.2773051\n",
      "epoch: 1 step: 477, loss is 2.284231\n",
      "epoch: 1 step: 478, loss is 2.307502\n",
      "epoch: 1 step: 479, loss is 2.2867544\n",
      "epoch: 1 step: 480, loss is 2.2930453\n",
      "epoch: 1 step: 481, loss is 2.2841344\n",
      "epoch: 1 step: 482, loss is 2.3097239\n",
      "epoch: 1 step: 483, loss is 2.2968452\n",
      "epoch: 1 step: 484, loss is 2.2906234\n",
      "epoch: 1 step: 485, loss is 2.2974985\n",
      "epoch: 1 step: 486, loss is 2.284427\n",
      "epoch: 1 step: 487, loss is 2.2958622\n",
      "epoch: 1 step: 488, loss is 2.288649\n",
      "epoch: 1 step: 489, loss is 2.3210661\n",
      "epoch: 1 step: 490, loss is 2.30447\n",
      "epoch: 1 step: 491, loss is 2.2936459\n",
      "epoch: 1 step: 492, loss is 2.3005872\n",
      "epoch: 1 step: 493, loss is 2.327982\n",
      "epoch: 1 step: 494, loss is 2.2823021\n",
      "epoch: 1 step: 495, loss is 2.301925\n",
      "epoch: 1 step: 496, loss is 2.3133576\n",
      "epoch: 1 step: 497, loss is 2.3027983\n",
      "epoch: 1 step: 498, loss is 2.3020124\n",
      "epoch: 1 step: 499, loss is 2.271111\n",
      "epoch: 1 step: 500, loss is 2.3080952\n",
      "epoch: 1 step: 501, loss is 2.3133595\n",
      "epoch: 1 step: 502, loss is 2.2879267\n",
      "epoch: 1 step: 503, loss is 2.2980099\n",
      "epoch: 1 step: 504, loss is 2.30351\n",
      "epoch: 1 step: 505, loss is 2.2870793\n",
      "epoch: 1 step: 506, loss is 2.3015718\n",
      "epoch: 1 step: 507, loss is 2.281212\n",
      "epoch: 1 step: 508, loss is 2.3028362\n",
      "epoch: 1 step: 509, loss is 2.3093758\n",
      "epoch: 1 step: 510, loss is 2.2624092\n",
      "epoch: 1 step: 511, loss is 2.3064275\n",
      "epoch: 1 step: 512, loss is 2.2992065\n",
      "epoch: 1 step: 513, loss is 2.3092985\n",
      "epoch: 1 step: 514, loss is 2.300976\n",
      "epoch: 1 step: 515, loss is 2.3343422\n",
      "epoch: 1 step: 516, loss is 2.2680998\n",
      "epoch: 1 step: 517, loss is 2.3132832\n",
      "epoch: 1 step: 518, loss is 2.2985992\n",
      "epoch: 1 step: 519, loss is 2.3143263\n",
      "epoch: 1 step: 520, loss is 2.2975507\n",
      "epoch: 1 step: 521, loss is 2.3060162\n",
      "epoch: 1 step: 522, loss is 2.304698\n",
      "epoch: 1 step: 523, loss is 2.2970593\n",
      "epoch: 1 step: 524, loss is 2.2665956\n",
      "epoch: 1 step: 525, loss is 2.275105\n",
      "epoch: 1 step: 526, loss is 2.2809942\n",
      "epoch: 1 step: 527, loss is 2.3019497\n",
      "epoch: 1 step: 528, loss is 2.306002\n",
      "epoch: 1 step: 529, loss is 2.3060188\n",
      "epoch: 1 step: 530, loss is 2.2842565\n",
      "epoch: 1 step: 531, loss is 2.3127253\n",
      "epoch: 1 step: 532, loss is 2.2862003\n",
      "epoch: 1 step: 533, loss is 2.3300269\n",
      "epoch: 1 step: 534, loss is 2.3061984\n",
      "epoch: 1 step: 535, loss is 2.315472\n",
      "epoch: 1 step: 536, loss is 2.3455253\n",
      "epoch: 1 step: 537, loss is 2.2816212\n",
      "epoch: 1 step: 538, loss is 2.289165\n",
      "epoch: 1 step: 539, loss is 2.2995539\n",
      "epoch: 1 step: 540, loss is 2.3120291\n",
      "epoch: 1 step: 541, loss is 2.2798283\n",
      "epoch: 1 step: 542, loss is 2.3099418\n",
      "epoch: 1 step: 543, loss is 2.280868\n",
      "epoch: 1 step: 544, loss is 2.3043272\n",
      "epoch: 1 step: 545, loss is 2.2784402\n",
      "epoch: 1 step: 546, loss is 2.2960207\n",
      "epoch: 1 step: 547, loss is 2.3100784\n",
      "epoch: 1 step: 548, loss is 2.314539\n",
      "epoch: 1 step: 549, loss is 2.2973\n",
      "epoch: 1 step: 550, loss is 2.3028457\n",
      "epoch: 1 step: 551, loss is 2.2903624\n",
      "epoch: 1 step: 552, loss is 2.318406\n",
      "epoch: 1 step: 553, loss is 2.3116772\n",
      "epoch: 1 step: 554, loss is 2.3062224\n",
      "epoch: 1 step: 555, loss is 2.297595\n",
      "epoch: 1 step: 556, loss is 2.32026\n",
      "epoch: 1 step: 557, loss is 2.3322275\n",
      "epoch: 1 step: 558, loss is 2.299881\n",
      "epoch: 1 step: 559, loss is 2.306275\n",
      "epoch: 1 step: 560, loss is 2.3130639\n",
      "epoch: 1 step: 561, loss is 2.3037798\n",
      "epoch: 1 step: 562, loss is 2.2987144\n",
      "epoch: 1 step: 563, loss is 2.3156054\n",
      "epoch: 1 step: 564, loss is 2.2953098\n",
      "epoch: 1 step: 565, loss is 2.2926533\n",
      "epoch: 1 step: 566, loss is 2.3258977\n",
      "epoch: 1 step: 567, loss is 2.2944798\n",
      "epoch: 1 step: 568, loss is 2.3287916\n",
      "epoch: 1 step: 569, loss is 2.3087208\n",
      "epoch: 1 step: 570, loss is 2.316341\n",
      "epoch: 1 step: 571, loss is 2.2899873\n",
      "epoch: 1 step: 572, loss is 2.2951424\n",
      "epoch: 1 step: 573, loss is 2.2965453\n",
      "epoch: 1 step: 574, loss is 2.3031151\n",
      "epoch: 1 step: 575, loss is 2.2867439\n",
      "epoch: 1 step: 576, loss is 2.3093975\n",
      "epoch: 1 step: 577, loss is 2.3031824\n",
      "epoch: 1 step: 578, loss is 2.2912614\n",
      "epoch: 1 step: 579, loss is 2.2986763\n",
      "epoch: 1 step: 580, loss is 2.316178\n",
      "epoch: 1 step: 581, loss is 2.3147986\n",
      "epoch: 1 step: 582, loss is 2.3118293\n",
      "epoch: 1 step: 583, loss is 2.3128264\n",
      "epoch: 1 step: 584, loss is 2.3124948\n",
      "epoch: 1 step: 585, loss is 2.3015416\n",
      "epoch: 1 step: 586, loss is 2.3077717\n",
      "epoch: 1 step: 587, loss is 2.2994423\n",
      "epoch: 1 step: 588, loss is 2.2858543\n",
      "epoch: 1 step: 589, loss is 2.3109136\n",
      "epoch: 1 step: 590, loss is 2.29757\n",
      "epoch: 1 step: 591, loss is 2.3058565\n",
      "epoch: 1 step: 592, loss is 2.3154747\n",
      "epoch: 1 step: 593, loss is 2.3151226\n",
      "epoch: 1 step: 594, loss is 2.316919\n",
      "epoch: 1 step: 595, loss is 2.3105605\n",
      "epoch: 1 step: 596, loss is 2.3111064\n",
      "epoch: 1 step: 597, loss is 2.3009853\n",
      "epoch: 1 step: 598, loss is 2.3058715\n",
      "epoch: 1 step: 599, loss is 2.3007164\n",
      "epoch: 1 step: 600, loss is 2.289353\n",
      "epoch: 1 step: 601, loss is 2.2941105\n",
      "epoch: 1 step: 602, loss is 2.290346\n",
      "epoch: 1 step: 603, loss is 2.3160956\n",
      "epoch: 1 step: 604, loss is 2.3007896\n",
      "epoch: 1 step: 605, loss is 2.297548\n",
      "epoch: 1 step: 606, loss is 2.2992547\n",
      "epoch: 1 step: 607, loss is 2.30692\n",
      "epoch: 1 step: 608, loss is 2.3098233\n",
      "epoch: 1 step: 609, loss is 2.2963655\n",
      "epoch: 1 step: 610, loss is 2.3154333\n",
      "epoch: 1 step: 611, loss is 2.2917016\n",
      "epoch: 1 step: 612, loss is 2.2830667\n",
      "epoch: 1 step: 613, loss is 2.3190918\n",
      "epoch: 1 step: 614, loss is 2.2916942\n",
      "epoch: 1 step: 615, loss is 2.301287\n",
      "epoch: 1 step: 616, loss is 2.3225846\n",
      "epoch: 1 step: 617, loss is 2.3069081\n",
      "epoch: 1 step: 618, loss is 2.3168805\n",
      "epoch: 1 step: 619, loss is 2.2911177\n",
      "epoch: 1 step: 620, loss is 2.3055806\n",
      "epoch: 1 step: 621, loss is 2.2836368\n",
      "epoch: 1 step: 622, loss is 2.3005083\n",
      "epoch: 1 step: 623, loss is 2.304304\n",
      "epoch: 1 step: 624, loss is 2.3159637\n",
      "epoch: 1 step: 625, loss is 2.3033142\n",
      "epoch: 1 step: 626, loss is 2.2998323\n",
      "epoch: 1 step: 627, loss is 2.3100722\n",
      "epoch: 1 step: 628, loss is 2.3048658\n",
      "epoch: 1 step: 629, loss is 2.3123822\n",
      "epoch: 1 step: 630, loss is 2.3023782\n",
      "epoch: 1 step: 631, loss is 2.3091145\n",
      "epoch: 1 step: 632, loss is 2.311011\n",
      "epoch: 1 step: 633, loss is 2.299066\n",
      "epoch: 1 step: 634, loss is 2.3022115\n",
      "epoch: 1 step: 635, loss is 2.2984424\n",
      "epoch: 1 step: 636, loss is 2.2909553\n",
      "epoch: 1 step: 637, loss is 2.2969868\n",
      "epoch: 1 step: 638, loss is 2.2976437\n",
      "epoch: 1 step: 639, loss is 2.2995129\n",
      "epoch: 1 step: 640, loss is 2.3067327\n",
      "epoch: 1 step: 641, loss is 2.2942946\n",
      "epoch: 1 step: 642, loss is 2.3181782\n",
      "epoch: 1 step: 643, loss is 2.28927\n",
      "epoch: 1 step: 644, loss is 2.2838898\n",
      "epoch: 1 step: 645, loss is 2.2690952\n",
      "epoch: 1 step: 646, loss is 2.3003242\n",
      "epoch: 1 step: 647, loss is 2.2867997\n",
      "epoch: 1 step: 648, loss is 2.315173\n",
      "epoch: 1 step: 649, loss is 2.311862\n",
      "epoch: 1 step: 650, loss is 2.30501\n",
      "epoch: 1 step: 651, loss is 2.3005223\n",
      "epoch: 1 step: 652, loss is 2.3027074\n",
      "epoch: 1 step: 653, loss is 2.3029723\n",
      "epoch: 1 step: 654, loss is 2.2901344\n",
      "epoch: 1 step: 655, loss is 2.3127406\n",
      "epoch: 1 step: 656, loss is 2.3111076\n",
      "epoch: 1 step: 657, loss is 2.2916481\n",
      "epoch: 1 step: 658, loss is 2.3021486\n",
      "epoch: 1 step: 659, loss is 2.309083\n",
      "epoch: 1 step: 660, loss is 2.3024495\n",
      "epoch: 1 step: 661, loss is 2.3188813\n",
      "epoch: 1 step: 662, loss is 2.3212118\n",
      "epoch: 1 step: 663, loss is 2.291047\n",
      "epoch: 1 step: 664, loss is 2.3138583\n",
      "epoch: 1 step: 665, loss is 2.299994\n",
      "epoch: 1 step: 666, loss is 2.3086557\n",
      "epoch: 1 step: 667, loss is 2.3044124\n",
      "epoch: 1 step: 668, loss is 2.2918947\n",
      "epoch: 1 step: 669, loss is 2.3236895\n",
      "epoch: 1 step: 670, loss is 2.291373\n",
      "epoch: 1 step: 671, loss is 2.3052285\n",
      "epoch: 1 step: 672, loss is 2.2885935\n",
      "epoch: 1 step: 673, loss is 2.3089204\n",
      "epoch: 1 step: 674, loss is 2.2899535\n",
      "epoch: 1 step: 675, loss is 2.2965338\n",
      "epoch: 1 step: 676, loss is 2.2943134\n",
      "epoch: 1 step: 677, loss is 2.289842\n",
      "epoch: 1 step: 678, loss is 2.298283\n",
      "epoch: 1 step: 679, loss is 2.3046694\n",
      "epoch: 1 step: 680, loss is 2.3003554\n",
      "epoch: 1 step: 681, loss is 2.2914248\n",
      "epoch: 1 step: 682, loss is 2.3012104\n",
      "epoch: 1 step: 683, loss is 2.305203\n",
      "epoch: 1 step: 684, loss is 2.2988544\n",
      "epoch: 1 step: 685, loss is 2.2975883\n",
      "epoch: 1 step: 686, loss is 2.3002434\n",
      "epoch: 1 step: 687, loss is 2.300445\n",
      "epoch: 1 step: 688, loss is 2.3078566\n",
      "epoch: 1 step: 689, loss is 2.2914517\n",
      "epoch: 1 step: 690, loss is 2.310784\n",
      "epoch: 1 step: 691, loss is 2.3113947\n",
      "epoch: 1 step: 692, loss is 2.2875984\n",
      "epoch: 1 step: 693, loss is 2.2875545\n",
      "epoch: 1 step: 694, loss is 2.2980437\n",
      "epoch: 1 step: 695, loss is 2.2824214\n",
      "epoch: 1 step: 696, loss is 2.2964015\n",
      "epoch: 1 step: 697, loss is 2.315135\n",
      "epoch: 1 step: 698, loss is 2.287846\n",
      "epoch: 1 step: 699, loss is 2.291555\n",
      "epoch: 1 step: 700, loss is 2.313566\n",
      "epoch: 1 step: 701, loss is 2.282096\n",
      "epoch: 1 step: 702, loss is 2.3158126\n",
      "epoch: 1 step: 703, loss is 2.296309\n",
      "epoch: 1 step: 704, loss is 2.311254\n",
      "epoch: 1 step: 705, loss is 2.299433\n",
      "epoch: 1 step: 706, loss is 2.2968407\n",
      "epoch: 1 step: 707, loss is 2.3086336\n",
      "epoch: 1 step: 708, loss is 2.2930486\n",
      "epoch: 1 step: 709, loss is 2.2880435\n",
      "epoch: 1 step: 710, loss is 2.299885\n",
      "epoch: 1 step: 711, loss is 2.3021512\n",
      "epoch: 1 step: 712, loss is 2.301987\n",
      "epoch: 1 step: 713, loss is 2.2984958\n",
      "epoch: 1 step: 714, loss is 2.3057222\n",
      "epoch: 1 step: 715, loss is 2.302468\n",
      "epoch: 1 step: 716, loss is 2.2890627\n",
      "epoch: 1 step: 717, loss is 2.3104205\n",
      "epoch: 1 step: 718, loss is 2.2995903\n",
      "epoch: 1 step: 719, loss is 2.3145916\n",
      "epoch: 1 step: 720, loss is 2.2896805\n",
      "epoch: 1 step: 721, loss is 2.296603\n",
      "epoch: 1 step: 722, loss is 2.2973933\n",
      "epoch: 1 step: 723, loss is 2.2999833\n",
      "epoch: 1 step: 724, loss is 2.2958355\n",
      "epoch: 1 step: 725, loss is 2.2997735\n",
      "epoch: 1 step: 726, loss is 2.317037\n",
      "epoch: 1 step: 727, loss is 2.2977173\n",
      "epoch: 1 step: 728, loss is 2.2898526\n",
      "epoch: 1 step: 729, loss is 2.310435\n",
      "epoch: 1 step: 730, loss is 2.307631\n",
      "epoch: 1 step: 731, loss is 2.2867882\n",
      "epoch: 1 step: 732, loss is 2.281994\n",
      "epoch: 1 step: 733, loss is 2.2966893\n",
      "epoch: 1 step: 734, loss is 2.3018675\n",
      "epoch: 1 step: 735, loss is 2.2860394\n",
      "epoch: 1 step: 736, loss is 2.298754\n",
      "epoch: 1 step: 737, loss is 2.3062482\n",
      "epoch: 1 step: 738, loss is 2.3039055\n",
      "epoch: 1 step: 739, loss is 2.2848814\n",
      "epoch: 1 step: 740, loss is 2.295402\n",
      "epoch: 1 step: 741, loss is 2.276783\n",
      "epoch: 1 step: 742, loss is 2.2956285\n",
      "epoch: 1 step: 743, loss is 2.2961411\n",
      "epoch: 1 step: 744, loss is 2.3043966\n",
      "epoch: 1 step: 745, loss is 2.3000724\n",
      "epoch: 1 step: 746, loss is 2.2953882\n",
      "epoch: 1 step: 747, loss is 2.2933853\n",
      "epoch: 1 step: 748, loss is 2.300092\n",
      "epoch: 1 step: 749, loss is 2.2778947\n",
      "epoch: 1 step: 750, loss is 2.30218\n",
      "epoch: 1 step: 751, loss is 2.286354\n",
      "epoch: 1 step: 752, loss is 2.2902508\n",
      "epoch: 1 step: 753, loss is 2.3030486\n",
      "epoch: 1 step: 754, loss is 2.294895\n",
      "epoch: 1 step: 755, loss is 2.3099513\n",
      "epoch: 1 step: 756, loss is 2.2886415\n",
      "epoch: 1 step: 757, loss is 2.3103507\n",
      "epoch: 1 step: 758, loss is 2.2854621\n",
      "epoch: 1 step: 759, loss is 2.2930427\n",
      "epoch: 1 step: 760, loss is 2.2970085\n",
      "epoch: 1 step: 761, loss is 2.2867317\n",
      "epoch: 1 step: 762, loss is 2.2928\n",
      "epoch: 1 step: 763, loss is 2.2854064\n",
      "epoch: 1 step: 764, loss is 2.2864804\n",
      "epoch: 1 step: 765, loss is 2.2863443\n",
      "epoch: 1 step: 766, loss is 2.3093631\n",
      "epoch: 1 step: 767, loss is 2.295768\n",
      "epoch: 1 step: 768, loss is 2.2884493\n",
      "epoch: 1 step: 769, loss is 2.2805004\n",
      "epoch: 1 step: 770, loss is 2.2826622\n",
      "epoch: 1 step: 771, loss is 2.2884295\n",
      "epoch: 1 step: 772, loss is 2.2753773\n",
      "epoch: 1 step: 773, loss is 2.2967095\n",
      "epoch: 1 step: 774, loss is 2.2611623\n",
      "epoch: 1 step: 775, loss is 2.284426\n",
      "epoch: 1 step: 776, loss is 2.297587\n",
      "epoch: 1 step: 777, loss is 2.2826943\n",
      "epoch: 1 step: 778, loss is 2.2790174\n",
      "epoch: 1 step: 779, loss is 2.2665455\n",
      "epoch: 1 step: 780, loss is 2.2768953\n",
      "epoch: 1 step: 781, loss is 2.2728658\n",
      "epoch: 1 step: 782, loss is 2.2673166\n",
      "epoch: 1 step: 783, loss is 2.2749836\n",
      "epoch: 1 step: 784, loss is 2.2695134\n",
      "epoch: 1 step: 785, loss is 2.2515798\n",
      "epoch: 1 step: 786, loss is 2.259583\n",
      "epoch: 1 step: 787, loss is 2.2509305\n",
      "epoch: 1 step: 788, loss is 2.2616186\n",
      "epoch: 1 step: 789, loss is 2.2484207\n",
      "epoch: 1 step: 790, loss is 2.2195027\n",
      "epoch: 1 step: 791, loss is 2.225703\n",
      "epoch: 1 step: 792, loss is 2.2350695\n",
      "epoch: 1 step: 793, loss is 2.2065911\n",
      "epoch: 1 step: 794, loss is 2.2341383\n",
      "epoch: 1 step: 795, loss is 2.196421\n",
      "epoch: 1 step: 796, loss is 2.211571\n",
      "epoch: 1 step: 797, loss is 2.1727755\n",
      "epoch: 1 step: 798, loss is 2.1776326\n",
      "epoch: 1 step: 799, loss is 2.138702\n",
      "epoch: 1 step: 800, loss is 2.1437232\n",
      "epoch: 1 step: 801, loss is 2.097529\n",
      "epoch: 1 step: 802, loss is 2.1482592\n",
      "epoch: 1 step: 803, loss is 2.0365183\n",
      "epoch: 1 step: 804, loss is 2.0790734\n",
      "epoch: 1 step: 805, loss is 1.9969549\n",
      "epoch: 1 step: 806, loss is 2.115068\n",
      "epoch: 1 step: 807, loss is 1.9178386\n",
      "epoch: 1 step: 808, loss is 1.8431098\n",
      "epoch: 1 step: 809, loss is 1.7614226\n",
      "epoch: 1 step: 810, loss is 1.8873127\n",
      "epoch: 1 step: 811, loss is 1.949654\n",
      "epoch: 1 step: 812, loss is 1.6604892\n",
      "epoch: 1 step: 813, loss is 1.5662594\n",
      "epoch: 1 step: 814, loss is 1.4169581\n",
      "epoch: 1 step: 815, loss is 1.322783\n",
      "epoch: 1 step: 816, loss is 1.8783584\n",
      "epoch: 1 step: 817, loss is 1.6546941\n",
      "epoch: 1 step: 818, loss is 2.3453465\n",
      "epoch: 1 step: 819, loss is 1.6246991\n",
      "epoch: 1 step: 820, loss is 1.7713223\n",
      "epoch: 1 step: 821, loss is 1.7207925\n",
      "epoch: 1 step: 822, loss is 1.4131129\n",
      "epoch: 1 step: 823, loss is 1.5729114\n",
      "epoch: 1 step: 824, loss is 1.3808072\n",
      "epoch: 1 step: 825, loss is 1.3442024\n",
      "epoch: 1 step: 826, loss is 1.7633971\n",
      "epoch: 1 step: 827, loss is 1.6115232\n",
      "epoch: 1 step: 828, loss is 1.4956601\n",
      "epoch: 1 step: 829, loss is 1.3900807\n",
      "epoch: 1 step: 830, loss is 1.528937\n",
      "epoch: 1 step: 831, loss is 1.6053663\n",
      "epoch: 1 step: 832, loss is 1.1277084\n",
      "epoch: 1 step: 833, loss is 1.1154617\n",
      "epoch: 1 step: 834, loss is 1.3854526\n",
      "epoch: 1 step: 835, loss is 1.2446134\n",
      "epoch: 1 step: 836, loss is 1.2389085\n",
      "epoch: 1 step: 837, loss is 0.9449928\n",
      "epoch: 1 step: 838, loss is 1.2425382\n",
      "epoch: 1 step: 839, loss is 1.3645402\n",
      "epoch: 1 step: 840, loss is 0.8906436\n",
      "epoch: 1 step: 841, loss is 1.3943074\n",
      "epoch: 1 step: 842, loss is 1.2149386\n",
      "epoch: 1 step: 843, loss is 0.85934234\n",
      "epoch: 1 step: 844, loss is 0.92601544\n",
      "epoch: 1 step: 845, loss is 0.692543\n",
      "epoch: 1 step: 846, loss is 1.1607939\n",
      "epoch: 1 step: 847, loss is 1.2888651\n",
      "epoch: 1 step: 848, loss is 0.87157124\n",
      "epoch: 1 step: 849, loss is 0.7347636\n",
      "epoch: 1 step: 850, loss is 0.9526169\n",
      "epoch: 1 step: 851, loss is 0.66794765\n",
      "epoch: 1 step: 852, loss is 1.0128076\n",
      "epoch: 1 step: 853, loss is 0.80777025\n",
      "epoch: 1 step: 854, loss is 0.7485663\n",
      "epoch: 1 step: 855, loss is 0.82613665\n",
      "epoch: 1 step: 856, loss is 0.86087793\n",
      "epoch: 1 step: 857, loss is 1.3084643\n",
      "epoch: 1 step: 858, loss is 1.2841797\n",
      "epoch: 1 step: 859, loss is 1.0779133\n",
      "epoch: 1 step: 860, loss is 1.0808657\n",
      "epoch: 1 step: 861, loss is 0.85474503\n",
      "epoch: 1 step: 862, loss is 0.7775774\n",
      "epoch: 1 step: 863, loss is 0.8863731\n",
      "epoch: 1 step: 864, loss is 1.0740404\n",
      "epoch: 1 step: 865, loss is 0.59034306\n",
      "epoch: 1 step: 866, loss is 0.80748117\n",
      "epoch: 1 step: 867, loss is 0.6537711\n",
      "epoch: 1 step: 868, loss is 0.6343501\n",
      "epoch: 1 step: 869, loss is 0.92781323\n",
      "epoch: 1 step: 870, loss is 1.3000813\n",
      "epoch: 1 step: 871, loss is 0.72684824\n",
      "epoch: 1 step: 872, loss is 0.63614845\n",
      "epoch: 1 step: 873, loss is 0.92195296\n",
      "epoch: 1 step: 874, loss is 0.53051126\n",
      "epoch: 1 step: 875, loss is 0.7089922\n",
      "epoch: 1 step: 876, loss is 0.57041985\n",
      "epoch: 1 step: 877, loss is 0.5822614\n",
      "epoch: 1 step: 878, loss is 0.9191224\n",
      "epoch: 1 step: 879, loss is 0.5025295\n",
      "epoch: 1 step: 880, loss is 0.8109819\n",
      "epoch: 1 step: 881, loss is 0.52547705\n",
      "epoch: 1 step: 882, loss is 0.5696578\n",
      "epoch: 1 step: 883, loss is 1.1046084\n",
      "epoch: 1 step: 884, loss is 0.8469118\n",
      "epoch: 1 step: 885, loss is 0.5098821\n",
      "epoch: 1 step: 886, loss is 0.9555789\n",
      "epoch: 1 step: 887, loss is 0.85352314\n",
      "epoch: 1 step: 888, loss is 0.491152\n",
      "epoch: 1 step: 889, loss is 0.59586644\n",
      "epoch: 1 step: 890, loss is 0.9991243\n",
      "epoch: 1 step: 891, loss is 0.6210368\n",
      "epoch: 1 step: 892, loss is 0.62743163\n",
      "epoch: 1 step: 893, loss is 0.60353345\n",
      "epoch: 1 step: 894, loss is 0.76614875\n",
      "epoch: 1 step: 895, loss is 0.8519464\n",
      "epoch: 1 step: 896, loss is 0.567157\n",
      "epoch: 1 step: 897, loss is 0.63256156\n",
      "epoch: 1 step: 898, loss is 0.494656\n",
      "epoch: 1 step: 899, loss is 0.32133907\n",
      "epoch: 1 step: 900, loss is 0.47912267\n",
      "epoch: 1 step: 901, loss is 0.5068209\n",
      "epoch: 1 step: 902, loss is 0.54320896\n",
      "epoch: 1 step: 903, loss is 1.0546591\n",
      "epoch: 1 step: 904, loss is 0.5783335\n",
      "epoch: 1 step: 905, loss is 0.23505546\n",
      "epoch: 1 step: 906, loss is 0.6403014\n",
      "epoch: 1 step: 907, loss is 0.47390336\n",
      "epoch: 1 step: 908, loss is 0.8369636\n",
      "epoch: 1 step: 909, loss is 0.38665748\n",
      "epoch: 1 step: 910, loss is 0.45996168\n",
      "epoch: 1 step: 911, loss is 0.44075498\n",
      "epoch: 1 step: 912, loss is 0.8369039\n",
      "epoch: 1 step: 913, loss is 0.12163612\n",
      "epoch: 1 step: 914, loss is 0.45692694\n",
      "epoch: 1 step: 915, loss is 1.44031\n",
      "epoch: 1 step: 916, loss is 0.42585787\n",
      "epoch: 1 step: 917, loss is 0.49998444\n",
      "epoch: 1 step: 918, loss is 0.507146\n",
      "epoch: 1 step: 919, loss is 0.5059718\n",
      "epoch: 1 step: 920, loss is 0.7509795\n",
      "epoch: 1 step: 921, loss is 0.3465192\n",
      "epoch: 1 step: 922, loss is 0.328068\n",
      "epoch: 1 step: 923, loss is 0.7435769\n",
      "epoch: 1 step: 924, loss is 0.754876\n",
      "epoch: 1 step: 925, loss is 0.4448662\n",
      "epoch: 1 step: 926, loss is 0.3925168\n",
      "epoch: 1 step: 927, loss is 0.6910807\n",
      "epoch: 1 step: 928, loss is 0.30485326\n",
      "epoch: 1 step: 929, loss is 0.37330273\n",
      "epoch: 1 step: 930, loss is 0.68037176\n",
      "epoch: 1 step: 931, loss is 0.61740994\n",
      "epoch: 1 step: 932, loss is 0.59468055\n",
      "epoch: 1 step: 933, loss is 0.5726037\n",
      "epoch: 1 step: 934, loss is 0.4526016\n",
      "epoch: 1 step: 935, loss is 0.46335334\n",
      "epoch: 1 step: 936, loss is 0.55079514\n",
      "epoch: 1 step: 937, loss is 0.28436986\n",
      "epoch: 1 step: 938, loss is 0.4407282\n",
      "epoch: 1 step: 939, loss is 0.34026143\n",
      "epoch: 1 step: 940, loss is 0.44492272\n",
      "epoch: 1 step: 941, loss is 0.37390792\n",
      "epoch: 1 step: 942, loss is 0.3457487\n",
      "epoch: 1 step: 943, loss is 0.40059337\n",
      "epoch: 1 step: 944, loss is 0.3806984\n",
      "epoch: 1 step: 945, loss is 0.44033414\n",
      "epoch: 1 step: 946, loss is 0.16927956\n",
      "epoch: 1 step: 947, loss is 0.44192603\n",
      "epoch: 1 step: 948, loss is 0.39346203\n",
      "epoch: 1 step: 949, loss is 0.62712646\n",
      "epoch: 1 step: 950, loss is 0.31183782\n",
      "epoch: 1 step: 951, loss is 0.5775218\n",
      "epoch: 1 step: 952, loss is 0.35523778\n",
      "epoch: 1 step: 953, loss is 0.36287066\n",
      "epoch: 1 step: 954, loss is 0.6066401\n",
      "epoch: 1 step: 955, loss is 0.26024267\n",
      "epoch: 1 step: 956, loss is 0.28363144\n",
      "epoch: 1 step: 957, loss is 0.1322147\n",
      "epoch: 1 step: 958, loss is 0.37929618\n",
      "epoch: 1 step: 959, loss is 0.32172903\n",
      "epoch: 1 step: 960, loss is 0.38803437\n",
      "epoch: 1 step: 961, loss is 0.6256964\n",
      "epoch: 1 step: 962, loss is 0.5555438\n",
      "epoch: 1 step: 963, loss is 0.3500975\n",
      "epoch: 1 step: 964, loss is 0.19020413\n",
      "epoch: 1 step: 965, loss is 0.8653549\n",
      "epoch: 1 step: 966, loss is 0.50468147\n",
      "epoch: 1 step: 967, loss is 0.2505315\n",
      "epoch: 1 step: 968, loss is 0.121066235\n",
      "epoch: 1 step: 969, loss is 0.3074775\n",
      "epoch: 1 step: 970, loss is 0.5512997\n",
      "epoch: 1 step: 971, loss is 0.34507528\n",
      "epoch: 1 step: 972, loss is 0.5080662\n",
      "epoch: 1 step: 973, loss is 0.3054217\n",
      "epoch: 1 step: 974, loss is 0.5018302\n",
      "epoch: 1 step: 975, loss is 0.39060116\n",
      "epoch: 1 step: 976, loss is 0.4069354\n",
      "epoch: 1 step: 977, loss is 0.55976933\n",
      "epoch: 1 step: 978, loss is 0.4308293\n",
      "epoch: 1 step: 979, loss is 0.46198702\n",
      "epoch: 1 step: 980, loss is 0.41993645\n",
      "epoch: 1 step: 981, loss is 0.38661024\n",
      "epoch: 1 step: 982, loss is 0.27007335\n",
      "epoch: 1 step: 983, loss is 0.40355486\n",
      "epoch: 1 step: 984, loss is 0.12955232\n",
      "epoch: 1 step: 985, loss is 0.33104602\n",
      "epoch: 1 step: 986, loss is 0.25495878\n",
      "epoch: 1 step: 987, loss is 0.16423386\n",
      "epoch: 1 step: 988, loss is 0.9881822\n",
      "epoch: 1 step: 989, loss is 0.17378312\n",
      "epoch: 1 step: 990, loss is 0.45837596\n",
      "epoch: 1 step: 991, loss is 0.43139887\n",
      "epoch: 1 step: 992, loss is 0.29668444\n",
      "epoch: 1 step: 993, loss is 0.75387985\n",
      "epoch: 1 step: 994, loss is 0.35023\n",
      "epoch: 1 step: 995, loss is 0.30298886\n",
      "epoch: 1 step: 996, loss is 0.12271506\n",
      "epoch: 1 step: 997, loss is 0.4411576\n",
      "epoch: 1 step: 998, loss is 0.20265682\n",
      "epoch: 1 step: 999, loss is 0.33955055\n",
      "epoch: 1 step: 1000, loss is 0.46897736\n",
      "epoch: 1 step: 1001, loss is 0.3412091\n",
      "epoch: 1 step: 1002, loss is 0.23694916\n",
      "epoch: 1 step: 1003, loss is 0.29854417\n",
      "epoch: 1 step: 1004, loss is 0.1024046\n",
      "epoch: 1 step: 1005, loss is 0.4078023\n",
      "epoch: 1 step: 1006, loss is 0.3196233\n",
      "epoch: 1 step: 1007, loss is 0.4683513\n",
      "epoch: 1 step: 1008, loss is 0.1295457\n",
      "epoch: 1 step: 1009, loss is 0.12638198\n",
      "epoch: 1 step: 1010, loss is 0.13297099\n",
      "epoch: 1 step: 1011, loss is 0.3084225\n",
      "epoch: 1 step: 1012, loss is 0.09477267\n",
      "epoch: 1 step: 1013, loss is 0.5093091\n",
      "epoch: 1 step: 1014, loss is 0.391245\n",
      "epoch: 1 step: 1015, loss is 0.05440425\n",
      "epoch: 1 step: 1016, loss is 0.2543913\n",
      "epoch: 1 step: 1017, loss is 0.102962755\n",
      "epoch: 1 step: 1018, loss is 0.11493717\n",
      "epoch: 1 step: 1019, loss is 0.17084089\n",
      "epoch: 1 step: 1020, loss is 0.18611144\n",
      "epoch: 1 step: 1021, loss is 0.49428314\n",
      "epoch: 1 step: 1022, loss is 0.2553613\n",
      "epoch: 1 step: 1023, loss is 0.12927575\n",
      "epoch: 1 step: 1024, loss is 0.2909699\n",
      "epoch: 1 step: 1025, loss is 0.44407105\n",
      "epoch: 1 step: 1026, loss is 0.081056826\n",
      "epoch: 1 step: 1027, loss is 0.30539483\n",
      "epoch: 1 step: 1028, loss is 0.3568304\n",
      "epoch: 1 step: 1029, loss is 0.21259016\n",
      "epoch: 1 step: 1030, loss is 0.1899625\n",
      "epoch: 1 step: 1031, loss is 0.34801346\n",
      "epoch: 1 step: 1032, loss is 0.13851535\n",
      "epoch: 1 step: 1033, loss is 0.10821267\n",
      "epoch: 1 step: 1034, loss is 0.2649599\n",
      "epoch: 1 step: 1035, loss is 0.28417072\n",
      "epoch: 1 step: 1036, loss is 0.14522742\n",
      "epoch: 1 step: 1037, loss is 0.07226477\n",
      "epoch: 1 step: 1038, loss is 0.17830653\n",
      "epoch: 1 step: 1039, loss is 0.30878797\n",
      "epoch: 1 step: 1040, loss is 0.45950937\n",
      "epoch: 1 step: 1041, loss is 0.12434361\n",
      "epoch: 1 step: 1042, loss is 0.15387037\n",
      "epoch: 1 step: 1043, loss is 0.027867874\n",
      "epoch: 1 step: 1044, loss is 0.14516957\n",
      "epoch: 1 step: 1045, loss is 0.14841934\n",
      "epoch: 1 step: 1046, loss is 0.7685978\n",
      "epoch: 1 step: 1047, loss is 0.2618634\n",
      "epoch: 1 step: 1048, loss is 0.08201204\n",
      "epoch: 1 step: 1049, loss is 0.4350752\n",
      "epoch: 1 step: 1050, loss is 0.16428362\n",
      "epoch: 1 step: 1051, loss is 0.1723868\n",
      "epoch: 1 step: 1052, loss is 0.27821034\n",
      "epoch: 1 step: 1053, loss is 0.29087132\n",
      "epoch: 1 step: 1054, loss is 0.07444935\n",
      "epoch: 1 step: 1055, loss is 0.4200782\n",
      "epoch: 1 step: 1056, loss is 0.51373094\n",
      "epoch: 1 step: 1057, loss is 0.17804752\n",
      "epoch: 1 step: 1058, loss is 0.36628765\n",
      "epoch: 1 step: 1059, loss is 0.20234796\n",
      "epoch: 1 step: 1060, loss is 0.15405443\n",
      "epoch: 1 step: 1061, loss is 0.21444608\n",
      "epoch: 1 step: 1062, loss is 0.17118073\n",
      "epoch: 1 step: 1063, loss is 0.2930186\n",
      "epoch: 1 step: 1064, loss is 0.27405262\n",
      "epoch: 1 step: 1065, loss is 0.6762603\n",
      "epoch: 1 step: 1066, loss is 0.26774377\n",
      "epoch: 1 step: 1067, loss is 0.17153068\n",
      "epoch: 1 step: 1068, loss is 0.37219822\n",
      "epoch: 1 step: 1069, loss is 0.26046127\n",
      "epoch: 1 step: 1070, loss is 0.11958872\n",
      "epoch: 1 step: 1071, loss is 0.14372109\n",
      "epoch: 1 step: 1072, loss is 0.60816866\n",
      "epoch: 1 step: 1073, loss is 0.42010412\n",
      "epoch: 1 step: 1074, loss is 0.2256956\n",
      "epoch: 1 step: 1075, loss is 0.36817673\n",
      "epoch: 1 step: 1076, loss is 0.24967155\n",
      "epoch: 1 step: 1077, loss is 0.15801336\n",
      "epoch: 1 step: 1078, loss is 0.41371015\n",
      "epoch: 1 step: 1079, loss is 0.11539656\n",
      "epoch: 1 step: 1080, loss is 0.12938328\n",
      "epoch: 1 step: 1081, loss is 0.053564183\n",
      "epoch: 1 step: 1082, loss is 0.341283\n",
      "epoch: 1 step: 1083, loss is 0.28631407\n",
      "epoch: 1 step: 1084, loss is 0.31409362\n",
      "epoch: 1 step: 1085, loss is 0.50250727\n",
      "epoch: 1 step: 1086, loss is 0.123546675\n",
      "epoch: 1 step: 1087, loss is 0.13844573\n",
      "epoch: 1 step: 1088, loss is 0.46986037\n",
      "epoch: 1 step: 1089, loss is 0.5047612\n",
      "epoch: 1 step: 1090, loss is 0.3329447\n",
      "epoch: 1 step: 1091, loss is 0.1916628\n",
      "epoch: 1 step: 1092, loss is 0.3788011\n",
      "epoch: 1 step: 1093, loss is 0.16452986\n",
      "epoch: 1 step: 1094, loss is 0.512108\n",
      "epoch: 1 step: 1095, loss is 0.1340217\n",
      "epoch: 1 step: 1096, loss is 0.24457306\n",
      "epoch: 1 step: 1097, loss is 0.11815532\n",
      "epoch: 1 step: 1098, loss is 0.23385578\n",
      "epoch: 1 step: 1099, loss is 0.093380906\n",
      "epoch: 1 step: 1100, loss is 0.19520675\n",
      "epoch: 1 step: 1101, loss is 0.105927885\n",
      "epoch: 1 step: 1102, loss is 0.25469592\n",
      "epoch: 1 step: 1103, loss is 0.12773293\n",
      "epoch: 1 step: 1104, loss is 0.11734712\n",
      "epoch: 1 step: 1105, loss is 0.12390253\n",
      "epoch: 1 step: 1106, loss is 0.25955954\n",
      "epoch: 1 step: 1107, loss is 0.3404825\n",
      "epoch: 1 step: 1108, loss is 0.28530163\n",
      "epoch: 1 step: 1109, loss is 0.18357918\n",
      "epoch: 1 step: 1110, loss is 0.5031979\n",
      "epoch: 1 step: 1111, loss is 0.22945505\n",
      "epoch: 1 step: 1112, loss is 0.169669\n",
      "epoch: 1 step: 1113, loss is 0.53221077\n",
      "epoch: 1 step: 1114, loss is 0.1672394\n",
      "epoch: 1 step: 1115, loss is 0.25480673\n",
      "epoch: 1 step: 1116, loss is 0.44246984\n",
      "epoch: 1 step: 1117, loss is 0.08239388\n",
      "epoch: 1 step: 1118, loss is 0.2297264\n",
      "epoch: 1 step: 1119, loss is 0.25320965\n",
      "epoch: 1 step: 1120, loss is 0.43711662\n",
      "epoch: 1 step: 1121, loss is 0.25194827\n",
      "epoch: 1 step: 1122, loss is 0.22317886\n",
      "epoch: 1 step: 1123, loss is 0.37282196\n",
      "epoch: 1 step: 1124, loss is 0.35971388\n",
      "epoch: 1 step: 1125, loss is 0.20873305\n",
      "epoch: 1 step: 1126, loss is 0.28445506\n",
      "epoch: 1 step: 1127, loss is 0.36391193\n",
      "epoch: 1 step: 1128, loss is 0.3339868\n",
      "epoch: 1 step: 1129, loss is 0.15795071\n",
      "epoch: 1 step: 1130, loss is 0.057677533\n",
      "epoch: 1 step: 1131, loss is 0.1509944\n",
      "epoch: 1 step: 1132, loss is 0.34633404\n",
      "epoch: 1 step: 1133, loss is 0.09941127\n",
      "epoch: 1 step: 1134, loss is 0.11223793\n",
      "epoch: 1 step: 1135, loss is 0.81278974\n",
      "epoch: 1 step: 1136, loss is 0.26911405\n",
      "epoch: 1 step: 1137, loss is 0.258937\n",
      "epoch: 1 step: 1138, loss is 0.10263768\n",
      "epoch: 1 step: 1139, loss is 0.3194302\n",
      "epoch: 1 step: 1140, loss is 0.57929\n",
      "epoch: 1 step: 1141, loss is 0.07850921\n",
      "epoch: 1 step: 1142, loss is 0.14159977\n",
      "epoch: 1 step: 1143, loss is 0.14103083\n",
      "epoch: 1 step: 1144, loss is 0.11576505\n",
      "epoch: 1 step: 1145, loss is 0.061307624\n",
      "epoch: 1 step: 1146, loss is 0.10513633\n",
      "epoch: 1 step: 1147, loss is 0.24056768\n",
      "epoch: 1 step: 1148, loss is 0.2927399\n",
      "epoch: 1 step: 1149, loss is 0.18528476\n",
      "epoch: 1 step: 1150, loss is 0.2349966\n",
      "epoch: 1 step: 1151, loss is 0.040936776\n",
      "epoch: 1 step: 1152, loss is 0.3774116\n",
      "epoch: 1 step: 1153, loss is 0.19840859\n",
      "epoch: 1 step: 1154, loss is 0.1290353\n",
      "epoch: 1 step: 1155, loss is 0.5070468\n",
      "epoch: 1 step: 1156, loss is 0.20207985\n",
      "epoch: 1 step: 1157, loss is 0.3392137\n",
      "epoch: 1 step: 1158, loss is 0.21134683\n",
      "epoch: 1 step: 1159, loss is 0.14249228\n",
      "epoch: 1 step: 1160, loss is 0.53406674\n",
      "epoch: 1 step: 1161, loss is 0.089897156\n",
      "epoch: 1 step: 1162, loss is 0.13943458\n",
      "epoch: 1 step: 1163, loss is 0.18014255\n",
      "epoch: 1 step: 1164, loss is 0.14150387\n",
      "epoch: 1 step: 1165, loss is 0.16508956\n",
      "epoch: 1 step: 1166, loss is 0.2988051\n",
      "epoch: 1 step: 1167, loss is 0.2464839\n",
      "epoch: 1 step: 1168, loss is 0.030765122\n",
      "epoch: 1 step: 1169, loss is 0.067719854\n",
      "epoch: 1 step: 1170, loss is 0.28847522\n",
      "epoch: 1 step: 1171, loss is 0.10238736\n",
      "epoch: 1 step: 1172, loss is 0.36290368\n",
      "epoch: 1 step: 1173, loss is 0.23872842\n",
      "epoch: 1 step: 1174, loss is 0.29888883\n",
      "epoch: 1 step: 1175, loss is 0.39550048\n",
      "epoch: 1 step: 1176, loss is 0.32572612\n",
      "epoch: 1 step: 1177, loss is 0.14062852\n",
      "epoch: 1 step: 1178, loss is 0.14141175\n",
      "epoch: 1 step: 1179, loss is 0.17858042\n",
      "epoch: 1 step: 1180, loss is 0.18512899\n",
      "epoch: 1 step: 1181, loss is 0.08106067\n",
      "epoch: 1 step: 1182, loss is 0.27341473\n",
      "epoch: 1 step: 1183, loss is 0.10997271\n",
      "epoch: 1 step: 1184, loss is 0.111172944\n",
      "epoch: 1 step: 1185, loss is 0.28969616\n",
      "epoch: 1 step: 1186, loss is 0.3696671\n",
      "epoch: 1 step: 1187, loss is 0.5666011\n",
      "epoch: 1 step: 1188, loss is 0.27293864\n",
      "epoch: 1 step: 1189, loss is 0.5101717\n",
      "epoch: 1 step: 1190, loss is 0.49156916\n",
      "epoch: 1 step: 1191, loss is 0.4230629\n",
      "epoch: 1 step: 1192, loss is 0.1579423\n",
      "epoch: 1 step: 1193, loss is 0.10872113\n",
      "epoch: 1 step: 1194, loss is 0.08554851\n",
      "epoch: 1 step: 1195, loss is 0.03453186\n",
      "epoch: 1 step: 1196, loss is 0.17097996\n",
      "epoch: 1 step: 1197, loss is 0.6251076\n",
      "epoch: 1 step: 1198, loss is 0.11387806\n",
      "epoch: 1 step: 1199, loss is 0.2281235\n",
      "epoch: 1 step: 1200, loss is 0.69871306\n",
      "epoch: 1 step: 1201, loss is 0.24044326\n",
      "epoch: 1 step: 1202, loss is 0.3249496\n",
      "epoch: 1 step: 1203, loss is 0.51998264\n",
      "epoch: 1 step: 1204, loss is 0.21774326\n",
      "epoch: 1 step: 1205, loss is 0.29125282\n",
      "epoch: 1 step: 1206, loss is 0.41505808\n",
      "epoch: 1 step: 1207, loss is 0.3997664\n",
      "epoch: 1 step: 1208, loss is 0.43460014\n",
      "epoch: 1 step: 1209, loss is 0.35361397\n",
      "epoch: 1 step: 1210, loss is 0.45321196\n",
      "epoch: 1 step: 1211, loss is 0.29073882\n",
      "epoch: 1 step: 1212, loss is 0.20526738\n",
      "epoch: 1 step: 1213, loss is 0.11302752\n",
      "epoch: 1 step: 1214, loss is 0.13720317\n",
      "epoch: 1 step: 1215, loss is 0.24188526\n",
      "epoch: 1 step: 1216, loss is 0.13389578\n",
      "epoch: 1 step: 1217, loss is 0.44809264\n",
      "epoch: 1 step: 1218, loss is 0.44946003\n",
      "epoch: 1 step: 1219, loss is 0.1689722\n",
      "epoch: 1 step: 1220, loss is 0.42562923\n",
      "epoch: 1 step: 1221, loss is 0.19514357\n",
      "epoch: 1 step: 1222, loss is 0.36422637\n",
      "epoch: 1 step: 1223, loss is 0.10596019\n",
      "epoch: 1 step: 1224, loss is 0.405204\n",
      "epoch: 1 step: 1225, loss is 0.39067304\n",
      "epoch: 1 step: 1226, loss is 0.41372582\n",
      "epoch: 1 step: 1227, loss is 0.25529557\n",
      "epoch: 1 step: 1228, loss is 0.15332872\n",
      "epoch: 1 step: 1229, loss is 0.078866825\n",
      "epoch: 1 step: 1230, loss is 0.31242958\n",
      "epoch: 1 step: 1231, loss is 0.25943387\n",
      "epoch: 1 step: 1232, loss is 0.37656483\n",
      "epoch: 1 step: 1233, loss is 0.34773013\n",
      "epoch: 1 step: 1234, loss is 0.17741299\n",
      "epoch: 1 step: 1235, loss is 0.08964566\n",
      "epoch: 1 step: 1236, loss is 0.1144797\n",
      "epoch: 1 step: 1237, loss is 0.3043221\n",
      "epoch: 1 step: 1238, loss is 0.3743184\n",
      "epoch: 1 step: 1239, loss is 0.32096887\n",
      "epoch: 1 step: 1240, loss is 0.3812036\n",
      "epoch: 1 step: 1241, loss is 0.2095582\n",
      "epoch: 1 step: 1242, loss is 0.22553721\n",
      "epoch: 1 step: 1243, loss is 0.17986465\n",
      "epoch: 1 step: 1244, loss is 0.2033335\n",
      "epoch: 1 step: 1245, loss is 0.28965637\n",
      "epoch: 1 step: 1246, loss is 0.2776925\n",
      "epoch: 1 step: 1247, loss is 0.16874613\n",
      "epoch: 1 step: 1248, loss is 0.18424442\n",
      "epoch: 1 step: 1249, loss is 0.14211813\n",
      "epoch: 1 step: 1250, loss is 0.450765\n",
      "epoch: 1 step: 1251, loss is 0.50865245\n",
      "epoch: 1 step: 1252, loss is 0.47297773\n",
      "epoch: 1 step: 1253, loss is 0.102875836\n",
      "epoch: 1 step: 1254, loss is 0.24042344\n",
      "epoch: 1 step: 1255, loss is 0.08212003\n",
      "epoch: 1 step: 1256, loss is 0.31689608\n",
      "epoch: 1 step: 1257, loss is 0.36329442\n",
      "epoch: 1 step: 1258, loss is 0.1167095\n",
      "epoch: 1 step: 1259, loss is 0.44277138\n",
      "epoch: 1 step: 1260, loss is 0.13312966\n",
      "epoch: 1 step: 1261, loss is 0.094266936\n",
      "epoch: 1 step: 1262, loss is 0.17532083\n",
      "epoch: 1 step: 1263, loss is 0.5822789\n",
      "epoch: 1 step: 1264, loss is 0.13556176\n",
      "epoch: 1 step: 1265, loss is 0.2772668\n",
      "epoch: 1 step: 1266, loss is 0.27383125\n",
      "epoch: 1 step: 1267, loss is 0.1132589\n",
      "epoch: 1 step: 1268, loss is 0.27883884\n",
      "epoch: 1 step: 1269, loss is 0.17884278\n",
      "epoch: 1 step: 1270, loss is 0.16066654\n",
      "epoch: 1 step: 1271, loss is 0.46237367\n",
      "epoch: 1 step: 1272, loss is 0.21745273\n",
      "epoch: 1 step: 1273, loss is 0.34187523\n",
      "epoch: 1 step: 1274, loss is 0.23027445\n",
      "epoch: 1 step: 1275, loss is 0.44428772\n",
      "epoch: 1 step: 1276, loss is 0.30352798\n",
      "epoch: 1 step: 1277, loss is 0.3382525\n",
      "epoch: 1 step: 1278, loss is 0.3115941\n",
      "epoch: 1 step: 1279, loss is 0.20008585\n",
      "epoch: 1 step: 1280, loss is 0.17357293\n",
      "epoch: 1 step: 1281, loss is 0.123948164\n",
      "epoch: 1 step: 1282, loss is 0.42504248\n",
      "epoch: 1 step: 1283, loss is 0.042839605\n",
      "epoch: 1 step: 1284, loss is 0.111261256\n",
      "epoch: 1 step: 1285, loss is 0.07718607\n",
      "epoch: 1 step: 1286, loss is 0.07115415\n",
      "epoch: 1 step: 1287, loss is 0.2577101\n",
      "epoch: 1 step: 1288, loss is 0.06598661\n",
      "epoch: 1 step: 1289, loss is 0.020311886\n",
      "epoch: 1 step: 1290, loss is 0.2879129\n",
      "epoch: 1 step: 1291, loss is 0.40918815\n",
      "epoch: 1 step: 1292, loss is 0.083634846\n",
      "epoch: 1 step: 1293, loss is 0.53607565\n",
      "epoch: 1 step: 1294, loss is 0.047375284\n",
      "epoch: 1 step: 1295, loss is 0.026776915\n",
      "epoch: 1 step: 1296, loss is 0.19228368\n",
      "epoch: 1 step: 1297, loss is 0.061808657\n",
      "epoch: 1 step: 1298, loss is 0.39814976\n",
      "epoch: 1 step: 1299, loss is 0.027427502\n",
      "epoch: 1 step: 1300, loss is 0.11324128\n",
      "epoch: 1 step: 1301, loss is 0.14229988\n",
      "epoch: 1 step: 1302, loss is 0.17389059\n",
      "epoch: 1 step: 1303, loss is 0.17251441\n",
      "epoch: 1 step: 1304, loss is 0.25720513\n",
      "epoch: 1 step: 1305, loss is 0.24371359\n",
      "epoch: 1 step: 1306, loss is 0.10934462\n",
      "epoch: 1 step: 1307, loss is 0.32893592\n",
      "epoch: 1 step: 1308, loss is 0.17910084\n",
      "epoch: 1 step: 1309, loss is 0.42680806\n",
      "epoch: 1 step: 1310, loss is 0.15787283\n",
      "epoch: 1 step: 1311, loss is 0.2999878\n",
      "epoch: 1 step: 1312, loss is 0.3603416\n",
      "epoch: 1 step: 1313, loss is 0.23926929\n",
      "epoch: 1 step: 1314, loss is 0.2633398\n",
      "epoch: 1 step: 1315, loss is 0.07277593\n",
      "epoch: 1 step: 1316, loss is 0.17419155\n",
      "epoch: 1 step: 1317, loss is 0.37026823\n",
      "epoch: 1 step: 1318, loss is 0.3856719\n",
      "epoch: 1 step: 1319, loss is 0.11234746\n",
      "epoch: 1 step: 1320, loss is 0.4271795\n",
      "epoch: 1 step: 1321, loss is 0.48426968\n",
      "epoch: 1 step: 1322, loss is 0.18672946\n",
      "epoch: 1 step: 1323, loss is 0.3788209\n",
      "epoch: 1 step: 1324, loss is 0.3475755\n",
      "epoch: 1 step: 1325, loss is 0.34277356\n",
      "epoch: 1 step: 1326, loss is 0.32415852\n",
      "epoch: 1 step: 1327, loss is 0.25863323\n",
      "epoch: 1 step: 1328, loss is 0.18074854\n",
      "epoch: 1 step: 1329, loss is 0.1664033\n",
      "epoch: 1 step: 1330, loss is 0.20262738\n",
      "epoch: 1 step: 1331, loss is 0.26138112\n",
      "epoch: 1 step: 1332, loss is 0.20152684\n",
      "epoch: 1 step: 1333, loss is 0.06862065\n",
      "epoch: 1 step: 1334, loss is 0.3741484\n",
      "epoch: 1 step: 1335, loss is 0.073920056\n",
      "epoch: 1 step: 1336, loss is 0.10608137\n",
      "epoch: 1 step: 1337, loss is 0.40423405\n",
      "epoch: 1 step: 1338, loss is 0.21166843\n",
      "epoch: 1 step: 1339, loss is 0.67540264\n",
      "epoch: 1 step: 1340, loss is 0.27370235\n",
      "epoch: 1 step: 1341, loss is 0.045798074\n",
      "epoch: 1 step: 1342, loss is 0.05117881\n",
      "epoch: 1 step: 1343, loss is 0.0705969\n",
      "epoch: 1 step: 1344, loss is 0.039862152\n",
      "epoch: 1 step: 1345, loss is 0.10749305\n",
      "epoch: 1 step: 1346, loss is 0.05197906\n",
      "epoch: 1 step: 1347, loss is 0.0871971\n",
      "epoch: 1 step: 1348, loss is 0.07383986\n",
      "epoch: 1 step: 1349, loss is 0.04706541\n",
      "epoch: 1 step: 1350, loss is 0.11929925\n",
      "epoch: 1 step: 1351, loss is 0.631802\n",
      "epoch: 1 step: 1352, loss is 0.06757154\n",
      "epoch: 1 step: 1353, loss is 0.35435688\n",
      "epoch: 1 step: 1354, loss is 0.315845\n",
      "epoch: 1 step: 1355, loss is 0.10134599\n",
      "epoch: 1 step: 1356, loss is 0.4411672\n",
      "epoch: 1 step: 1357, loss is 0.3197583\n",
      "epoch: 1 step: 1358, loss is 0.16068809\n",
      "epoch: 1 step: 1359, loss is 0.32894823\n",
      "epoch: 1 step: 1360, loss is 0.015481987\n",
      "epoch: 1 step: 1361, loss is 0.15795472\n",
      "epoch: 1 step: 1362, loss is 0.050714225\n",
      "epoch: 1 step: 1363, loss is 0.3040222\n",
      "epoch: 1 step: 1364, loss is 0.44932044\n",
      "epoch: 1 step: 1365, loss is 0.12598696\n",
      "epoch: 1 step: 1366, loss is 0.15507631\n",
      "epoch: 1 step: 1367, loss is 0.09936237\n",
      "epoch: 1 step: 1368, loss is 0.16993171\n",
      "epoch: 1 step: 1369, loss is 0.06380693\n",
      "epoch: 1 step: 1370, loss is 0.5255973\n",
      "epoch: 1 step: 1371, loss is 0.3498303\n",
      "epoch: 1 step: 1372, loss is 0.07846819\n",
      "epoch: 1 step: 1373, loss is 0.21183905\n",
      "epoch: 1 step: 1374, loss is 0.095155105\n",
      "epoch: 1 step: 1375, loss is 0.077946596\n",
      "epoch: 1 step: 1376, loss is 0.37408897\n",
      "epoch: 1 step: 1377, loss is 0.38733393\n",
      "epoch: 1 step: 1378, loss is 0.1254105\n",
      "epoch: 1 step: 1379, loss is 0.11475061\n",
      "epoch: 1 step: 1380, loss is 0.09945489\n",
      "epoch: 1 step: 1381, loss is 0.28945595\n",
      "epoch: 1 step: 1382, loss is 0.28777456\n",
      "epoch: 1 step: 1383, loss is 0.050984245\n",
      "epoch: 1 step: 1384, loss is 0.15514608\n",
      "epoch: 1 step: 1385, loss is 0.093229964\n",
      "epoch: 1 step: 1386, loss is 0.10492382\n",
      "epoch: 1 step: 1387, loss is 0.21192245\n",
      "epoch: 1 step: 1388, loss is 0.0934484\n",
      "epoch: 1 step: 1389, loss is 0.1666043\n",
      "epoch: 1 step: 1390, loss is 0.14711851\n",
      "epoch: 1 step: 1391, loss is 0.62835974\n",
      "epoch: 1 step: 1392, loss is 0.034710996\n",
      "epoch: 1 step: 1393, loss is 0.25066945\n",
      "epoch: 1 step: 1394, loss is 0.069164716\n",
      "epoch: 1 step: 1395, loss is 0.13573852\n",
      "epoch: 1 step: 1396, loss is 0.18505251\n",
      "epoch: 1 step: 1397, loss is 0.35603464\n",
      "epoch: 1 step: 1398, loss is 0.32536742\n",
      "epoch: 1 step: 1399, loss is 0.2337578\n",
      "epoch: 1 step: 1400, loss is 0.12025493\n",
      "epoch: 1 step: 1401, loss is 0.10730523\n",
      "epoch: 1 step: 1402, loss is 0.2376547\n",
      "epoch: 1 step: 1403, loss is 0.32349083\n",
      "epoch: 1 step: 1404, loss is 0.3082743\n",
      "epoch: 1 step: 1405, loss is 0.1800904\n",
      "epoch: 1 step: 1406, loss is 0.115133405\n",
      "epoch: 1 step: 1407, loss is 0.16325784\n",
      "epoch: 1 step: 1408, loss is 0.1772825\n",
      "epoch: 1 step: 1409, loss is 0.27179924\n",
      "epoch: 1 step: 1410, loss is 0.048275035\n",
      "epoch: 1 step: 1411, loss is 0.45613468\n",
      "epoch: 1 step: 1412, loss is 0.054517116\n",
      "epoch: 1 step: 1413, loss is 0.39720023\n",
      "epoch: 1 step: 1414, loss is 0.052841965\n",
      "epoch: 1 step: 1415, loss is 0.14369525\n",
      "epoch: 1 step: 1416, loss is 0.073611565\n",
      "epoch: 1 step: 1417, loss is 0.115727015\n",
      "epoch: 1 step: 1418, loss is 0.16306098\n",
      "epoch: 1 step: 1419, loss is 0.2207256\n",
      "epoch: 1 step: 1420, loss is 0.1329746\n",
      "epoch: 1 step: 1421, loss is 0.29456964\n",
      "epoch: 1 step: 1422, loss is 0.08577898\n",
      "epoch: 1 step: 1423, loss is 0.25536826\n",
      "epoch: 1 step: 1424, loss is 0.22727992\n",
      "epoch: 1 step: 1425, loss is 0.18817146\n",
      "epoch: 1 step: 1426, loss is 0.033383675\n",
      "epoch: 1 step: 1427, loss is 0.5283094\n",
      "epoch: 1 step: 1428, loss is 0.3534191\n",
      "epoch: 1 step: 1429, loss is 0.21654396\n",
      "epoch: 1 step: 1430, loss is 0.09432087\n",
      "epoch: 1 step: 1431, loss is 0.061158422\n",
      "epoch: 1 step: 1432, loss is 0.12004464\n",
      "epoch: 1 step: 1433, loss is 0.33935758\n",
      "epoch: 1 step: 1434, loss is 0.10315648\n",
      "epoch: 1 step: 1435, loss is 0.25825024\n",
      "epoch: 1 step: 1436, loss is 0.1678144\n",
      "epoch: 1 step: 1437, loss is 0.47459447\n",
      "epoch: 1 step: 1438, loss is 0.13126582\n",
      "epoch: 1 step: 1439, loss is 0.13531332\n",
      "epoch: 1 step: 1440, loss is 0.11817111\n",
      "epoch: 1 step: 1441, loss is 0.37162703\n",
      "epoch: 1 step: 1442, loss is 0.115969695\n",
      "epoch: 1 step: 1443, loss is 0.26396734\n",
      "epoch: 1 step: 1444, loss is 0.5598451\n",
      "epoch: 1 step: 1445, loss is 0.12500213\n",
      "epoch: 1 step: 1446, loss is 0.19099085\n",
      "epoch: 1 step: 1447, loss is 0.22434522\n",
      "epoch: 1 step: 1448, loss is 0.2752494\n",
      "epoch: 1 step: 1449, loss is 0.17685032\n",
      "epoch: 1 step: 1450, loss is 0.052688718\n",
      "epoch: 1 step: 1451, loss is 0.14988899\n",
      "epoch: 1 step: 1452, loss is 0.08720945\n",
      "epoch: 1 step: 1453, loss is 0.11588818\n",
      "epoch: 1 step: 1454, loss is 0.19573903\n",
      "epoch: 1 step: 1455, loss is 0.10028536\n",
      "epoch: 1 step: 1456, loss is 0.2770977\n",
      "epoch: 1 step: 1457, loss is 0.3730066\n",
      "epoch: 1 step: 1458, loss is 0.3096216\n",
      "epoch: 1 step: 1459, loss is 0.11934225\n",
      "epoch: 1 step: 1460, loss is 0.14683758\n",
      "epoch: 1 step: 1461, loss is 0.1903706\n",
      "epoch: 1 step: 1462, loss is 0.15262592\n",
      "epoch: 1 step: 1463, loss is 0.08000363\n",
      "epoch: 1 step: 1464, loss is 0.072930425\n",
      "epoch: 1 step: 1465, loss is 0.08593485\n",
      "epoch: 1 step: 1466, loss is 0.043588813\n",
      "epoch: 1 step: 1467, loss is 0.3407275\n",
      "epoch: 1 step: 1468, loss is 0.2218855\n",
      "epoch: 1 step: 1469, loss is 0.039528016\n",
      "epoch: 1 step: 1470, loss is 0.13673384\n",
      "epoch: 1 step: 1471, loss is 0.23832549\n",
      "epoch: 1 step: 1472, loss is 0.289913\n",
      "epoch: 1 step: 1473, loss is 0.03574769\n",
      "epoch: 1 step: 1474, loss is 0.028916504\n",
      "epoch: 1 step: 1475, loss is 0.2910205\n",
      "epoch: 1 step: 1476, loss is 0.13754967\n",
      "epoch: 1 step: 1477, loss is 0.02069127\n",
      "epoch: 1 step: 1478, loss is 0.112767845\n",
      "epoch: 1 step: 1479, loss is 0.088213086\n",
      "epoch: 1 step: 1480, loss is 0.13034736\n",
      "epoch: 1 step: 1481, loss is 0.24812405\n",
      "epoch: 1 step: 1482, loss is 0.17066944\n",
      "epoch: 1 step: 1483, loss is 0.29521647\n",
      "epoch: 1 step: 1484, loss is 0.21457677\n",
      "epoch: 1 step: 1485, loss is 0.06705454\n",
      "epoch: 1 step: 1486, loss is 0.26406258\n",
      "epoch: 1 step: 1487, loss is 0.21706712\n",
      "epoch: 1 step: 1488, loss is 0.11257946\n",
      "epoch: 1 step: 1489, loss is 0.12577918\n",
      "epoch: 1 step: 1490, loss is 0.24610955\n",
      "epoch: 1 step: 1491, loss is 0.17301339\n",
      "epoch: 1 step: 1492, loss is 0.020925617\n",
      "epoch: 1 step: 1493, loss is 0.07704173\n",
      "epoch: 1 step: 1494, loss is 0.15228\n",
      "epoch: 1 step: 1495, loss is 0.029312534\n",
      "epoch: 1 step: 1496, loss is 0.04514688\n",
      "epoch: 1 step: 1497, loss is 0.08731983\n",
      "epoch: 1 step: 1498, loss is 0.18157525\n",
      "epoch: 1 step: 1499, loss is 0.11853466\n",
      "epoch: 1 step: 1500, loss is 0.19061504\n",
      "epoch: 1 step: 1501, loss is 0.3644039\n",
      "epoch: 1 step: 1502, loss is 0.046943832\n",
      "epoch: 1 step: 1503, loss is 0.29765597\n",
      "epoch: 1 step: 1504, loss is 0.03743837\n",
      "epoch: 1 step: 1505, loss is 0.19519214\n",
      "epoch: 1 step: 1506, loss is 0.5239403\n",
      "epoch: 1 step: 1507, loss is 0.13383943\n",
      "epoch: 1 step: 1508, loss is 0.48209542\n",
      "epoch: 1 step: 1509, loss is 0.27354974\n",
      "epoch: 1 step: 1510, loss is 0.17086871\n",
      "epoch: 1 step: 1511, loss is 0.08303521\n",
      "epoch: 1 step: 1512, loss is 0.21345\n",
      "epoch: 1 step: 1513, loss is 0.070997566\n",
      "epoch: 1 step: 1514, loss is 0.13123502\n",
      "epoch: 1 step: 1515, loss is 0.10222988\n",
      "epoch: 1 step: 1516, loss is 0.19319566\n",
      "epoch: 1 step: 1517, loss is 0.18104647\n",
      "epoch: 1 step: 1518, loss is 0.16905649\n",
      "epoch: 1 step: 1519, loss is 0.208814\n",
      "epoch: 1 step: 1520, loss is 0.09830721\n",
      "epoch: 1 step: 1521, loss is 0.2381494\n",
      "epoch: 1 step: 1522, loss is 0.21158907\n",
      "epoch: 1 step: 1523, loss is 0.20306262\n",
      "epoch: 1 step: 1524, loss is 0.08141655\n",
      "epoch: 1 step: 1525, loss is 0.062445763\n",
      "epoch: 1 step: 1526, loss is 0.43988067\n",
      "epoch: 1 step: 1527, loss is 0.088068634\n",
      "epoch: 1 step: 1528, loss is 0.21499735\n",
      "epoch: 1 step: 1529, loss is 0.24952093\n",
      "epoch: 1 step: 1530, loss is 0.19509234\n",
      "epoch: 1 step: 1531, loss is 0.27778015\n",
      "epoch: 1 step: 1532, loss is 0.078432165\n",
      "epoch: 1 step: 1533, loss is 0.27153215\n",
      "epoch: 1 step: 1534, loss is 0.26564646\n",
      "epoch: 1 step: 1535, loss is 0.2912028\n",
      "epoch: 1 step: 1536, loss is 0.094479255\n",
      "epoch: 1 step: 1537, loss is 0.11353329\n",
      "epoch: 1 step: 1538, loss is 0.41604763\n",
      "epoch: 1 step: 1539, loss is 0.16271386\n",
      "epoch: 1 step: 1540, loss is 0.2367987\n",
      "epoch: 1 step: 1541, loss is 0.1332614\n",
      "epoch: 1 step: 1542, loss is 0.104412414\n",
      "epoch: 1 step: 1543, loss is 0.12689416\n",
      "epoch: 1 step: 1544, loss is 0.17542125\n",
      "epoch: 1 step: 1545, loss is 0.16167933\n",
      "epoch: 1 step: 1546, loss is 0.2760571\n",
      "epoch: 1 step: 1547, loss is 0.20416318\n",
      "epoch: 1 step: 1548, loss is 0.29937482\n",
      "epoch: 1 step: 1549, loss is 0.2628082\n",
      "epoch: 1 step: 1550, loss is 0.30989638\n",
      "epoch: 1 step: 1551, loss is 0.10073456\n",
      "epoch: 1 step: 1552, loss is 0.08387393\n",
      "epoch: 1 step: 1553, loss is 0.06538818\n",
      "epoch: 1 step: 1554, loss is 0.17118055\n",
      "epoch: 1 step: 1555, loss is 0.18550837\n",
      "epoch: 1 step: 1556, loss is 0.029755697\n",
      "epoch: 1 step: 1557, loss is 0.16304694\n",
      "epoch: 1 step: 1558, loss is 0.029668394\n",
      "epoch: 1 step: 1559, loss is 0.28071383\n",
      "epoch: 1 step: 1560, loss is 0.15643857\n",
      "epoch: 1 step: 1561, loss is 0.11804331\n",
      "epoch: 1 step: 1562, loss is 0.39190575\n",
      "epoch: 1 step: 1563, loss is 0.08042642\n",
      "epoch: 1 step: 1564, loss is 0.07708642\n",
      "epoch: 1 step: 1565, loss is 0.27111292\n",
      "epoch: 1 step: 1566, loss is 0.13076216\n",
      "epoch: 1 step: 1567, loss is 0.18644974\n",
      "epoch: 1 step: 1568, loss is 0.21012516\n",
      "epoch: 1 step: 1569, loss is 0.397973\n",
      "epoch: 1 step: 1570, loss is 0.069583036\n",
      "epoch: 1 step: 1571, loss is 0.124043174\n",
      "epoch: 1 step: 1572, loss is 0.16358864\n",
      "epoch: 1 step: 1573, loss is 0.24389434\n",
      "epoch: 1 step: 1574, loss is 0.4344964\n",
      "epoch: 1 step: 1575, loss is 0.03195203\n",
      "epoch: 1 step: 1576, loss is 0.15070932\n",
      "epoch: 1 step: 1577, loss is 0.07006734\n",
      "epoch: 1 step: 1578, loss is 0.13989508\n",
      "epoch: 1 step: 1579, loss is 0.37157387\n",
      "epoch: 1 step: 1580, loss is 0.29492813\n",
      "epoch: 1 step: 1581, loss is 0.055367615\n",
      "epoch: 1 step: 1582, loss is 0.14074282\n",
      "epoch: 1 step: 1583, loss is 0.32997584\n",
      "epoch: 1 step: 1584, loss is 0.10310009\n",
      "epoch: 1 step: 1585, loss is 0.3222878\n",
      "epoch: 1 step: 1586, loss is 0.055129033\n",
      "epoch: 1 step: 1587, loss is 0.4183457\n",
      "epoch: 1 step: 1588, loss is 0.1249273\n",
      "epoch: 1 step: 1589, loss is 0.3760226\n",
      "epoch: 1 step: 1590, loss is 0.29149944\n",
      "epoch: 1 step: 1591, loss is 0.27015513\n",
      "epoch: 1 step: 1592, loss is 0.3042557\n",
      "epoch: 1 step: 1593, loss is 0.20122416\n",
      "epoch: 1 step: 1594, loss is 0.3103624\n",
      "epoch: 1 step: 1595, loss is 0.17564979\n",
      "epoch: 1 step: 1596, loss is 0.2515337\n",
      "epoch: 1 step: 1597, loss is 0.29169372\n",
      "epoch: 1 step: 1598, loss is 0.21254885\n",
      "epoch: 1 step: 1599, loss is 0.064448915\n",
      "epoch: 1 step: 1600, loss is 0.09478714\n",
      "epoch: 1 step: 1601, loss is 0.13812196\n",
      "epoch: 1 step: 1602, loss is 0.32812992\n",
      "epoch: 1 step: 1603, loss is 0.14554878\n",
      "epoch: 1 step: 1604, loss is 0.41471696\n",
      "epoch: 1 step: 1605, loss is 0.23496257\n",
      "epoch: 1 step: 1606, loss is 0.069476776\n",
      "epoch: 1 step: 1607, loss is 0.09954287\n",
      "epoch: 1 step: 1608, loss is 0.29476395\n",
      "epoch: 1 step: 1609, loss is 0.069185\n",
      "epoch: 1 step: 1610, loss is 0.12455644\n",
      "epoch: 1 step: 1611, loss is 0.23956344\n",
      "epoch: 1 step: 1612, loss is 0.027341088\n",
      "epoch: 1 step: 1613, loss is 0.16421014\n",
      "epoch: 1 step: 1614, loss is 0.12678474\n",
      "epoch: 1 step: 1615, loss is 0.24282815\n",
      "epoch: 1 step: 1616, loss is 0.07549445\n",
      "epoch: 1 step: 1617, loss is 0.11726262\n",
      "epoch: 1 step: 1618, loss is 0.18407974\n",
      "epoch: 1 step: 1619, loss is 0.027500562\n",
      "epoch: 1 step: 1620, loss is 0.07483549\n",
      "epoch: 1 step: 1621, loss is 0.20298977\n",
      "epoch: 1 step: 1622, loss is 0.2175414\n",
      "epoch: 1 step: 1623, loss is 0.35984933\n",
      "epoch: 1 step: 1624, loss is 0.37265056\n",
      "epoch: 1 step: 1625, loss is 0.22580026\n",
      "epoch: 1 step: 1626, loss is 0.010105163\n",
      "epoch: 1 step: 1627, loss is 0.19747013\n",
      "epoch: 1 step: 1628, loss is 0.17816186\n",
      "epoch: 1 step: 1629, loss is 0.026357822\n",
      "epoch: 1 step: 1630, loss is 0.22547537\n",
      "epoch: 1 step: 1631, loss is 0.067420945\n",
      "epoch: 1 step: 1632, loss is 0.31902358\n",
      "epoch: 1 step: 1633, loss is 0.1921083\n",
      "epoch: 1 step: 1634, loss is 0.10761356\n",
      "epoch: 1 step: 1635, loss is 0.11012117\n",
      "epoch: 1 step: 1636, loss is 0.12379742\n",
      "epoch: 1 step: 1637, loss is 0.17140695\n",
      "epoch: 1 step: 1638, loss is 0.26034164\n",
      "epoch: 1 step: 1639, loss is 0.2995533\n",
      "epoch: 1 step: 1640, loss is 0.03297362\n",
      "epoch: 1 step: 1641, loss is 0.31139013\n",
      "epoch: 1 step: 1642, loss is 0.14821051\n",
      "epoch: 1 step: 1643, loss is 0.22826028\n",
      "epoch: 1 step: 1644, loss is 0.14885375\n",
      "epoch: 1 step: 1645, loss is 0.15916605\n",
      "epoch: 1 step: 1646, loss is 0.08476709\n",
      "epoch: 1 step: 1647, loss is 0.11093648\n",
      "epoch: 1 step: 1648, loss is 0.30493784\n",
      "epoch: 1 step: 1649, loss is 0.08953209\n",
      "epoch: 1 step: 1650, loss is 0.32665023\n",
      "epoch: 1 step: 1651, loss is 0.18699439\n",
      "epoch: 1 step: 1652, loss is 0.11107866\n",
      "epoch: 1 step: 1653, loss is 0.34993023\n",
      "epoch: 1 step: 1654, loss is 0.2542051\n",
      "epoch: 1 step: 1655, loss is 0.08346854\n",
      "epoch: 1 step: 1656, loss is 0.17027962\n",
      "epoch: 1 step: 1657, loss is 0.119322166\n",
      "epoch: 1 step: 1658, loss is 0.18032996\n",
      "epoch: 1 step: 1659, loss is 0.24005918\n",
      "epoch: 1 step: 1660, loss is 0.12121758\n",
      "epoch: 1 step: 1661, loss is 0.34319225\n",
      "epoch: 1 step: 1662, loss is 0.11106263\n",
      "epoch: 1 step: 1663, loss is 0.097085334\n",
      "epoch: 1 step: 1664, loss is 0.11479594\n",
      "epoch: 1 step: 1665, loss is 0.14179003\n",
      "epoch: 1 step: 1666, loss is 0.21099997\n",
      "epoch: 1 step: 1667, loss is 0.3221765\n",
      "epoch: 1 step: 1668, loss is 0.07394659\n",
      "epoch: 1 step: 1669, loss is 0.039287746\n",
      "epoch: 1 step: 1670, loss is 0.27630833\n",
      "epoch: 1 step: 1671, loss is 0.11286908\n",
      "epoch: 1 step: 1672, loss is 0.114948474\n",
      "epoch: 1 step: 1673, loss is 0.25535923\n",
      "epoch: 1 step: 1674, loss is 0.06565524\n",
      "epoch: 1 step: 1675, loss is 0.0920072\n",
      "epoch: 1 step: 1676, loss is 0.2385478\n",
      "epoch: 1 step: 1677, loss is 0.053550962\n",
      "epoch: 1 step: 1678, loss is 0.16641748\n",
      "epoch: 1 step: 1679, loss is 0.20534885\n",
      "epoch: 1 step: 1680, loss is 0.13910626\n",
      "epoch: 1 step: 1681, loss is 0.11252219\n",
      "epoch: 1 step: 1682, loss is 0.07968205\n",
      "epoch: 1 step: 1683, loss is 0.038957424\n",
      "epoch: 1 step: 1684, loss is 0.26535916\n",
      "epoch: 1 step: 1685, loss is 0.15753233\n",
      "epoch: 1 step: 1686, loss is 0.24570248\n",
      "epoch: 1 step: 1687, loss is 0.22861801\n",
      "epoch: 1 step: 1688, loss is 0.016721353\n",
      "epoch: 1 step: 1689, loss is 0.3711078\n",
      "epoch: 1 step: 1690, loss is 0.4645063\n",
      "epoch: 1 step: 1691, loss is 0.36361283\n",
      "epoch: 1 step: 1692, loss is 0.41809332\n",
      "epoch: 1 step: 1693, loss is 0.047545776\n",
      "epoch: 1 step: 1694, loss is 0.25461206\n",
      "epoch: 1 step: 1695, loss is 0.09580818\n",
      "epoch: 1 step: 1696, loss is 0.17586039\n",
      "epoch: 1 step: 1697, loss is 0.07459151\n",
      "epoch: 1 step: 1698, loss is 0.288898\n",
      "epoch: 1 step: 1699, loss is 0.17214362\n",
      "epoch: 1 step: 1700, loss is 0.16127582\n",
      "epoch: 1 step: 1701, loss is 0.060178652\n",
      "epoch: 1 step: 1702, loss is 0.21252382\n",
      "epoch: 1 step: 1703, loss is 0.18971\n",
      "epoch: 1 step: 1704, loss is 0.07962133\n",
      "epoch: 1 step: 1705, loss is 0.12796369\n",
      "epoch: 1 step: 1706, loss is 0.23809265\n",
      "epoch: 1 step: 1707, loss is 0.19663885\n",
      "epoch: 1 step: 1708, loss is 0.06086895\n",
      "epoch: 1 step: 1709, loss is 0.09468615\n",
      "epoch: 1 step: 1710, loss is 0.25615907\n",
      "epoch: 1 step: 1711, loss is 0.31315544\n",
      "epoch: 1 step: 1712, loss is 0.058840666\n",
      "epoch: 1 step: 1713, loss is 0.29592472\n",
      "epoch: 1 step: 1714, loss is 0.307419\n",
      "epoch: 1 step: 1715, loss is 0.052813917\n",
      "epoch: 1 step: 1716, loss is 0.16498435\n",
      "epoch: 1 step: 1717, loss is 0.2661754\n",
      "epoch: 1 step: 1718, loss is 0.14394335\n",
      "epoch: 1 step: 1719, loss is 0.03000108\n",
      "epoch: 1 step: 1720, loss is 0.04994378\n",
      "epoch: 1 step: 1721, loss is 0.12616812\n",
      "epoch: 1 step: 1722, loss is 0.23347588\n",
      "epoch: 1 step: 1723, loss is 0.032790277\n",
      "epoch: 1 step: 1724, loss is 0.03364016\n",
      "epoch: 1 step: 1725, loss is 0.5088002\n",
      "epoch: 1 step: 1726, loss is 0.17006624\n",
      "epoch: 1 step: 1727, loss is 0.029274143\n",
      "epoch: 1 step: 1728, loss is 0.19939673\n",
      "epoch: 1 step: 1729, loss is 0.026353274\n",
      "epoch: 1 step: 1730, loss is 0.06381926\n",
      "epoch: 1 step: 1731, loss is 0.17745738\n",
      "epoch: 1 step: 1732, loss is 0.13682653\n",
      "epoch: 1 step: 1733, loss is 0.40301704\n",
      "epoch: 1 step: 1734, loss is 0.193586\n",
      "epoch: 1 step: 1735, loss is 0.15972866\n",
      "epoch: 1 step: 1736, loss is 0.216931\n",
      "epoch: 1 step: 1737, loss is 0.039635606\n",
      "epoch: 1 step: 1738, loss is 0.15397796\n",
      "epoch: 1 step: 1739, loss is 0.098494746\n",
      "epoch: 1 step: 1740, loss is 0.011458352\n",
      "epoch: 1 step: 1741, loss is 0.048442002\n",
      "epoch: 1 step: 1742, loss is 0.16524705\n",
      "epoch: 1 step: 1743, loss is 0.01150594\n",
      "epoch: 1 step: 1744, loss is 0.16159515\n",
      "epoch: 1 step: 1745, loss is 0.22783223\n",
      "epoch: 1 step: 1746, loss is 0.036683574\n",
      "epoch: 1 step: 1747, loss is 0.030235928\n",
      "epoch: 1 step: 1748, loss is 0.17648\n",
      "epoch: 1 step: 1749, loss is 0.006986331\n",
      "epoch: 1 step: 1750, loss is 0.16913864\n",
      "epoch: 1 step: 1751, loss is 0.1114797\n",
      "epoch: 1 step: 1752, loss is 0.08105351\n",
      "epoch: 1 step: 1753, loss is 0.4865072\n",
      "epoch: 1 step: 1754, loss is 0.27757493\n",
      "epoch: 1 step: 1755, loss is 0.16147617\n",
      "epoch: 1 step: 1756, loss is 0.12652422\n",
      "epoch: 1 step: 1757, loss is 0.19581088\n",
      "epoch: 1 step: 1758, loss is 0.37795052\n",
      "epoch: 1 step: 1759, loss is 0.13498671\n",
      "epoch: 1 step: 1760, loss is 0.033461343\n",
      "epoch: 1 step: 1761, loss is 0.09513603\n",
      "epoch: 1 step: 1762, loss is 0.058470447\n",
      "epoch: 1 step: 1763, loss is 0.068255164\n",
      "epoch: 1 step: 1764, loss is 0.32029185\n",
      "epoch: 1 step: 1765, loss is 0.38945243\n",
      "epoch: 1 step: 1766, loss is 0.14299615\n",
      "epoch: 1 step: 1767, loss is 0.10107185\n",
      "epoch: 1 step: 1768, loss is 0.03360427\n",
      "epoch: 1 step: 1769, loss is 0.036207955\n",
      "epoch: 1 step: 1770, loss is 0.11875492\n",
      "epoch: 1 step: 1771, loss is 0.15450665\n",
      "epoch: 1 step: 1772, loss is 0.09809557\n",
      "epoch: 1 step: 1773, loss is 0.023175882\n",
      "epoch: 1 step: 1774, loss is 0.056851834\n",
      "epoch: 1 step: 1775, loss is 0.13480175\n",
      "epoch: 1 step: 1776, loss is 0.030399088\n",
      "epoch: 1 step: 1777, loss is 0.0982062\n",
      "epoch: 1 step: 1778, loss is 0.06099522\n",
      "epoch: 1 step: 1779, loss is 0.032840174\n",
      "epoch: 1 step: 1780, loss is 0.11812971\n",
      "epoch: 1 step: 1781, loss is 0.18395974\n",
      "epoch: 1 step: 1782, loss is 0.11766694\n",
      "epoch: 1 step: 1783, loss is 0.04587852\n",
      "epoch: 1 step: 1784, loss is 0.11095729\n",
      "epoch: 1 step: 1785, loss is 0.13071896\n",
      "epoch: 1 step: 1786, loss is 0.16715041\n",
      "epoch: 1 step: 1787, loss is 0.052474633\n",
      "epoch: 1 step: 1788, loss is 0.4877681\n",
      "epoch: 1 step: 1789, loss is 0.0794527\n",
      "epoch: 1 step: 1790, loss is 0.036605492\n",
      "epoch: 1 step: 1791, loss is 0.017847292\n",
      "epoch: 1 step: 1792, loss is 0.21889044\n",
      "epoch: 1 step: 1793, loss is 0.11705706\n",
      "epoch: 1 step: 1794, loss is 0.086345576\n",
      "epoch: 1 step: 1795, loss is 0.009568223\n",
      "epoch: 1 step: 1796, loss is 0.21392138\n",
      "epoch: 1 step: 1797, loss is 0.028465962\n",
      "epoch: 1 step: 1798, loss is 0.10600714\n",
      "epoch: 1 step: 1799, loss is 0.14746848\n",
      "epoch: 1 step: 1800, loss is 0.22177942\n",
      "epoch: 1 step: 1801, loss is 0.25701594\n",
      "epoch: 1 step: 1802, loss is 0.08778616\n",
      "epoch: 1 step: 1803, loss is 0.18759173\n",
      "epoch: 1 step: 1804, loss is 0.026639566\n",
      "epoch: 1 step: 1805, loss is 0.049131975\n",
      "epoch: 1 step: 1806, loss is 0.09192606\n",
      "epoch: 1 step: 1807, loss is 0.104861826\n",
      "epoch: 1 step: 1808, loss is 0.04958271\n",
      "epoch: 1 step: 1809, loss is 0.38066208\n",
      "epoch: 1 step: 1810, loss is 0.15859523\n",
      "epoch: 1 step: 1811, loss is 0.023831332\n",
      "epoch: 1 step: 1812, loss is 0.066514395\n",
      "epoch: 1 step: 1813, loss is 0.1419433\n",
      "epoch: 1 step: 1814, loss is 0.4506274\n",
      "epoch: 1 step: 1815, loss is 0.017089838\n",
      "epoch: 1 step: 1816, loss is 0.04518575\n",
      "epoch: 1 step: 1817, loss is 0.074014306\n",
      "epoch: 1 step: 1818, loss is 0.05257067\n",
      "epoch: 1 step: 1819, loss is 0.3264218\n",
      "epoch: 1 step: 1820, loss is 0.10709732\n",
      "epoch: 1 step: 1821, loss is 0.10662168\n",
      "epoch: 1 step: 1822, loss is 0.075797364\n",
      "epoch: 1 step: 1823, loss is 0.085059546\n",
      "epoch: 1 step: 1824, loss is 0.1615236\n",
      "epoch: 1 step: 1825, loss is 0.13852598\n",
      "epoch: 1 step: 1826, loss is 0.0863316\n",
      "epoch: 1 step: 1827, loss is 0.10572633\n",
      "epoch: 1 step: 1828, loss is 0.20009339\n",
      "epoch: 1 step: 1829, loss is 0.04651812\n",
      "epoch: 1 step: 1830, loss is 0.3043043\n",
      "epoch: 1 step: 1831, loss is 0.035488326\n",
      "epoch: 1 step: 1832, loss is 0.07847328\n",
      "epoch: 1 step: 1833, loss is 0.067277536\n",
      "epoch: 1 step: 1834, loss is 0.27580974\n",
      "epoch: 1 step: 1835, loss is 0.06480168\n",
      "epoch: 1 step: 1836, loss is 0.021113042\n",
      "epoch: 1 step: 1837, loss is 0.086231254\n",
      "epoch: 1 step: 1838, loss is 0.01858722\n",
      "epoch: 1 step: 1839, loss is 0.07386793\n",
      "epoch: 1 step: 1840, loss is 0.050863232\n",
      "epoch: 1 step: 1841, loss is 0.19947708\n",
      "epoch: 1 step: 1842, loss is 0.015373258\n",
      "epoch: 1 step: 1843, loss is 0.02295683\n",
      "epoch: 1 step: 1844, loss is 0.039390244\n",
      "epoch: 1 step: 1845, loss is 0.10433718\n",
      "epoch: 1 step: 1846, loss is 0.6598892\n",
      "epoch: 1 step: 1847, loss is 0.11062658\n",
      "epoch: 1 step: 1848, loss is 0.007316492\n",
      "epoch: 1 step: 1849, loss is 0.02995275\n",
      "epoch: 1 step: 1850, loss is 0.022295695\n",
      "epoch: 1 step: 1851, loss is 0.11736388\n",
      "epoch: 1 step: 1852, loss is 0.2880928\n",
      "epoch: 1 step: 1853, loss is 0.018674253\n",
      "epoch: 1 step: 1854, loss is 0.13956642\n",
      "epoch: 1 step: 1855, loss is 0.060267426\n",
      "epoch: 1 step: 1856, loss is 0.0663595\n",
      "epoch: 1 step: 1857, loss is 0.24403329\n",
      "epoch: 1 step: 1858, loss is 0.025257137\n",
      "epoch: 1 step: 1859, loss is 0.06415175\n",
      "epoch: 1 step: 1860, loss is 0.15574008\n",
      "epoch: 1 step: 1861, loss is 0.14517386\n",
      "epoch: 1 step: 1862, loss is 0.14862712\n",
      "epoch: 1 step: 1863, loss is 0.00718774\n",
      "epoch: 1 step: 1864, loss is 0.028858269\n",
      "epoch: 1 step: 1865, loss is 0.0033063972\n",
      "epoch: 1 step: 1866, loss is 0.15685405\n",
      "epoch: 1 step: 1867, loss is 0.37809345\n",
      "epoch: 1 step: 1868, loss is 0.08586388\n",
      "epoch: 1 step: 1869, loss is 0.18781342\n",
      "epoch: 1 step: 1870, loss is 0.021245858\n",
      "epoch: 1 step: 1871, loss is 0.19507441\n",
      "epoch: 1 step: 1872, loss is 0.2810352\n",
      "epoch: 1 step: 1873, loss is 0.08578612\n",
      "epoch: 1 step: 1874, loss is 0.1706382\n",
      "epoch: 1 step: 1875, loss is 0.081777535\n",
      "{'Accuracy': 0.9547275641025641}\n"
     ]
    }
   ],
   "source": [
    "train_epoch = 1\n",
    "mnist_path = \"./datasets/MNIST_Data\"\n",
    "dataset_size = 1\n",
    "model = Model(net, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "train_net(args, model, train_epoch, mnist_path, dataset_size, ckpoint, False)\n",
    "test_net(net, model, mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "使用以下命令运行脚本：\n",
    "\n",
    "```bash\n",
    "python lenet.py --device_target=CPU\n",
    "```\n",
    "\n",
    "其中，  \n",
    "`lenet.py`：为你根据教程编写的脚本文件。  \n",
    "`--device_target=CPU`：指定运行硬件平台，参数为`CPU`、`GPU`或者`Ascend`，根据你的实际运行硬件平台来指定。\n",
    "\n",
    "训练过程中会打印loss值，类似下图。loss值会波动，但总体来说loss值会逐步减小，精度逐步提高。每个人运行的loss值有一定随机性，不一定完全相同。\n",
    "训练过程中loss打印示例如下：\n",
    "\n",
    "```bash\n",
    "epoch: 1 step: 1, loss is 2.3025916\n",
    "epoch: 1 step: 2, loss is 2.302577\n",
    "...\n",
    "epoch: 1 step: 1871, loss is 0.048939988\n",
    "epoch: 1 step: 1872, loss is 0.028885357\n",
    "epoch: 1 step: 1873, loss is 0.09475248\n",
    "epoch: 1 step: 1874, loss is 0.046067055\n",
    "epoch: 1 step: 1875, loss is 0.12366105\n",
    "{'Accuracy': 0.9663477564102564}\n",
    "```\n",
    "\n",
    "可以在打印信息中看出模型精度数据，示例中精度数据达到96.6%，模型质量良好。随着网络迭代次数`train_epoch`增加，模型精度会进一步提高。\n",
    "\n",
    "## 加载模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "# 加载已经保存的用于测试的模型\n",
    "param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
    "# 加载参数到网络中\n",
    "load_param_into_net(net, param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">阅读更多有关[MindSpore加载模型](https://www.mindspore.cn/tutorial/training/zh-CN/master/quick_start/quick_start/xxx.html)的信息。\n",
    "\n",
    "## 验证模型\n",
    "\n",
    "加载模型后，可以使用该模型进行预测。\n",
    "\n",
    "```python\n",
    "classes = [\n",
    "    \"Zero\",\n",
    "    \"One\",\n",
    "    \"Two\",\n",
    "    \"Three\",\n",
    "    \"Four\",\n",
    "    \"Fives\",\n",
    "    \"Six\",\n",
    "    \"Seven\",\n",
    "    \"Eight\",\n",
    "    \"Nine\",\n",
    "]\n",
    "\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "pred = model(x)\n",
    "predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "```\n",
    "\n",
    "运行结果示例如下：\n",
    "\n",
    "```text\n",
    "Predicted: \"Eight\", Actual: \"Eight\"\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
