{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "mindspore1.2",
   "display_name": "MindSpore1.2",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 使用PyNative模式调试\n",
    "\n",
    "## 概述\n",
    "\n",
    "MindSpore支持两种运行模式，在调试或者运行方面做了不同的优化:\n",
    "\n",
    "- PyNative模式：也称动态图模式，将神经网络中的各个算子逐一下发执行，方便用户编写和调试神经网络模型。\n",
    "- Graph模式：也称静态图模式或者图模式，将神经网络模型编译成一整张图，然后下发执行。该模式利用图优化等技术提高运行性能，同时有助于规模部署和跨平台运行。\n",
    "\n",
    "默认情况下，MindSpore处于PyNative模式，可以通过`context.set_context(mode=context.GRAPH_MODE)`切换为Graph模式；同样地，MindSpore处于Graph模式时，可以通过 `context.set_context(mode=context.PYNATIVE_MODE)`切换为PyNative模式。\n",
    "\n",
    "PyNative模式下，支持执行单算子、普通函数和网络，以及单独求梯度的操作。下面将详细介绍使用方法和注意事项。\n",
    "\n",
    "> PyNative模式下为了提升性能，算子在device上使用了异步执行方式，因此在算子执行错误的时候，错误信息可能会在程序执行到最后才显示。\n",
    ">\n",
    "> PyNative不支持summary功能，图模式summary相关算子不能使用。\n",
    "\n",
    "## 执行单算子\n",
    "\n",
    "执行单个算子，并打印相关结果，如下例所示。\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context, Tensor\n",
    "from mindspore import ParameterTuple\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "from mindspore.nn import Dense, WithLossCell, SoftmaxCrossEntropyWithLogits, Momentum\n",
    "from mindspore.common.initializer import Normal\n",
    "import mindspore.ops as ops\n",
    "from mindspore import ms_function\n",
    "from mindspore import dtype as mstype\n",
    "\n",
    "# 切换为PyNative模式\n",
    "context.set_context(mode=context.PYNATIVE_MODE, device_target=\"GPU\")\n",
    "\n",
    "# 打印Cond2d算子的输出\n",
    "conv = nn.Conv2d(3, 4, 3, bias_init='zeros')\n",
    "input_data = Tensor(np.ones([1, 3, 5, 5]).astype(np.float32))\n",
    "output = conv(input_data)\n",
    "print(output.asnumpy())"
   ]
  },
  {
   "source": [
    "## 执行普通函数\n",
    "\n",
    "将若干算子组合成一个函数，然后直接通过函数调用的方式执行这些算子，并打印相关结果，如下例所示。\n",
    "\n",
    "> 本例中`add`以普通PyNative方法执行"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义add函数\n",
    "def add_func(x, y):\n",
    "    z = ops.add(x, y)\n",
    "    z = ops.add(z, x)\n",
    "    return z\n",
    "\n",
    "x = Tensor(np.ones([3, 3], dtype=np.float32))\n",
    "y = Tensor(np.ones([3, 3], dtype=np.float32))\n",
    "output = add_func(x, y)\n",
    "print(output.asnumpy())"
   ]
  },
  {
   "source": [
    "### 提升PyNative性能\n",
    "\n",
    "为了提高PyNative模式下的前向计算任务执行速度，MindSpore提供了Staging功能，该功能可以在PyNative模式下将Python函数或者Python类的方法编译成计算图，通过图优化等技术提高运行速度，是一种混合运行机制。Staging功能的使用通过`ms_function`装饰器达成，该装饰器会将将模块编译成计算图，在给定输入之后，以图的形式下发执行。如下例所示：\n",
    "\n",
    "> 本例中`add`以Staging模式执行\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入ms_function\n",
    "from mindspore import ms_function\n",
    "\n",
    "# 仍设定为PyNative模式\n",
    "context.set_context(mode=context.PYNATIVE_MODE, device_target=\"GPU\")\n",
    "\n",
    "add = ops.Add()\n",
    "\n",
    "# 使用装饰器编译计算图\n",
    "@ms_function\n",
    "def add_fn(x, y):\n",
    "    res = add(x, y)\n",
    "    return res\n",
    "\n",
    "x = Tensor(np.ones([4, 4]).astype(np.float32))\n",
    "y = Tensor(np.ones([4, 4]).astype(np.float32))\n",
    "z = add_fn(x, y)\n",
    "print(z.asnumpy())"
   ]
  },
  {
   "source": [
    "在加装了`ms_function`装饰器的函数中，如果包含不需要进行参数训练的算子（如`pooling`、`add`等算子），则这些算子可以在被装饰的函数中直接调用，如上例所示。如果被装饰的函数中包含了需要进行参数训练的算子（如`Convolution`、`BatchNorm`等算子），则这些算子必须在被装饰等函数之外完成实例化操作。\n",
    "\n",
    "示例代码：\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d实例化操作\n",
    "conv_obj = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, stride=2, padding=0)\n",
    "conv_obj.init_parameters_data()\n",
    "\n",
    "@ms_function\n",
    "def conv_fn(x):\n",
    "    res = conv_obj(x)\n",
    "    return res\n",
    "\n",
    "input_data = np.random.randn(2, 3, 6, 6).astype(np.float32)\n",
    "z = conv_fn(Tensor(input_data))\n",
    "print(z.asnumpy())"
   ]
  },
  {
   "source": [
    "## 调试网络训练模型\n",
    "\n",
    "PyNative模式下，还可以支持单独求梯度的操作。如下例所示，可通过`GradOperation`求该函数或者网络所有的输入梯度。需要注意，输入类型仅支持Tensor。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(x, y):\n",
    "    return x * y\n",
    "\n",
    "def mainf(x, y):\n",
    "    return ops.GradOperation(get_all=True)(mul)(x, y)\n",
    "\n",
    "print(mainf(Tensor(1, mstype.int32), Tensor(2, mstype.int32)))"
   ]
  },
  {
   "source": [
    "在进行网络训练时，求得梯度然后调用优化器对参数进行优化（暂不支持在反向计算梯度的过程中设置断点），然后再利用前向计算loss，从而实现在PyNative模式下进行网络训练。\n",
    "\n",
    "下面在PyNative模式下进行LeNet训练：\n",
    "\n",
    "1. 构建网络，该部分内容可以参考[快速入门-初学入门](https://www.mindspore.cn/tutorial/zh-CN/r1.2/quick_start.html)对应内容。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.PYNATIVE_MODE, device_target=\"GPU\")\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"\n",
    "    Lenet网络结构\n",
    "    \"\"\"\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # 定义所需要的运算\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 使用定义好的运算构建前向网络\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 实例化网络\n",
    "net = LeNet5()"
   ]
  },
  {
   "source": [
    "2. 如上文所说，利用`GradOperation`求函数的输入梯度。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradWrap(nn.Cell):\n",
    "    \"\"\"求函数输入梯度\"\"\"\n",
    "    def __init__(self, network):\n",
    "        super(GradWrap, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        # 用Tuple的形式包装weight\n",
    "        self.weights = ParameterTuple(filter(lambda x: x.requires_grad, network.get_parameters()))\n",
    "\n",
    "    def construct(self, x, label):\n",
    "        weights = self.weights\n",
    "        # 返回值为梯度\n",
    "        return ops.GradOperation(get_by_list=True)(self.network, weights)(x, label)"
   ]
  },
  {
   "source": [
    "3. 在PyNative模式中进行网络训练。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定优化器、损失函数\n",
    "optimizer = Momentum(filter(lambda x: x.requires_grad, net.get_parameters()), 0.1, 0.9)\n",
    "criterion = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "# 通过WithLossCell获取Loss值\n",
    "net_with_criterion = WithLossCell(net, criterion)\n",
    "# 调用GradWrap\n",
    "train_network = GradWrap(net_with_criterion)\n",
    "train_network.set_train()\n",
    "\n",
    "# 产生输入数据\n",
    "input_data = Tensor(np.ones([32, 1, 32, 32]).astype(np.float32) * 0.01)\n",
    "label = Tensor(np.ones([32]).astype(np.int32))\n",
    "output = net(Tensor(input_data))\n",
    "\n",
    "# 利用前向网络计算loss\n",
    "loss_output = criterion(output, label)\n",
    "# 求得梯度\n",
    "grads = train_network(input_data, label)\n",
    "# 优化参数\n",
    "success = optimizer(grads)\n",
    "loss = loss_output.asnumpy()\n",
    "print(loss)"
   ]
  }
 ]
}