{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText实现文本分类\n",
    "\n",
    "[![](https://gitee.com/mindspore/docs/raw/r1.2/resource/_static/logo_source.png)](https://gitee.com/mindspore/docs/blob/r1.2/tutorials/source_zh_cn/intermediate/text/text_sentiment_ngrams_tutorial.ipynb)&emsp;[![](https://gitee.com/mindspore/docs/raw/r1.2/resource/_static/logo_notebook.png)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r1.2/quick_start/mindspore_text_sentiment_ngrams_tutorial.ipynb)&emsp;[![](https://gitee.com/mindspore/docs/raw/r1.2/resource/_static/logo_modelarts.png)](https://authoring-modelarts-cnnorth4.huaweicloud.com/console/lab?share-url-b64=aHR0cHM6Ly9taW5kc3BvcmUtd2Vic2l0ZS5vYnMuY24tbm9ydGgtNC5teWh1YXdlaWNsb3VkLmNvbS9ub3RlYm9vay9tYXN0ZXIvdHV0b3JpYWxzL3poX2NuL21pbmRzcG9yZV90ZXh0X3NlbnRpbWVudF9uZ3JhbXNfdHV0b3JpYWwuaXB5bmI=&imageid=65f636a0-56cf-49df-b941-7d2a07ba8c8c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本教程中，我们将在MindSpore中使用`MindRecord`加载并构建文本数据集，用户可以从教程中了解到如何：\n",
    "\n",
    "- 创建迭代数据集\n",
    "- 将文本转换为向量\n",
    "- 对数据进行shuffle等操作\n",
    "\n",
    "此外，本教程使用N-Gram，即N元语法模型来判断语句单词的构成顺序。N-Gram可以按照字节顺序，将文本内容进行大小为N的划窗操作，最终形成长度为N的字节片段序列。实践中经常使用二元或三元模型，本例通过将`ngram`参数设定为2，将二元模型应用在文本分类案例中。\n",
    "\n",
    ">注意：该教程环境为MindSpore1.2.0版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "本教程采用了AG_NEWS数据集，该数据集拥有超过 100 万篇新闻文章，该数据集仅采用了标题和描述字段，每一条数据有三列，第一列为标签（label），第二列为题目（title），第三列为内容（content），每种类别均拥有 30000 个训练样本和 1900 个测试样本。\n",
    "\n",
    "点击下载[文本分类AG_NEWS数据集](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/intermediate/ag_news_csv.tgz) ，在教程的同级目录下新建`data`文件夹，将下载好的数据集存放在`data`中。\n",
    "\n",
    "目录如下：\n",
    "```text\n",
    "project\n",
    "│  text_sentiment_ngrams_tutorial.ipynb      \n",
    "└─data\n",
    "   │   train.csv\n",
    "   │   test.csv\n",
    "```\n",
    "在进行其他操作之前，需要先安装`sklearn`和`spacy`工具包，并导入所需要的库并进行参数设置。\n",
    "\n",
    "可在Jupyter Notebook中执行以下代码，完成工具包的安装及数据集的下载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn spacy -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!mkdir -p ./data\n",
    "!wget -N https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/intermediate/ag_news_csv.tgz\n",
    "!tar -zxvf ag_news_csv.tgz\n",
    "!mv ./ag_news_csv/* ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import ast\n",
    "import html\n",
    "\n",
    "import mindspore.common.dtype as mstype\n",
    "import mindspore.dataset.transforms.c_transforms as deC\n",
    "from mindspore import nn\n",
    "from mindspore import context\n",
    "from mindspore import dataset as ds\n",
    "import mindspore.ops.operations as P\n",
    "from mindspore.ops import composite as C\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from mindspore import Tensor, Model, ParameterTuple\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.common.initializer import XavierUniform\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本教程我们在GPU环境下，使用图模式运行实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE,save_graphs=False,device_target=\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据\n",
    "\n",
    "定义`_get_bucket_length`和`generate_gram`函数，分别实现如下功能：\n",
    "\n",
    "- _get_bucket_length：将对应长度词句分类到相应的词句桶中。\n",
    "\n",
    "- generate_gram：为词句提供分词功能，此教程中取值为2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bucket_length(x, bts):\n",
    "    # 返回词句对应的长度\n",
    "    x_len = len(x)\n",
    "    \n",
    "    for index in range(1, len(bts)):\n",
    "        if bts[index - 1] < x_len <= bts[index]:\n",
    "            return bts[index]\n",
    "    return bts[0]\n",
    "\n",
    "\n",
    "def generate_gram(words, num=2):\n",
    "    # 生成步长为2的词\n",
    "    return [' '.join(words[i: i + num]) for i in range(len(words) - num + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始处理数据前，需要先定义填词的标志位，同时也需要创建保存向量和单词相互转换的两个字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = dict()\n",
    "vec2words = dict()\n",
    "word2vec['PAD'] = 0\n",
    "vec2words[0] = 'PAD'\n",
    "word2vec['UNK'] = 1\n",
    "vec2words[1] = 'UNK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义`input_preprocess`函数，首先将需要处理的文本拼接在一起，然后拼接的文本通过`spacy_nlp`来对词进行标注，然后通过实例化`generate_gram`将词条按照步长切分为词语，最后返回词语转为向量的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_preprocess(src_text1, src_text2, spacy_nlp, train_mode):\n",
    "    \"\"\"数据处理函数\"\"\"\n",
    "    non_str = '\\\\'\n",
    "    end_string = ['.', '?', '!']\n",
    "    str_html = re.compile(r'<[^>]+>')\n",
    "    src_text1 = src_text1.strip()\n",
    "    text_greater = '>'\n",
    "    text_less = '<'\n",
    "\n",
    "    # 拼接文本\n",
    "    if src_text1 and src_text1[-1] not in end_string:\n",
    "        src_text1 = src_text1 + '.'\n",
    "\n",
    "    if src_text2:\n",
    "        src_text2 = src_text2.strip()\n",
    "        sent_describe = src_text1 + ' ' + src_text2\n",
    "    else:\n",
    "        sent_describe = src_text1\n",
    "    if non_str in sent_describe:\n",
    "        sent_describe = sent_describe.replace(non_str, ' ')\n",
    "\n",
    "    sent_describe = html.unescape(sent_describe)\n",
    "\n",
    "    if text_less in sent_describe and text_greater in sent_describe:\n",
    "        sent_describe = str_html.sub('', sent_describe)\n",
    "\n",
    "    # 转换词向量\n",
    "    doc = spacy_nlp(sent_describe)\n",
    "    bows_token = [token.text for token in doc]\n",
    "\n",
    "    try:\n",
    "        tagged_sent_desc = '<p> ' + ' </s> '.join([s.text for s in doc.sents]) + ' </p>'\n",
    "    except ValueError:\n",
    "        tagged_sent_desc = '<p> ' + sent_describe + ' </p>'\n",
    "    doc = spacy_nlp(tagged_sent_desc)\n",
    "    ngrams = generate_gram([token.text for token in doc], num=2)\n",
    "    bo_ngrams = bows_token + ngrams\n",
    "\n",
    "    if train_mode is True:\n",
    "        for ngms in bo_ngrams:\n",
    "            idx = word2vec.get(ngms)\n",
    "            if idx is None:\n",
    "                idx = len(word2vec)\n",
    "                word2vec[ngms] = idx\n",
    "                vec2words[idx] = ngms\n",
    "                \n",
    "    # 返回转为向量后的列表\n",
    "    processed_out = [word2vec[ng] if ng in word2vec else word2vec['UNK'] for ng in bo_ngrams]\n",
    "\n",
    "    return processed_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义`common_block`函数来完成以下功能：\n",
    "- 读取每条新闻信息的标签（label）。\n",
    "- 根据每条新闻信息的长度分别进行相对应的处理。\n",
    "- 获取`input_preprocess`函数得到的token长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_block(_pair_sen, spacy_nlp):\n",
    "    \"\"\"新闻信息处理函数\"\"\"\n",
    "    label_idx = int(_pair_sen[0]) - 1\n",
    "\n",
    "    # 根据不同文本长度来处理数据\n",
    "    if len(_pair_sen) == 3:\n",
    "        src_tokens = input_preprocess(src_text1=_pair_sen[1],\n",
    "                                      src_text2=_pair_sen[2],\n",
    "                                      spacy_nlp=spacy_nlp,\n",
    "                                      train_mode=True)\n",
    "        src_tokens_length = len(src_tokens)\n",
    "    elif len(_pair_sen) == 2:\n",
    "        src_tokens = input_preprocess(src_text1=_pair_sen[1],\n",
    "                                      src_text2=None,\n",
    "                                      spacy_nlp=spacy_nlp,\n",
    "                                      train_mode=True)\n",
    "        src_tokens_length = len(src_tokens)\n",
    "    elif len(_pair_sen) == 4:\n",
    "        # 判断是否有三个文本，如果有的话先拼接前两个文本\n",
    "        if _pair_sen[2]:\n",
    "            sen_o_t = _pair_sen[1] + ' ' + _pair_sen[2]\n",
    "        else:\n",
    "            sen_o_t = _pair_sen[1]\n",
    "        src_tokens = input_preprocess(src_text1=sen_o_t,\n",
    "                                      src_text2=_pair_sen[3],\n",
    "                                      spacy_nlp=spacy_nlp,\n",
    "                                      train_mode=True)\n",
    "        src_tokens_length = len(src_tokens)\n",
    "    return src_tokens, src_tokens_length, label_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后来定义`load`函数来形成数据处理流，包括如下步骤：\n",
    "1. 首先将处理后的数据集文件以列表的形式保存。\n",
    "2. 根据前面数据分箱后的长度来将不足的长度进行填充。\n",
    "3. 将处理后的数据保存为列表返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(train_path, test_path, train_feature_dict, test_feature_dict):\n",
    "    \"\"\"数据读取\"\"\"\n",
    "    train_dataset_list = []\n",
    "    test_dataset_list = []\n",
    "    spacy_nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner', 'lemmatizer'])\n",
    "    spacy_nlp.add_pipe('sentencizer')\n",
    "\n",
    "    with open(train_path, 'r', newline='', encoding='utf-8') as src_file:\n",
    "        print(\"开始处理训练数据\")\n",
    "        reader = csv.reader(src_file, delimiter=\",\", quotechar='\"')\n",
    "        for _, _pair_sen in enumerate(reader):\n",
    "            src_tokens, src_tokens_length, label_idx = common_block(_pair_sen=_pair_sen,\n",
    "                                                                    spacy_nlp=spacy_nlp)\n",
    "            train_dataset_list.append([src_tokens, src_tokens_length, label_idx])\n",
    "\n",
    "    with open(test_path, 'r', newline='', encoding='utf-8') as test_file:\n",
    "        print(\"开始处理测试数据\")\n",
    "        reader2 = csv.reader(test_file, delimiter=\",\", quotechar='\"')\n",
    "        for _, _test_sen in enumerate(reader2):\n",
    "            src_tokens, src_tokens_length, label_idx = common_block(_pair_sen=_test_sen,\n",
    "                                                                    spacy_nlp=spacy_nlp)\n",
    "            test_dataset_list.append([src_tokens, src_tokens_length, label_idx])\n",
    "\n",
    "    train_dataset_list_length = len(train_dataset_list)\n",
    "    test_dataset_list_length = len(test_dataset_list)\n",
    "\n",
    "    # 用定义的word2vec['PAD']为训练数据填充0\n",
    "    for l in range(train_dataset_list_length):\n",
    "        bucket_length = _get_bucket_length(train_dataset_list[l][0], [64, 128, 467])\n",
    "        while len(train_dataset_list[l][0]) < bucket_length:\n",
    "            train_dataset_list[l][0].append(word2vec['PAD'])\n",
    "        train_dataset_list[l][1] = len(train_dataset_list[l][0])\n",
    "\n",
    "    # 用定义的word2vec['PAD']为测试数据填充0\n",
    "    for j in range(test_dataset_list_length):\n",
    "        test_bucket_length = _get_bucket_length(test_dataset_list[j][0], [64, 128, 467])\n",
    "        while len(test_dataset_list[j][0]) < test_bucket_length:\n",
    "            test_dataset_list[j][0].append(word2vec['PAD'])\n",
    "        test_dataset_list[j][1] = len(test_dataset_list[j][0])\n",
    "\n",
    "    train_example_data = []\n",
    "    test_example_data = []\n",
    "\n",
    "    # 将训练样例以字典的方式存入\n",
    "    for idx in range(train_dataset_list_length):\n",
    "        train_example_data.append({\n",
    "            \"src_tokens\": train_dataset_list[idx][0],\n",
    "            \"src_tokens_length\": train_dataset_list[idx][1],\n",
    "            \"label_idx\": train_dataset_list[idx][2],\n",
    "        })\n",
    "        for key in train_feature_dict:\n",
    "            if key == train_example_data[idx]['src_tokens_length']:\n",
    "                train_feature_dict[key].append(train_example_data[idx])\n",
    "\n",
    "    #将测试样例以字典的方式存入\n",
    "    for h in range(test_dataset_list_length):\n",
    "        test_example_data.append({\n",
    "            \"src_tokens\": test_dataset_list[h][0],\n",
    "            \"src_tokens_length\": test_dataset_list[h][1],\n",
    "            \"label_idx\": test_dataset_list[h][2],\n",
    "        })\n",
    "        for key in test_feature_dict:\n",
    "            if key == test_example_data[h]['src_tokens_length']:\n",
    "                test_feature_dict[key].append(test_example_data[h])\n",
    "\n",
    "    print(\"train vocab size is \", len(word2vec))\n",
    "    return train_feature_dict, test_feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成预处理数据\n",
    "\n",
    "现在调用上一步定义好的`load`函数，获取训练与测试的预处理数据，以便于下一步使用`mindspore.dataset.MindRecord`接口进一步转换数据格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理训练数据\n",
      "开始处理测试数据\n",
      "train vocab size is  1071957\n"
     ]
    }
   ],
   "source": [
    "train_feature_dicts = {}\n",
    "# 通过循环将bucket中的长度都加载到空字典\n",
    "for i in [64, 128, 467]:\n",
    "    train_feature_dicts[i] = []\n",
    "test_feature_dicts = {}\n",
    "for i in [64, 128, 467]:\n",
    "    test_feature_dicts[i] = []\n",
    "data_path = \"./data/\"\n",
    "# 读取bucket的test和train数据进行处理\n",
    "train_data_example, test_data_example = load(train_path=os.path.join(data_path, \"train.csv\"),\n",
    "                                             test_path=os.path.join(data_path, \"test.csv\"),\n",
    "                                            train_feature_dict = train_feature_dicts,\n",
    "                                            test_feature_dict = test_feature_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成MindRecord转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们通过定义`write_to_mindrecord`方法来将预处理后的基本数据转换为MindRecord格式，该方法提供两个参数：\n",
    "\n",
    "- data：AG_NEWS数据集的路径。\n",
    "\n",
    "- path：定义生成MindRecord格式文件路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_mindrecord(data, path, shared_num=1):\n",
    "    \"\"\"生成MindRecord\"\"\"\n",
    "    if not os.path.isabs(path):\n",
    "        path = os.path.abspath(path)\n",
    "\n",
    "    writer = FileWriter(path, shared_num)\n",
    "    data_schema = {\n",
    "        \"src_tokens\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "        \"src_tokens_length\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "        \"label_idx\": {\"type\": \"int32\", \"shape\": [-1]}\n",
    "    }\n",
    "    writer.add_schema(data_schema, \"fasttext\")\n",
    "    \n",
    "    for item in data:\n",
    "        item['src_tokens'] = np.array(item['src_tokens'], dtype=np.int32)\n",
    "        item['src_tokens_length'] = np.array(item['src_tokens_length'], dtype=np.int32)\n",
    "        item['label_idx'] = np.array(item['label_idx'], dtype=np.int32)\n",
    "        writer.write_raw_data([item])\n",
    "    writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遍历原始数据集，将所有数据全部写为MindRecord数据格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train data to MindRecord file.....\n",
      "Writing test data to MindRecord file.....\n"
     ]
    }
   ],
   "source": [
    "# 通过循环来将文件转换成拼接的MindRecord文件\n",
    "print(\"Writing train data to MindRecord file.....\")\n",
    "for i in [64, 128, 467]:\n",
    "    write_to_mindrecord(train_data_example[i], './train/train_dataset_bs_' + str(i) + '.mindrecord', 1)\n",
    "print(\"Writing test data to MindRecord file.....\")\n",
    "for k in [64, 128, 467]:\n",
    "    write_to_mindrecord(test_data_example[k], './test/test_dataset_bs_' + str(k) + '.mindrecord', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成统一数据集\n",
    "\n",
    "经过`write_to_mindrecord`，现在我们已经得到了全部数据的MindRecord格式的数据集，接下来进一步调用`batch_per_bucket`将所有数据合并到统一数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_per_bucket(bucket_length, input_file):\n",
    "    # 拼接MindRecord文件\n",
    "    input_file = input_file + 'train/train_dataset_bs_' + str(bucket_length) + '.mindrecord'\n",
    "    # 判断是否存在文件\n",
    "    if not input_file:\n",
    "        raise FileNotFoundError(\"input file parameter must not be empty.\")\n",
    "        \n",
    "    # 按照文件格式读取文件数据\n",
    "    data_set = ds.MindDataset(input_file,\n",
    "                              columns_list=['src_tokens', 'src_tokens_length', 'label_idx'],\n",
    "                              shuffle=True,\n",
    "                              num_shards=1,\n",
    "                              shard_id=0,\n",
    "                              num_parallel_workers=4)\n",
    "    \n",
    "    # 返回文件数据\n",
    "    ori_dataset_size = data_set.get_dataset_size()\n",
    "    print(f\"Dataset size: {ori_dataset_size}\")\n",
    "    repeat_count = 1\n",
    "    data_set = data_set.rename(input_columns=['src_tokens', 'src_tokens_length', 'label_idx'],\n",
    "                               output_columns=['src_token_text', 'src_tokens_text_length', 'label_idx_tag'])\n",
    "    data_set = data_set.batch(batch_size=512, drop_remainder=False)\n",
    "    data_set = data_set.repeat(repeat_count)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成训练数据\n",
    "通过循环方式来遍历所有MindRecord文件，最后完成训练数据集的收集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4780\n",
      "Dataset size: 73255\n",
      "Dataset size: 6706\n"
     ]
    }
   ],
   "source": [
    "bucket=[64,128,467]\n",
    "for i, _ in enumerate(bucket):\n",
    "    bucket_len = bucket[i]\n",
    "    ds_per = batch_per_bucket(bucket_len, input_file=\"\")\n",
    "    \n",
    "    # 判断次序来拼接数据\n",
    "    if i == 0:\n",
    "        data_set = ds_per\n",
    "    else:\n",
    "        data_set = data_set + ds_per\n",
    "        \n",
    "data_set = data_set.shuffle(data_set.get_dataset_size())\n",
    "data_set.channel_name = 'fasttext'\n",
    "preprocessed_data = data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "论文[Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)中详细阐述了FastText模型的实现原理，模型结构如图所示：\n",
    "\n",
    "![fasttext](images/fasttext.png)\n",
    "\n",
    "> 图片出处：https://arxiv.org/pdf/1607.01759.pdf 。\n",
    "\n",
    "可以看到，FastText模型只有三层：输入层、隐藏层（hidden层）、输出层，输入是多个向量表示的单词，输出是一个特定的target，隐藏层（hidden层）是对多个词向量的叠加平均。\n",
    "> FastText在输入时，将单词的字符级别的n-gram向量作为额外的特征；在输出时，FastText采用了分层Softmax，大大降低了模型训练时间。\n",
    "\n",
    "下面定义FastText网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Cell):\n",
    "    def __init__(self, vocab_size, embedding_dims, num_class):\n",
    "        \"\"\"定义FastText网络\"\"\"\n",
    "        super(FastText, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_dims = embedding_dims\n",
    "        self.num_class = num_class\n",
    "        self.embeding_func = nn.Embedding(vocab_size=self.vocab_size,\n",
    "                                          embedding_size=self.embeding_dims,\n",
    "                                          padding_idx=0, embedding_table='Zeros')\n",
    "        self.fc = nn.Dense(self.embeding_dims, out_channels=self.num_class,\n",
    "                           weight_init=XavierUniform(1)).to_float(mstype.float16)\n",
    "        self.reducesum = P.ReduceSum()\n",
    "        self.cast = P.Cast()\n",
    "        self.realdiv = P.RealDiv()\n",
    "\n",
    "    def construct(self, src_tokens, src_token_length):\n",
    "        \"\"\" FastText网络构建 \"\"\"\n",
    "        src_tokens = self.embeding_func(src_tokens)\n",
    "        embeding = self.reducesum(src_tokens, 1)\n",
    "        embeding = self.realdiv(embeding, src_token_length)\n",
    "        embeding = self.cast(embeding, mstype.float16)\n",
    "        classifier = self.fc(embeding)\n",
    "        classifier = self.cast(classifier, mstype.float32)\n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启动实例\n",
    "\n",
    "`AG_NEWS`数据集具有四个标签，因此类别数是四个。\n",
    "\n",
    "```py\n",
    "1 : World\n",
    "2 : Sports\n",
    "3 : Business\n",
    "4 : Sci/Tec\n",
    "\n",
    "```\n",
    "\n",
    "在网络中，`vocab_size`为词汇数据的长度，其中包括单个单词和N元组。类的数量等于标签的数量，在`AG_NEWS`情况下为4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_net = FastText(1383812, 16, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "我们在此处使用MindSpore数据集接口`MindDataset`加载`AG_NEWS`数据集，并将其发送到模型以进行训练/验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提供FastTextloss计算\n",
    "\n",
    "我们已经在前面定义了一个完整的`FastText`网络，现在需要来为网络提供一个计算loss值的方法，这一过程由`FastTextNetWithLoss`类来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextNetWithLoss(nn.Cell):\n",
    "    \"\"\"\n",
    "    提供FastText的loss运算\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, vocab_size, embedding_dims, num_class):\n",
    "        super(FastTextNetWithLoss, self).__init__()\n",
    "        self.fasttext = network\n",
    "        self.loss_func = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "        self.squeeze = P.Squeeze(axis=1)\n",
    "        self.print = P.Print()\n",
    "\n",
    "    def construct(self, src_tokens, src_tokens_lengths, label_idx):\n",
    "        \"\"\"\n",
    "        带有loss的FastText网络\n",
    "        \"\"\"\n",
    "        predict_score = self.fasttext(src_tokens, src_tokens_lengths)\n",
    "        label_idx = self.squeeze(label_idx)\n",
    "        predict_score = self.loss_func(predict_score, label_idx)\n",
    "\n",
    "        return predict_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建网络计算loss值\n",
    "\n",
    "在这一步中实例化`FastTextNetWithLoss`类。将定义好的网络`FastTextNet`、vocab的大小、embedding的数量和类别数放入到实例中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Parameter (name=fasttext.embeding_func.embedding_table, shape=(1383812, 16), dtype=Float32, requires_grad=True): Parameter (name=fasttext.embeding_func.embedding_table, shape=(1383812, 16), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fasttext.fc.weight, shape=(4, 16), dtype=Float32, requires_grad=True): Parameter (name=fasttext.fc.weight, shape=(4, 16), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fasttext.fc.bias, shape=(4,), dtype=Float32, requires_grad=True): Parameter (name=fasttext.fc.bias, shape=(4,), dtype=Float32, requires_grad=True)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_with_loss = FastTextNetWithLoss(fast_text_net, 1383812, 16, 4)\n",
    "net_with_loss.init_parameters_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置学习率和优化器\n",
    "\n",
    "现在我们需要为`mindspore.nn.Adam`优化器来定义一个学习率变化方式，以此来为优化器提供所需学习率参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mindspore.common.tensor.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from mindspore.nn import Adam\n",
    "from mindspore.nn import piecewise_constant_lr\n",
    "\n",
    "# 定义学习率和学习率变化曲线\n",
    "learn_rate = 0.2\n",
    "min_lr = 0.000001\n",
    "decay_steps = preprocessed_data.get_dataset_size()\n",
    "update_steps = 5 * preprocessed_data.get_dataset_size()\n",
    "lr_step = [i+1 for i in range(update_steps)]\n",
    "lr_list = [learn_rate - min_lr * i for i in range(update_steps)]\n",
    "lr = Tensor(piecewise_constant_lr(lr_step,lr_list), dtype=mstype.float32)\n",
    "print(type(lr))\n",
    "\n",
    "# 实例化优化器\n",
    "optimizer = Adam(net_with_loss.trainable_params(), lr, beta1=0.9, beta2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练pipeline\n",
    "\n",
    "当所有准备完毕后，我们要规划一次训练所需要的pipeline，于是定义了`TrainOneStepCell`类，该类主要实现以下方法：\n",
    "\n",
    "set_sens：将获取值转为sens类型方便后续传入`tuple_to_array`转换。\n",
    "\n",
    "construct：定义一次训练结算所需要的流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextTrainOneStepCell(nn.Cell):\n",
    "\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(FastTextTrainOneStepCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.weights = ParameterTuple(network.trainable_params())\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = C.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "        self.reducer_flag = False\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        \n",
    "        # 判断计算模式合理性\n",
    "        if self.parallel_mode not in ParallelMode.MODE_LIST:\n",
    "            raise ValueError(\"Parallel mode does not support: \", self.parallel_mode)\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        self.grad_reducer = None\n",
    "        # 如果递减成立，则自动检索所需模式\n",
    "        if self.reducer_flag:\n",
    "            mean = context.get_auto_parallel_context(\"gradients_mean\")\n",
    "            degree = get_group_size()\n",
    "            self.grad_reducer = DistributedGradReducer(optimizer.parameters, mean, degree)\n",
    "        # 将算子应用到每个网络序列中\n",
    "        self.hyper_map = C.HyperMap()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def set_sens(self, value):\n",
    "        self.sens = value\n",
    "\n",
    "    def construct(self,\n",
    "                  src_token_text,\n",
    "                  src_tokens_text_length,\n",
    "                  label_idx_tag):\n",
    "        \"\"\"定义执行运算.\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(src_token_text,\n",
    "                            src_tokens_text_length,\n",
    "                            label_idx_tag)\n",
    "        grads = self.grad(self.network, weights)(src_token_text,\n",
    "                                                 src_tokens_text_length,\n",
    "                                                 label_idx_tag,\n",
    "                                                 self.cast(F.tuple_to_array((self.sens,)),\n",
    "                                                           mstype.float32))\n",
    "        grads = self.hyper_map(F.partial(clip_grad, GRADIENT_CLIP_TYPE, GRADIENT_CLIP_VALUE), grads)\n",
    "        \n",
    "        # 实现梯度消除\n",
    "        if self.reducer_flag:\n",
    "            grads = self.grad_reducer(grads)\n",
    "\n",
    "        succ = self.optimizer(grads)\n",
    "        return F.Depend(loss, succ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义梯度\n",
    "\n",
    "因为本次梯度所需格式的不同，需要通过`clip_grad`修饰器重新定义`_clip_grad`传入参数的类型，如下所示：\n",
    "\n",
    "- clip_type为数字类型。\n",
    "\n",
    "- clip_value为数字类型。\n",
    "\n",
    "- grad为张量类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_CLIP_TYPE = 1\n",
    "GRADIENT_CLIP_VALUE = 1.0\n",
    "\n",
    "# 生成clip_grad的重载函数\n",
    "clip_grad = C.MultitypeFuncGraph(\"clip_grad\")\n",
    "\n",
    "\n",
    "@clip_grad.register(\"Number\", \"Number\", \"Tensor\")\n",
    "def _clip_grad(clip_type, clip_value, grad):\n",
    "    # 如果梯度不在范围内直接返回梯度\n",
    "    if clip_type not in (0, 1):\n",
    "        return grad\n",
    "    \n",
    "    dt = F.dtype(grad)\n",
    "    \n",
    "    # 如果梯度为0，则计算新的梯度\n",
    "    if clip_type == 0:\n",
    "        new_grad = C.clip_by_value(grad, F.cast(F.tuple_to_array((-clip_value,)), dt),\n",
    "                                   F.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "    else:\n",
    "        new_grad = nn.ClipByNorm()(grad, F.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "    return new_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行模型训练\n",
    "\n",
    "调用之前设定的`FastTextTrainOneStepCell`并迭代数据集，完成模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastTextTrainOneStepCell<\n",
       "  (network): FastTextNetWithLoss<\n",
       "    (fasttext): FastText<\n",
       "      (embeding_func): Embedding<vocab_size=1383812, embedding_size=16, use_one_hot=False, embedding_table=Parameter (name=fasttext.embeding_func.embedding_table, shape=(1383812, 16), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=0>\n",
       "      (fc): Dense<input_channels=16, output_channels=4, has_bias=True>\n",
       "      (log_softmax): LogSoftmax<>\n",
       "      >\n",
       "    (loss_func): SoftmaxCrossEntropyWithLogits<>\n",
       "    >\n",
       "  (optimizer): Adam<\n",
       "    (learning_rate): _IteratorLearningRate<>\n",
       "    >\n",
       "  >"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_with_grads = FastTextTrainOneStepCell(net_with_loss, optimizer=optimizer)\n",
    "net_with_grads.set_train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3239299\n",
      "1.2918508\n",
      "1.236133\n",
      "1.1651388\n",
      "1.074889\n",
      "1.1294309\n",
      "0.9561551\n",
      "0.9522176\n",
      "0.91801494\n",
      "0.8881521\n",
      "0.80080545\n",
      "0.7337659\n",
      "0.6696707\n",
      "0.63573897\n",
      "0.5883118\n",
      "0.23005332\n",
      "0.4515081\n",
      "0.20126605\n",
      "0.4553006\n",
      "0.21953695\n",
      "0.15097088\n",
      "0.22751673\n",
      "0.299681\n",
      "0.23459665\n",
      "0.17367001\n",
      "0.32614958\n",
      "0.24170385\n",
      "0.18644962\n",
      "0.14626658\n",
      "0.18693896\n",
      "0.22911525\n",
      "0.30018106\n",
      "0.28360566\n",
      "0.22088502\n",
      "0.21194872\n",
      "0.17272016\n",
      "0.21119592\n",
      "0.21003135\n",
      "0.17690946\n",
      "0.18701789\n",
      "0.22161637\n",
      "0.18359481\n",
      "0.25332585\n",
      "0.1607348\n",
      "0.18905574\n",
      "0.21450931\n",
      "0.4525343\n",
      "0.048400477\n",
      "0.06543859\n",
      "0.04598104\n",
      "0.046952773\n",
      "0.05878158\n",
      "0.05802965\n",
      "0.021141667\n",
      "0.016563205\n",
      "0.0599133\n",
      "0.03379585\n",
      "0.020350233\n",
      "0.033926312\n",
      "0.10194215\n",
      "0.034460913\n",
      "0.055590115\n",
      "0.014893334\n",
      "0.060085252\n",
      "0.028355705\n",
      "0.056327038\n",
      "0.024952719\n",
      "0.032113466\n",
      "0.023740696\n",
      "0.01511923\n",
      "0.034571428\n",
      "0.037790537\n",
      "0.07907674\n",
      "0.032159526\n",
      "0.046872605\n",
      "0.028533353\n",
      "0.0076825884\n",
      "0.0077427584\n",
      "0.040141877\n",
      "0.013469651\n",
      "0.029853245\n",
      "1.1512634\n",
      "0.010118321\n",
      "0.025405075\n",
      "0.026934445\n",
      "0.031721305\n",
      "0.042373456\n",
      "0.0452683\n",
      "0.07718848\n",
      "0.06898584\n",
      "0.06665465\n",
      "0.030750485\n",
      "0.039185237\n",
      "0.017627863\n",
      "0.04209162\n",
      "0.020786878\n",
      "0.021398135\n",
      "0.018585052\n",
      "0.018579647\n",
      "0.012931412\n",
      "0.018248955\n",
      "0.019529575\n",
      "0.0103960065\n",
      "0.018511338\n",
      "0.014498311\n",
      "0.015237848\n",
      "0.0048193294\n",
      "0.012299601\n",
      "0.0012418798\n",
      "0.0017256059\n",
      "0.017027915\n",
      "0.010947452\n",
      "0.0053985277\n",
      "0.005133066\n"
     ]
    }
   ],
   "source": [
    "# 进行epoch训练\n",
    "for i in range(20):\n",
    "    for d in preprocessed_data.create_dict_iterator():\n",
    "        net_with_grads(d[\"src_token_text\"],len(d[\"src_token_text\"]),d[\"label_idx_tag\"])\n",
    "        # 输出loss值\n",
    "        print(net_with_loss(d[\"src_token_text\"],len(d[\"src_token_text\"]),d[\"label_idx_tag\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用测试数据集评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取验证集\n",
    "\n",
    "如同读取训练数据集一样，这里定义`batch_per_bucket`方法来读取测试数据集，其中入参分别为：\n",
    "\n",
    "batch_size：测试集中的batch数量。\n",
    "\n",
    "bucket_len：箱的长度。\n",
    "\n",
    "input_file：MindRecord文件路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_per_bucket(batch_size, bucket_length, input_file):\n",
    "    input_file = input_file + 'test/test_dataset_bs_' + str(bucket_length) + '.mindrecord'\n",
    "    if not input_file:\n",
    "        raise FileNotFoundError(\"input file parameter must not be empty.\")\n",
    "\n",
    "    data_set = ds.MindDataset(input_file,\n",
    "                              columns_list=['src_tokens', 'src_tokens_length', 'label_idx'])\n",
    "    type_cast_op = deC.TypeCast(mstype.int32)\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"src_tokens\")\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"src_tokens_length\")\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"label_idx\")\n",
    "    data_set = data_set.batch(batch_size, drop_remainder=False)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成测速数据\n",
    "通过循环方式来遍历所有MindRecord文件，最后完成数据集的收集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = [64, 128, 467]\n",
    "for i, _ in enumerate(bucket):\n",
    "    bucket_len = bucket[i]\n",
    "    ds_per = batch_per_bucket(512, bucket_len, input_file=\"\")\n",
    "    \n",
    "    if i == 0:\n",
    "        load_test_data = ds_per\n",
    "    else:\n",
    "        load_test_data = load_test_data + ds_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义验证方法\n",
    "\n",
    "现在传入训练后的网络`network`，通过`FastTextInferCell`来完成我们的验证流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextInferCell(nn.Cell):\n",
    "\n",
    "    def __init__(self, network):\n",
    "        super(FastTextInferCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.argmax = P.ArgMaxWithValue(axis=1, keep_dims=True)\n",
    "        self.log_softmax = nn.LogSoftmax(axis=1)\n",
    "\n",
    "    def construct(self, src_tokens, src_tokens_lengths):\n",
    "        prediction = self.network(src_tokens, src_tokens_lengths)\n",
    "        predicted_idx = self.log_softmax(prediction)\n",
    "        predicted_idx, _ = self.argmax(predicted_idx)\n",
    "\n",
    "        return predicted_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据并推理模型\n",
    "\n",
    "最后，实例化`load_infer_dataset`和`FastTextInferCell`来模型推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test_data = load_infer_dataset(batch_size=512,\n",
    "                                     datafile=\"\",\n",
    "                                     bucket=[64,128,467])\n",
    "# 创建测试pepiline\n",
    "ft_infer = FastTextInferCell(fast_text_net)\n",
    "predictions = []\n",
    "target_sens = []\n",
    "model = Model(ft_infer)\n",
    "\n",
    "# 计算每组测试批量的acc值\n",
    "for batch in load_test_data.create_dict_iterator(output_numpy=True, num_epochs=1):\n",
    "    target_sens.append(batch['label_idx'])\n",
    "    src_tokens = Tensor(batch['src_tokens'], mstype.int32)\n",
    "    src_tokens_length = Tensor(batch['src_tokens_length'], mstype.int32)\n",
    "    predicted_idx = ft_infer(src_tokens, src_tokens_length)\n",
    "    predictions.append(predicted_idx.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估模型\n",
    "\n",
    "计算模型的预测值与真实值之前的误差，输出模型的每个batch精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8404494382022472\n",
      "Accuracy:  0.9140625\n",
      "Accuracy:  0.912109375\n",
      "Accuracy:  0.91796875\n",
      "Accuracy:  0.923828125\n",
      "Accuracy:  0.93359375\n",
      "Accuracy:  0.9453125\n",
      "Accuracy:  0.923828125\n",
      "Accuracy:  0.90625\n",
      "Accuracy:  0.9140625\n",
      "Accuracy:  0.9375\n",
      "Accuracy:  0.91796875\n",
      "Accuracy:  0.923828125\n",
      "Accuracy:  0.9050772626931567\n",
      "Accuracy:  0.912109375\n",
      "Accuracy:  0.9347826086956522\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(predictions).flatten()\n",
    "merge_predictions = []\n",
    "\n",
    "# 调整输出格式\n",
    "for prediction in predictions:\n",
    "    merge_predictions.extend([prediction])\n",
    "predictions = merge_predictions\n",
    "target_sens = np.array(target_sens).flatten()\n",
    "merge_target_sens = []\n",
    "\n",
    "# 放入到测试列表中\n",
    "for target_sen in target_sens:\n",
    "    merge_target_sens.extend([target_sen])\n",
    "target_sens = merge_target_sens\n",
    "\n",
    "# 输出每组batch对应的acc值\n",
    "for i in range(len(target_sens)):\n",
    "    acc = accuracy_score(target_sens[i], predictions[i])\n",
    "    print(\"Accuracy: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
