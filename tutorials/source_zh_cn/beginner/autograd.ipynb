{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动微分\n",
    "\n",
    "[![下载Notebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_notebook.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/master/tutorials/zh_cn/beginner/mindspore_autograd.ipynb)&emsp;[![下载样例代码](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_download_code.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/master/tutorials/zh_cn/beginner/mindspore_autograd.py)&emsp;[![查看源文件](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png)](https://gitee.com/mindspore/docs/blob/master/tutorials/source_zh_cn/beginner/autograd.ipynb)\n",
    "\n",
    "自动微分能够计算可导函数在某点处的导数值，是反向传播算法的一般化。自动微分主要解决的问题是将一个复杂的数学运算分解为一系列简单的基本运算，该功能对用户屏蔽了大量的求导细节和过程，大大降低了框架的使用门槛。\n",
    "\n",
    "MindSpore使用`ops.grad`和`ops.value_and_grad`计算一阶导数。`ops.grad`只返回梯度，`ops.value_and_grad`同时返回网络正向计算结果和梯度。`ops.value_and_grad`属性如下：\n",
    "\n",
    "+ `fn`：待求导的函数或网络。\n",
    "+ `grad_position`：指定求导输入位置的索引。若为int类型，表示对单个输入求导；若为tuple类型，表示对tuple内索引的位置求导，其中索引从0开始；若是None，表示不对输入求导，这种场景下，`weights`非None。默认值：0。\n",
    "+ `weights`：训练网络中需要返回梯度的网络变量。一般可通过`weights = net.trainable_params()`获取。默认值：None。\n",
    "+ `has_aux`：是否返回辅助参数的标志。若为True，`fn`输出数量必须超过一个，其中只有`fn`第一个输出参与求导，其他输出值将直接返回。默认值：False。\n",
    "\n",
    "本章使用MindSpore中的`ops.value_and_grad`对网络求一阶导数。\n",
    "\n",
    "## 对网络权重求梯度\n",
    "\n",
    "由于MindSpore的自动微分建议采用函数式编程，因此样例将以函数式编程方式呈现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mindspore import ops, Tensor\n",
    "import mindspore.nn as nn\n",
    "import mindspore as ms\n",
    "\n",
    "# 定义网络\n",
    "net = nn.Dense(10, 1)\n",
    "\n",
    "# 定义损失函数\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 结合前向网络和损失函数\n",
    "def forward(inputs, labels):\n",
    "    logits = net(inputs)\n",
    "    loss = loss_fn(logits, labels)\n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对权重参数求一阶导，需要将权重传入`ops.value_and_grad`中的`weights`。这里不需要求对输入的导数，将`grad_position`设置成None。\n",
    "\n",
    "接下来，对网络权重求导："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1) 2 2\n"
     ]
    }
   ],
   "source": [
    "inputs = Tensor(np.random.randn(16, 10).astype(np.float32))\n",
    "labels = Tensor(np.random.randn(16, 1).astype(np.float32))\n",
    "weights = net.trainable_params()\n",
    "\n",
    "# 这里has_aux设置成True，表示只有loss参与求导，而logits不参与\n",
    "grad_fn = ops.value_and_grad(forward, grad_position=None, weights=weights, has_aux=True)\n",
    "(loss, logits), params_gradient = grad_fn(inputs, labels)\n",
    "\n",
    "# 打印结果\n",
    "print(logits.shape, len(weights), len(params_gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 停止计算梯度\n",
    "\n",
    "若某些权重不需要进行求导，则在定义求导网络时，相应的权重参数声明定义的时候，将其属性`requires_grad`需设置为`False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor(shape=[1], dtype=Float32, value= [ 5.00000000e+00]),)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = ms.Parameter(ms.Tensor(np.array([6], np.float32)), name='w')\n",
    "        self.b = ms.Parameter(ms.Tensor(np.array([1.0], np.float32)), name='b', requires_grad=False)\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = x * self.w + self.b\n",
    "        return out\n",
    "\n",
    "# 构建求导网络\n",
    "net = Net()\n",
    "params = net.trainable_params()\n",
    "x = ms.Tensor([5], dtype=ms.float32)\n",
    "value, gradient = ops.value_and_grad(net, grad_position=None, weights=params)(x)\n",
    "\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`ops.stop_gradient`可以停止计算梯度，示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wgrad: [0.]\n",
      "bgrad: [0.]\n"
     ]
    }
   ],
   "source": [
    "from mindspore.ops import stop_gradient\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = ms.Parameter(ms.Tensor(np.array([6], np.float32)), name='w')\n",
    "        self.b = ms.Parameter(ms.Tensor(np.array([1.0], np.float32)), name='b')\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = x * self.w + self.b\n",
    "        # 停止梯度更新，out对梯度计算无贡献\n",
    "        out = stop_gradient(out)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "params = net.trainable_params()\n",
    "x = ms.Tensor([100], dtype=ms.float32)\n",
    "value, output = ops.value_and_grad(net, grad_position=None, weights=params)(x)\n",
    "\n",
    "print(f\"wgrad: {output[0]}\\nbgrad: {output[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
