{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬æ•°æ®åŠ è½½ä¸å¢å¼º\n",
    "\n",
    "[![ä¸‹è½½Notebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_notebook.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r1.7/tutorials/zh_cn/advanced/dataset/mindspore_enhanced_text_data.ipynb)&emsp;\n",
    "[![ä¸‹è½½æ ·ä¾‹ä»£ç ](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_download_code.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r1.7/tutorials/zh_cn/advanced/dataset/mindspore_enhanced_text_data.py)&emsp;\n",
    "[![æŸ¥çœ‹æºæ–‡ä»¶](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source.png)](https://gitee.com/mindspore/docs/blob/r1.7/tutorials/source_zh_cn/advanced/dataset/enhanced_text_data.ipynb)\n",
    "\n",
    "éšç€å¯è·å¾—çš„æ–‡æœ¬æ•°æ®é€æ­¥å¢å¤šï¼Œå¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿è·å¾—å¯ç”¨äºç½‘ç»œè®­ç»ƒæ‰€éœ€å¹²å‡€æ•°æ®çš„è¯‰æ±‚ä¹Ÿæ›´ä¸ºè¿«åˆ‡ã€‚æ–‡æœ¬æ•°æ®é›†é¢„å¤„ç†é€šå¸¸åŒ…æ‹¬æ–‡æœ¬æ•°æ®é›†åŠ è½½ä¸æ•°æ®å¢å¼ºä¸¤éƒ¨åˆ†ã€‚\n",
    "\n",
    "æ–‡æœ¬æ•°æ®åŠ è½½é€šå¸¸åŒ…å«ä»¥ä¸‹ä¸‰ç§æ–¹å¼ï¼š\n",
    "\n",
    "1. é€šè¿‡æ–‡æœ¬è¯»å–çš„Datasetæ¥å£å¦‚[ClueDataset](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/dataset/mindspore.dataset.CLUEDataset.html#mindspore.dataset.CLUEDataset)ã€[TextFileDataset](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/dataset/mindspore.dataset.TextFileDataset.html#mindspore.dataset.TextFileDataset)è¿›è¡Œè¯»å–ã€‚\n",
    "2. å°†æ•°æ®é›†è½¬æˆæ ‡å‡†æ ¼å¼ï¼ˆå¦‚MindRecordæ ¼å¼ï¼‰ï¼Œå†é€šè¿‡å¯¹åº”æ¥å£ï¼ˆå¦‚[MindDataset](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/dataset/mindspore.dataset.MindDataset.html#mindspore.dataset.MindDataset)ï¼‰è¿›è¡Œè¯»å–ã€‚\n",
    "3. é€šè¿‡GeneratorDatasetæ¥å£ï¼Œæ¥æ”¶ç”¨æˆ·è‡ªå®šä¹‰çš„æ•°æ®é›†åŠ è½½å‡½æ•°ï¼Œè¿›è¡Œæ•°æ®åŠ è½½ï¼Œç”¨æ³•å¯å‚è€ƒ[è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½](https://www.mindspore.cn/tutorials/zh-CN/r1.7/advanced/dataset/custom.html)ç« èŠ‚ã€‚\n",
    "\n",
    "## åŠ è½½æ–‡æœ¬æ•°æ®\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬ä»¥ä»TXTæ–‡ä»¶ä¸­è¯»å–æ•°æ®ä¸ºä¾‹ï¼Œä»‹ç»`TextFileDataset`çš„ä½¿ç”¨æ–¹å¼ï¼Œæ›´å¤šæ–‡æœ¬æ•°æ®é›†åŠ è½½ç›¸å…³ä¿¡æ¯å¯å‚è€ƒ[APIæ–‡æ¡£](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/mindspore.dataset.html#id2)ã€‚\n",
    "\n",
    "1. å‡†å¤‡æ–‡æœ¬æ•°æ®ï¼Œå†…å®¹å¦‚ä¸‹ï¼š\n",
    "\n",
    "```text\n",
    "Welcome to Beijing\n",
    "åŒ—äº¬æ¬¢è¿æ‚¨ï¼\n",
    "æˆ‘å–œæ¬¢China!\n",
    "```\n",
    "\n",
    "2. åˆ›å»º`tokenizer.txt`æ–‡ä»¶å¹¶å¤åˆ¶æ–‡æœ¬æ•°æ®åˆ°è¯¥æ–‡ä»¶ä¸­ï¼Œå°†è¯¥æ–‡ä»¶å­˜æ”¾åœ¨./datasetsè·¯å¾„ä¸‹ã€‚æ‰§è¡Œå¦‚ä¸‹ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('./datasets'):\n",
    "    os.mkdir('./datasets')\n",
    "\n",
    "# æŠŠä¸Šé¢çš„æ–‡æœ¬æ•°æ®å†™å…¥æ–‡ä»¶tokenizer.txt\n",
    "file_handle = open('./datasets/tokenizer.txt', mode='w')\n",
    "file_handle.write('Welcome to Beijing \\nåŒ—äº¬æ¬¢è¿æ‚¨ï¼ \\næˆ‘å–œæ¬¢China! \\n')\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢çš„ä»£ç æ‰§è¡Œå®Œåï¼Œæ•°æ®é›†ç»“æ„ä¸ºï¼š\n",
    "\n",
    "```text\n",
    "./datasets\n",
    "â””â”€â”€ tokenizer.txt\n",
    "```\n",
    "\n",
    "3. ä»TXTæ–‡ä»¶ä¸­åŠ è½½æ•°æ®é›†å¹¶æ‰“å°ã€‚ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Beijing \n",
      "åŒ—äº¬æ¬¢è¿æ‚¨ï¼ \n",
      "æˆ‘å–œæ¬¢China! \n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "# å®šä¹‰æ–‡æœ¬æ•°æ®é›†çš„åŠ è½½è·¯å¾„\n",
    "DATA_FILE = './datasets/tokenizer.txt'\n",
    "\n",
    "# ä»tokenizer.txtä¸­åŠ è½½æ•°æ®é›†\n",
    "dataset = ds.TextFileDataset(DATA_FILE, shuffle=False)\n",
    "\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ–‡æœ¬æ•°æ®å¢å¼º\n",
    "\n",
    "é’ˆå¯¹æ–‡æœ¬æ•°æ®å¢å¼ºï¼Œå¸¸ç”¨æ“ä½œåŒ…å«æ–‡æœ¬åˆ†è¯ã€è¯æ±‡è¡¨æŸ¥æ‰¾ç­‰ï¼š\n",
    "\n",
    "- æ–‡æœ¬åˆ†è¯ï¼šå°†åŸå§‹ä¸€é•¿ä¸²å¥å­åˆ†å‰²æˆå¤šä¸ªåŸºæœ¬çš„è¯æ±‡ã€‚\n",
    "- è¯æ±‡è¡¨æŸ¥æ‰¾ï¼šæŸ¥æ‰¾åˆ†å‰²åå„è¯æ±‡å¯¹åº”çš„idï¼Œå¹¶å°†å¥å­ä¸­åŒ…å«çš„idç»„æˆè¯å‘é‡ä¼ å…¥ç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚\n",
    "\n",
    "ä¸‹é¢å¯¹æ•°æ®å¢å¼ºè¿‡ç¨‹ä¸­ç”¨åˆ°çš„åˆ†è¯åŠŸèƒ½ã€è¯æ±‡è¡¨æŸ¥æ‰¾ç­‰åŠŸèƒ½è¿›è¡Œä»‹ç»ï¼Œæ›´å¤šå…³äºæ–‡æœ¬å¤„ç†APIçš„ä½¿ç”¨è¯´æ˜ï¼Œå¯ä»¥å‚è€ƒ[APIæ–‡æ¡£](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/mindspore.dataset.text.html)ã€‚\n",
    "\n",
    "### æ„é€ ä¸ä½¿ç”¨è¯æ±‡è¡¨\n",
    "\n",
    "è¯æ±‡è¡¨æä¾›äº†å•è¯ä¸idå¯¹åº”çš„æ˜ å°„å…³ç³»ï¼Œé€šè¿‡è¯æ±‡è¡¨ï¼Œè¾“å…¥å•è¯èƒ½æ‰¾åˆ°å¯¹åº”çš„å•è¯idï¼Œåä¹‹ä¾æ®å•è¯idä¹Ÿèƒ½è·å–å¯¹åº”çš„å•è¯ã€‚\n",
    "\n",
    "MindSporeæä¾›äº†å¤šç§æ„é€ è¯æ±‡è¡¨ï¼ˆVocabï¼‰çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å­—å…¸ã€æ–‡ä»¶ã€åˆ—è¡¨ä»¥åŠDatasetå¯¹è±¡ä¸­è·å–åŸå§‹æ•°æ®ï¼Œä»¥ä¾¿æ„é€ è¯æ±‡è¡¨ï¼Œå¯¹åº”çš„æ¥å£ä¸ºï¼š[from_dict](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/dataset_text/mindspore.dataset.text.Vocab.html#mindsporedatasettextvocabfrom-dict)ã€[from_file](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/dataset_text/mindspore.dataset.text.Vocab.html#mindsporedatasettextvocabfrom-file)ã€[from_list](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/dataset_text/mindspore.dataset.text.Vocab.html#mindsporedatasettextvocabfrom-list)ã€[from_dataset](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/dataset_text/mindspore.dataset.text.Vocab.html#mindsporedatasettextvocabfrom-dataset)ã€‚\n",
    "\n",
    "ä»¥from_dictä¸ºä¾‹ï¼Œæ„é€ Vocabçš„æ–¹å¼å¦‚ä¸‹ï¼Œä¼ å…¥çš„dictä¸­åŒ…å«å¤šç»„å•è¯å’Œidå¯¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.dataset import text\n",
    "\n",
    "# æ„é€ è¯æ±‡è¡¨\n",
    "vocab = text.Vocab.from_dict({\"home\": 3, \"behind\": 2, \"the\": 4, \"world\": 5, \"<unk>\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabæä¾›äº†å•è¯ä¸idä¹‹é—´ç›¸äº’æŸ¥è¯¢çš„æ–¹æ³•ï¼Œå³ï¼š[tokens_to_ids](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/dataset_text/mindspore.dataset.text.Vocab.html#mindsporedatasettextvocabtokens-to-ids)å’Œ[ids_to_tokens](https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/dataset_text/mindspore.dataset.text.Vocab.html#mindsporedatasettextvocabids-to-tokens)æ–¹æ³•ï¼Œç”¨æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids:  [3, 5]\n",
      "tokens:  ['behind', 'world']\n"
     ]
    }
   ],
   "source": [
    "# æ ¹æ®å•è¯æŸ¥æ‰¾id\n",
    "ids = vocab.tokens_to_ids([\"home\", \"world\"])\n",
    "print(\"ids: \", ids)\n",
    "\n",
    "# æ ¹æ®idæŸ¥æ‰¾å•è¯\n",
    "tokens = vocab.ids_to_tokens([2, 5])\n",
    "print(\"tokens: \", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢æ‰“å°çš„ç»“æœå¯ä»¥çœ‹å‡ºï¼š\n",
    "\n",
    "- å•è¯`\"home\"`å’Œ`\"world\"`çš„idåˆ†åˆ«ä¸º`3`å’Œ`5`ï¼›\n",
    "- idä¸º`2`çš„å•è¯ä¸º`\"behind\"`ï¼Œidä¸º`5`çš„å•è¯ä¸º`\"world\"`ï¼›\n",
    "\n",
    "è¿™ä¸€ç»“æœä¹Ÿä¸è¯æ±‡è¡¨ä¸€è‡´ã€‚æ­¤å¤–Vocabä¹Ÿæ˜¯å¤šç§åˆ†è¯å™¨ï¼ˆå¦‚WordpieceTokenizerï¼‰çš„å¿…è¦å…¥å‚ï¼Œåˆ†è¯æ—¶ä¼šå°†å¥å­ä¸­å­˜åœ¨äºè¯æ±‡è¡¨çš„å•è¯ï¼Œå‰ååˆ†å‰²å¼€ï¼Œå˜æˆå•ç‹¬çš„ä¸€ä¸ªè¯æ±‡ï¼Œä¹‹åé€šè¿‡æŸ¥æ‰¾è¯æ±‡è¡¨èƒ½å¤Ÿè·å–å¯¹åº”çš„è¯æ±‡idã€‚\n",
    "\n",
    "### åˆ†è¯å™¨\n",
    "\n",
    "åˆ†è¯å°±æ˜¯å°†è¿ç»­çš„å­—åºåˆ—æŒ‰ç…§ä¸€å®šçš„è§„èŒƒåˆ’åˆ†æˆè¯åºåˆ—çš„è¿‡ç¨‹ï¼Œåˆç†çš„åˆ†è¯æœ‰åŠ©äºè¯­ä¹‰ç†è§£ã€‚\n",
    "\n",
    "MindSporeæä¾›äº†å¤šç§ä¸åŒç”¨é€”çš„åˆ†è¯å™¨ï¼Œå¦‚BasicTokenizerã€BertTokenizerã€JiebaTokenizerç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·é«˜æ€§èƒ½åœ°å¤„ç†æ–‡æœ¬ã€‚ç”¨æˆ·å¯ä»¥æ„å»ºè‡ªå·±çš„å­—å…¸ï¼Œä½¿ç”¨é€‚å½“çš„æ ‡è®°å™¨å°†å¥å­æ‹†åˆ†ä¸ºä¸åŒçš„æ ‡è®°ï¼Œå¹¶é€šè¿‡æŸ¥æ‰¾æ“ä½œè·å–å­—å…¸ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å®ç°è‡ªå®šä¹‰çš„åˆ†è¯å™¨ã€‚\n",
    "\n",
    "> ä¸‹é¢ä»‹ç»å‡ ç§å¸¸ç”¨åˆ†è¯å™¨çš„ä½¿ç”¨æ–¹æ³•ï¼Œæ›´å¤šåˆ†è¯å™¨ç›¸å…³ä¿¡æ¯è¯·å‚è€ƒ[APIæ–‡æ¡£](https://mindspore.cn/docs/zh-CN/r1.7/api_python/mindspore.dataset.text.html#mindspore-dataset-text-transforms)ã€‚\n",
    "\n",
    "#### BertTokenizer\n",
    "\n",
    "`BertTokenizer`æ“ä½œæ˜¯é€šè¿‡è°ƒç”¨`BasicTokenizer`å’Œ`WordpieceTokenizer`æ¥è¿›è¡Œåˆ†è¯çš„ã€‚\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†å’Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç„¶åé€šè¿‡`BertTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "åºŠå‰æ˜æœˆå…‰\n",
      "ç–‘æ˜¯åœ°ä¸Šéœœ\n",
      "ä¸¾å¤´æœ›æ˜æœˆ\n",
      "ä½å¤´æ€æ•…ä¹¡\n",
      "I am making small mistakes during working hours\n",
      "ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»\n",
      "ç¹é«”å­—\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "# æ„é€ å¾…åˆ†è¯æ•°æ®\n",
    "input_list = [\"åºŠå‰æ˜æœˆå…‰\", \"ç–‘æ˜¯åœ°ä¸Šéœœ\", \"ä¸¾å¤´æœ›æ˜æœˆ\", \"ä½å¤´æ€æ•…ä¹¡\", \"I am making small mistakes during working hours\",\n",
    "              \"ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»\", \"ç¹é«”å­—\"]\n",
    "\n",
    "# åŠ è½½æ–‡æœ¬æ•°æ®é›†\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢ä¸ºæ•°æ®é›†æœªè¢«åˆ†è¯å‰çš„æ•°æ®æ‰“å°æƒ…å†µï¼Œä¸‹é¢ä½¿ç”¨`BertTokenizer`åˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------after tokenization-----------------------------\n",
      "['åºŠ' 'å‰' 'æ˜' 'æœˆ' 'å…‰']\n",
      "['ç–‘' 'æ˜¯' 'åœ°' 'ä¸Š' 'éœœ']\n",
      "['ä¸¾' 'å¤´' 'æœ›' 'æ˜' 'æœˆ']\n",
      "['ä½' 'å¤´' 'æ€' 'æ•…' 'ä¹¡']\n",
      "['I' 'am' 'mak' '##ing' 'small' 'mistake' '##s' 'during' 'work' '##ing'\n",
      " 'hour' '##s']\n",
      "['ğŸ˜€' 'å˜¿' 'å˜¿' 'ğŸ˜ƒ' 'å“ˆ' 'å“ˆ' 'ğŸ˜„' 'å¤§' 'ç¬‘' 'ğŸ˜' 'å˜»' 'å˜»']\n",
      "['ç¹' 'é«”' 'å­—']\n"
     ]
    }
   ],
   "source": [
    "# æ„å»ºè¯æ±‡è¡¨\n",
    "vocab_list = [\n",
    "    \"åºŠ\", \"å‰\", \"æ˜\", \"æœˆ\", \"å…‰\", \"ç–‘\", \"æ˜¯\", \"åœ°\", \"ä¸Š\", \"éœœ\", \"ä¸¾\", \"å¤´\", \"æœ›\", \"ä½\", \"æ€\", \"æ•…\", \"ä¹¡\",\n",
    "    \"ç¹\", \"é«”\", \"å­—\", \"å˜¿\", \"å“ˆ\", \"å¤§\", \"ç¬‘\", \"å˜»\", \"i\", \"am\", \"mak\", \"make\", \"small\", \"mistake\",\n",
    "    \"##s\", \"during\", \"work\", \"##ing\", \"hour\", \"ğŸ˜€\", \"ğŸ˜ƒ\", \"ğŸ˜„\", \"ğŸ˜\", \"+\", \"/\", \"-\", \"=\", \"12\",\n",
    "    \"28\", \"40\", \"16\", \" \", \"I\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[PAD]\", \"[MASK]\", \"[unused1]\", \"[unused10]\"]\n",
    "\n",
    "# åŠ è½½è¯æ±‡è¡¨\n",
    "vocab = text.Vocab.from_list(vocab_list)\n",
    "\n",
    "# ä½¿ç”¨BertTokenizeråˆ†è¯å™¨å¯¹æ–‡æœ¬æ•°æ®é›†è¿›è¡Œåˆ†è¯æ“ä½œ\n",
    "tokenizer_op = text.BertTokenizer(vocab=vocab)\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢ä¸¤æ¬¡çš„æ‰“å°ç»“æœå¯ä»¥çœ‹å‡ºï¼Œæ•°æ®é›†ä¸­çš„å¥å­ã€è¯è¯­å’Œè¡¨æƒ…ç¬¦å·ç­‰éƒ½è¢«`BertTokenizer`åˆ†è¯å™¨ä»¥è¯æ±‡è¡¨ä¸­çš„è¯æ±‡ä¸ºæœ€å°å•å…ƒè¿›è¡Œäº†åˆ†å‰²ï¼Œâ€œæ•…ä¹¡â€è¢«åˆ†å‰²æˆäº†â€˜æ•…â€™å’Œâ€˜ä¹¡â€™ï¼Œâ€œæ˜æœˆâ€è¢«åˆ†å‰²æˆäº†â€˜æ˜â€™å’Œâ€˜æœˆâ€™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œâ€œmistakesâ€è¢«åˆ†å‰²æˆäº†â€˜mistakeâ€™å’Œâ€˜##sâ€™ã€‚\n",
    "\n",
    "#### JiebaTokenizer\n",
    "\n",
    "`JiebaTokenizer`æ“ä½œæ˜¯åŸºäºjiebaçš„ä¸­æ–‡åˆ†è¯ã€‚\n",
    "\n",
    "ä»¥ä¸‹ç¤ºä¾‹ä»£ç å®Œæˆä¸‹è½½å­—å…¸æ–‡ä»¶`hmm_model.utf8`å’Œ`jieba.dict.utf8`ï¼Œå¹¶å°†å…¶æ”¾åˆ°æŒ‡å®šä½ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvision.dataset import DownLoad\n",
    "\n",
    "# å­—å…¸æ–‡ä»¶å­˜æ”¾è·¯å¾„\n",
    "dl_path = \"./dictionary\"\n",
    "\n",
    "# è·å–å­—å…¸æ–‡ä»¶æº\n",
    "dl_url_hmm = \"https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/hmm_model.utf8\"\n",
    "dl_url_jieba = \"https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/jieba.dict.utf8\"\n",
    "\n",
    "# ä¸‹è½½å­—å…¸æ–‡ä»¶\n",
    "dl = DownLoad()\n",
    "dl.download_url(url=dl_url_hmm, path=dl_path)\n",
    "dl.download_url(url=dl_url_jieba, path=dl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹è½½çš„æ–‡ä»¶æ”¾ç½®çš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\n",
    "\n",
    "```text\n",
    "./dictionary/\n",
    "â”œâ”€â”€ hmm_model.utf8\n",
    "â””â”€â”€ jieba.dict.utf8\n",
    "```\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨HMMä¸MPå­—å…¸æ–‡ä»¶åˆ›å»º`JiebaTokenizer`å¯¹è±¡ï¼Œå¹¶å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œæœ€åå±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "æ˜å¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "# æ„é€ å¾…åˆ†è¯æ•°æ®\n",
    "input_list = [\"æ˜å¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§\"]\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢ä¸ºæ•°æ®é›†æœªè¢«åˆ†è¯å‰çš„æ•°æ®æ‰“å°æƒ…å†µï¼Œä¸‹é¢ä½¿ç”¨`JiebaTokenizer`åˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------after tokenization-----------------------------\n",
      "['æ˜å¤©' 'å¤©æ°”' 'å¤ªå¥½äº†' 'æˆ‘ä»¬' 'ä¸€èµ·' 'å»' 'å¤–é¢' 'ç©å§']\n"
     ]
    }
   ],
   "source": [
    "HMM_FILE = \"./dictionary/hmm_model.utf8\"\n",
    "MP_FILE = \"./dictionary/jieba.dict.utf8\"\n",
    "\n",
    "# ä½¿ç”¨JiebaTokenizeråˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯\n",
    "jieba_op = text.JiebaTokenizer(HMM_FILE, MP_FILE)\n",
    "dataset = dataset.map(operations=jieba_op, input_columns=[\"text\"], num_parallel_workers=1)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "for data in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢ä¸¤æ¬¡æ‰“å°ç»“æœæ¥çœ‹ï¼Œæ•°æ®é›†ä¸­çš„å¥å­è¢«`JiebaTokenizer`åˆ†è¯å™¨ä»¥è¯è¯­ä¸ºæœ€å°å•å…ƒè¿›è¡Œäº†åˆ’åˆ†ã€‚\n",
    "\n",
    "#### SentencePieceTokenizer\n",
    "\n",
    "`SentencePieceTokenizer`æ“ä½œæ˜¯åŸºäºå¼€æºè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…[SentencePiece](https://github.com/google/sentencepiece)å°è£…çš„åˆ†è¯å™¨ã€‚\n",
    "\n",
    "ä»¥ä¸‹ç¤ºä¾‹ä»£ç å°†ä¸‹è½½æ–‡æœ¬æ•°æ®é›†æ–‡ä»¶`botchan.txt`ï¼Œå¹¶å°†å…¶æ”¾ç½®åˆ°æŒ‡å®šä½ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é›†å­˜æ”¾ä½ç½®\n",
    "dl_path = \"./datasets\"\n",
    "\n",
    "# è·å–è¯­æ–™æ•°æ®æº\n",
    "dl_url_botchan = \"https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/botchan.txt\"\n",
    "\n",
    "# ä¸‹è½½è¯­æ–™æ•°æ®\n",
    "dl.download_url(url=dl_url_botchan, path=dl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹è½½çš„æ–‡ä»¶æ”¾ç½®çš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\n",
    "\n",
    "```text\n",
    "./datasets/\n",
    "â””â”€â”€ botchan.txt\n",
    "```\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä»`vocab_file`æ–‡ä»¶ä¸­æ„å»ºä¸€ä¸ª`vocab`å¯¹è±¡ï¼Œå†é€šè¿‡`SentencePieceTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "Nothing in the world is difficult for one who sets his mind on it.\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "from mindspore.dataset.text import SentencePieceModel, SPieceTokenizerOutType\n",
    "\n",
    "# æ„é€ å¾…åˆ†è¯æ•°æ®\n",
    "input_list = [\"Nothing in the world is difficult for one who sets his mind on it.\"]\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢ä¸ºæ•°æ®é›†æœªè¢«åˆ†è¯å‰çš„æ•°æ®æ‰“å°æƒ…å†µï¼Œä¸‹é¢ä½¿ç”¨`SentencePieceTokenizer`åˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------after tokenization-----------------------------\n",
      "['â–Nothing' 'â–in' 'â–the' 'â–world' 'â–is' 'â–difficult' 'â–for' 'â–one' 'â–who'\n",
      " 'â–sets' 'â–his' 'â–mind' 'â–on' 'â–it.']\n"
     ]
    }
   ],
   "source": [
    "# è¯­æ–™æ•°æ®æ–‡ä»¶å­˜æ”¾è·¯å¾„\n",
    "vocab_file = \"./datasets/botchan.txt\"\n",
    "\n",
    "# ä»è¯­æ–™æ•°æ®ä¸­å­¦ä¹ æ„å»ºè¯æ±‡è¡¨\n",
    "vocab = text.SentencePieceVocab.from_file([vocab_file], 5000, 0.9995, SentencePieceModel.WORD, {})\n",
    "\n",
    "# ä½¿ç”¨SentencePieceTokenizeråˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯\n",
    "tokenizer_op = text.SentencePieceTokenizer(vocab, out_type=SPieceTokenizerOutType.STRING)\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢ä¸¤æ¬¡æ‰“å°ç»“æœæ¥çœ‹ï¼Œæ•°æ®é›†ä¸­çš„å¥å­è¢«`SentencePieceTokenizer`åˆ†è¯å™¨ä»¥è¯è¯­ä¸ºæœ€å°å•å…ƒè¿›è¡Œäº†åˆ’åˆ†ã€‚åœ¨`SentencePieceTokenizer`åˆ†è¯å™¨çš„å¤„ç†è¿‡ç¨‹ä¸­ï¼Œç©ºæ ¼ä½œä¸ºæ™®é€šç¬¦å·å¤„ç†ï¼Œå¹¶ä½¿ç”¨ä¸‹åˆ’çº¿æ ‡è®°ç©ºæ ¼ã€‚\n",
    "\n",
    "#### UnicodeCharTokenizer\n",
    "\n",
    "`UnicodeCharTokenizer`æ“ä½œæ˜¯æ ¹æ®Unicodeå­—ç¬¦é›†æ¥åˆ†è¯çš„ã€‚\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åé€šè¿‡`UnicodeCharTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "Welcome to Beijing!\n",
      "åŒ—äº¬æ¬¢è¿æ‚¨ï¼\n",
      "æˆ‘å–œæ¬¢China!\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "# æ„é€ å¾…åˆ†è¯æ•°æ®\n",
    "input_list = [\"Welcome to Beijing!\", \"åŒ—äº¬æ¬¢è¿æ‚¨ï¼\", \"æˆ‘å–œæ¬¢China!\"]\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢ä¸ºæ•°æ®é›†æœªè¢«åˆ†è¯å‰çš„æ•°æ®æ‰“å°æƒ…å†µï¼Œä¸‹é¢ä½¿ç”¨`UnicodeCharTokenizer`åˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------after tokenization-----------------------------\n",
      "['W', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'B', 'e', 'i', 'j', 'i', 'n', 'g', '!']\n",
      "['åŒ—', 'äº¬', 'æ¬¢', 'è¿', 'æ‚¨', 'ï¼']\n",
      "['æˆ‘', 'å–œ', 'æ¬¢', 'C', 'h', 'i', 'n', 'a', '!']\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨UnicodeCharTokenizeråˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯\n",
    "tokenizer_op = text.UnicodeCharTokenizer()\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "for data in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(data['text']).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢ä¸¤æ¬¡æ‰“å°ç»“æœå¯ä»¥çœ‹å‡ºï¼Œæ•°æ®é›†ä¸­çš„å¥å­è¢«`UnicodeCharTokenizer`åˆ†è¯å™¨è¿›è¡Œåˆ†å‰²ï¼Œä¸­æ–‡ä»¥å•ä¸ªæ±‰å­—ä¸ºæœ€å°å•å…ƒï¼Œè‹±æ–‡ä»¥å•ä¸ªå­—æ¯ä¸ºæœ€å°å•å…ƒã€‚\n",
    "\n",
    "#### WhitespaceTokenizer\n",
    "\n",
    "`WhitespaceTokenizer`æ“ä½œæ˜¯æ ¹æ®ç©ºæ ¼æ¥è¿›è¡Œåˆ†è¯çš„ã€‚\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åé€šè¿‡`WhitespaceTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "Welcome to Beijing!\n",
      "åŒ—äº¬æ¬¢è¿æ‚¨ï¼\n",
      "æˆ‘å–œæ¬¢China!\n",
      "åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "# æ„é€ å¾…åˆ†è¯æ•°æ®\n",
    "input_list = [\"Welcome to Beijing!\", \"åŒ—äº¬æ¬¢è¿æ‚¨ï¼\", \"æˆ‘å–œæ¬¢China!\", \"åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚\"]\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢ä¸ºæ•°æ®é›†æœªè¢«åˆ†è¯å‰çš„æ•°æ®æ‰“å°æƒ…å†µï¼Œä¸‹é¢ä½¿ç”¨`WhitespaceTokenizer`åˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------after tokenization-----------------------------\n",
      "['Welcome', 'to', 'Beijing!']\n",
      "['åŒ—äº¬æ¬¢è¿æ‚¨ï¼']\n",
      "['æˆ‘å–œæ¬¢China!']\n",
      "['åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚']\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨WhitespaceTokenizeråˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯\n",
    "tokenizer_op = text.WhitespaceTokenizer()\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢ä¸¤æ¬¡æ‰“å°ç»“æœå¯ä»¥çœ‹å‡ºï¼Œæ•°æ®é›†ä¸­çš„å¥å­è¢«`WhitespaceTokenizer`åˆ†è¯å™¨ä»¥ç©ºæ ¼ä¸ºåˆ†éš”ç¬¦è¿›è¡Œåˆ†å‰²ã€‚\n",
    "\n",
    "#### WordpieceTokenizer\n",
    "\n",
    "`WordpieceTokenizer`æ“ä½œæ˜¯åŸºäºå•è¯é›†æ¥è¿›è¡Œåˆ’åˆ†çš„ï¼Œåˆ’åˆ†ä¾æ®å¯ä»¥æ˜¯å•è¯é›†ä¸­çš„å•ä¸ªå•è¯ï¼Œæˆ–è€…å¤šä¸ªå•è¯çš„ç»„åˆå½¢å¼ã€‚\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä»å•è¯åˆ—è¡¨ä¸­æ„å»º`vocab`å¯¹è±¡ï¼Œé€šè¿‡`WordpieceTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "My\n",
      "favorite\n",
      "book\n",
      "is\n",
      "love\n",
      "during\n",
      "the\n",
      "cholera\n",
      "era\n",
      ".\n",
      "what\n",
      "æˆ‘\n",
      "æœ€\n",
      "å–œ\n",
      "æ¬¢\n",
      "çš„\n",
      "ä¹¦\n",
      "æ˜¯\n",
      "éœ\n",
      "ä¹±\n",
      "æ—¶\n",
      "æœŸ\n",
      "çš„\n",
      "çˆ±\n",
      "æƒ…\n",
      "ã€‚\n",
      "å¥½\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "# æ„é€ å¾…åˆ†è¯æ•°æ®\n",
    "input_list = [\"My\", \"favorite\", \"book\", \"is\", \"love\", \"during\", \"the\", \"cholera\", \"era\", \".\", \"what\",\n",
    "              \"æˆ‘\", \"æœ€\", \"å–œ\", \"æ¬¢\", \"çš„\", \"ä¹¦\", \"æ˜¯\", \"éœ\", \"ä¹±\", \"æ—¶\", \"æœŸ\", \"çš„\", \"çˆ±\", \"æƒ…\", \"ã€‚\", \"å¥½\"]\n",
    "\n",
    "# æ„é€ è‹±æ–‡è¯æ±‡è¡¨\n",
    "vocab_english = [\"book\", \"cholera\", \"era\", \"favor\", \"##ite\", \"My\", \"is\", \"love\", \"dur\", \"##ing\", \"the\", \".\"]\n",
    "\n",
    "# æ„é€ ä¸­æ–‡è¯æ±‡è¡¨\n",
    "vocab_chinese = ['æˆ‘', 'æœ€', 'å–œ', 'æ¬¢', 'çš„', 'ä¹¦', 'æ˜¯', 'éœ', 'ä¹±', 'æ—¶', 'æœŸ', 'çˆ±', 'æƒ…', 'ã€‚']\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢ä¸ºæ•°æ®é›†æœªè¢«åˆ†è¯å‰çš„æ•°æ®æ‰“å°æƒ…å†µï¼Œæ­¤å¤„ç‰¹æ„æ„é€ äº†è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„å•è¯â€œwhatâ€å’Œâ€œå¥½â€ï¼Œä¸‹é¢ä½¿ç”¨`WordpieceTokenizer`åˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------after tokenization-----------------------------\n",
      "['My']\n",
      "['favor' '##ite']\n",
      "['book']\n",
      "['is']\n",
      "['love']\n",
      "['dur' '##ing']\n",
      "['the']\n",
      "['cholera']\n",
      "['era']\n",
      "['.']\n",
      "['[UNK]']\n",
      "['æˆ‘']\n",
      "['æœ€']\n",
      "['å–œ']\n",
      "['æ¬¢']\n",
      "['çš„']\n",
      "['ä¹¦']\n",
      "['æ˜¯']\n",
      "['éœ']\n",
      "['ä¹±']\n",
      "['æ—¶']\n",
      "['æœŸ']\n",
      "['çš„']\n",
      "['çˆ±']\n",
      "['æƒ…']\n",
      "['ã€‚']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨WordpieceTokenizeråˆ†è¯å™¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯\n",
    "vocab = text.Vocab.from_list(vocab_english+vocab_chinese)\n",
    "tokenizer_op = text.WordpieceTokenizer(vocab=vocab)\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢ä¸¤æ¬¡æ‰“å°ç»“æœå¯ä»¥çœ‹å‡ºï¼Œæ•°æ®é›†ä¸­çš„è¯è¯­è¢«`WordpieceTokenizer`åˆ†è¯å™¨ä»¥æ„é€ çš„è¯æ±‡è¡¨è¿›è¡Œåˆ†è¯ï¼Œâ€œMyâ€ä»ç„¶è¢«åˆ†ä¸ºâ€œMyâ€ï¼Œâ€œloveâ€ä»ç„¶è¢«åˆ†ä¸ºâ€œloveâ€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œâ€œfavoriteâ€è¢«åˆ†ä¸ºäº†â€œfavorâ€å’Œâ€œ##iteâ€ï¼Œç”±äºâ€œwordâ€å’Œâ€œå¥½â€åœ¨è¯æ±‡è¡¨ä¸­æœªæ‰¾åˆ°ï¼Œæ‰€ä»¥ä½¿ç”¨\\[UNK\\]è¡¨ç¤ºã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
