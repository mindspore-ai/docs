{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fca2d6c-be41-47be-b020-228c6e2acc98",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 混合精度\n",
    "\n",
    "[![下载Notebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_notebook.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/master/tutorials/zh_cn/mindspore_mixed_precision.ipynb)&emsp;[![下载样例代码](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_download_code.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/master/tutorials/zh_cn/mindspore_mixed_precision.py)&emsp;[![查看源文件](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png)](https://gitee.com/mindspore/docs/blob/master/tutorials/source_zh_cn/mixed_precision.ipynb)\n",
    "\n",
    "## 概述\n",
    "\n",
    "通常我们训练神经网络模型的时候，默认使用的数据类型为单精度FP32。近年来，为了加快训练时间、减少网络训练时候所占用的内存，并且保存训练出来的模型精度持平的条件下，业界提出越来越多的混合精度训练的方法。这里的混合精度训练是指在训练的过程中，同时使用单精度（FP32）和半精度（FP16）。\n",
    "\n",
    "## 浮点数据类型\n",
    "\n",
    "浮点数据类型主要分为双精度（FP64）、单精度（FP32）、半精度（FP16）。在神经网络模型的训练过程中，一般默认采用单精度（FP32）浮点数据类型，来表示网络模型权重和其他参数。在了解混合精度训练之前，这里简单了解浮点数据类型。\n",
    "\n",
    "根据IEEE二进制浮点数算术标准（[IEEE 754](https://en.wikipedia.org/wiki/IEEE_754)）的定义，浮点数据类型分为双精度（FP64）、单精度（FP32）、半精度（FP16）三种，其中每一种都有三个不同的位来表示。FP64表示采用8个字节共64位，来进行的编码存储的一种数据类型；同理，FP32表示采用4个字节共32位来表示；FP16则是采用2字节共16位来表示。如图所示：\n",
    "\n",
    "![fp16-vs-fp32](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/experts/source_zh_cn/others/images/fp16_vs_fp32.png)\n",
    "\n",
    "从图中可以看出，与FP32相比，FP16的存储空间是FP32的一半，FP32则是FP64的一半。主要分为三种类型：\n",
    "\n",
    "- 最高位表示符号位sign bit。\n",
    "- 中间表示指数位exponent bit。\n",
    "- 低位表示分数位fraction bit。\n",
    "\n",
    "以FP16为例子，第一位符号位sign表示正负符号；接着5个比特位表示指数exponent，全0和全1有特殊用途，因此二进制范围为00001~11110；最后10个比特位表示分数fraction。假设`S`表示sign bit的十进制值，`E`表示exponent的十进制值，`fraction`表示fraction的十进制值，则公式为：\n",
    "\n",
    "$$x=(-1)^{S}\\times2^{E-15}\\times(1+\\frac{fraction}{1024})$$\n",
    "\n",
    "同理，假设`M`为分数值，则一个规则化的FP32的真值为：\n",
    "\n",
    "$$x=(-1)^{S}\\times2^{E-127}\\times(1.M)$$\n",
    "\n",
    "一个规格化的FP64的真值为：\n",
    "\n",
    "$$x=(-1)^{S}\\times2^{E-1023}\\times(1.M)$$\n",
    "\n",
    "FP16可以表示的最大值为 0 11110 1111111111，计算方法为：\n",
    "\n",
    "$$(-1)^0\\times2^{30-15}\\times1.1111111111 = 1.1111111111(b)\\times2^{15} = 1.9990234375(d)\\times2^{15} = 65504$$\n",
    "\n",
    "其中`b`表示二进制值(binary)，`d`表示十进制值(decimal)。\n",
    "\n",
    "FP16可以表示的最小值为 0 00001 0000000000，计算方法为：\n",
    "\n",
    "$$ (-1)^{1}\\times2^{1-15}=2^{-14}=6.104×10^{-5}=-65504$$\n",
    "\n",
    "因此FP16的最大取值范围是[-65504, 65504]，能表示的精度范围是 $2^{-24}$，超过这个数值的数字会被直接置0。\n",
    "\n",
    "## 使用FP16训练问题\n",
    "\n",
    "首先来看看为什么需要混合精度。使用FP16训练神经网络，相对比使用FP32带来的优点有：\n",
    "\n",
    "- 减少内存占用：FP16的位宽是FP32的一半，因此权重等参数所占用的内存也是原来的一半，节省下来的内存可以放更大的网络模型或者使用更多的数据进行训练。\n",
    "- 加快通讯效率：针对分布式训练，特别是在大模型训练的过程中，通讯的开销制约了网络模型训练的整体性能，通讯的位宽少了意味着可以提升通讯性能，减少等待时间，加快数据的流通。\n",
    "- 计算效率更高：在特殊的AI加速芯片如华为昇腾系列的Ascend 910和310系列，或者NVIDIA VOLTA架构的GPU上，使用FP16的执行运算性能比FP32更加快。\n",
    "\n",
    "但是使用FP16同样会带来一些问题，其中最重要的是精度溢出和舍入误差。\n",
    "\n",
    "- 数据溢出：数据溢出比较好理解，FP16的有效数据表示范围为 $[6.10\\times10^{-5}, 65504]$，FP32的有效数据表示范围为 $[1.4\\times10^{-45}, 1.7\\times10^{38}]$。可见FP16相比FP32的有效范围要窄很多，使用FP16替换FP32会出现上溢（Overflow）和下溢（Underflow）的情况。而在深度学习中，需要计算网络模型中权重的梯度（一阶导数），因此梯度会比权重值更加小，往往容易出现下溢情况。\n",
    "- 舍入误差：Rounding Error指示是当网络模型的反向梯度很小，一般FP32能够表示，但是转换到FP16会小于当前区间内的最小间隔，会导致数据溢出。如0.00006666666在FP32中能正常表示，转换到FP16后会表示成为0.000067，不满足FP16最小间隔的数会强制舍入。\n",
    "\n",
    "## 混合精度计算流程\n",
    "\n",
    "MindSpore混合精度典型的计算流程如下图所示：\n",
    "\n",
    "![mix precision](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/experts/source_zh_cn/others/images/mix_precision_fp16.png)\n",
    "\n",
    "1. 参数以FP32存储；\n",
    "2. 正向计算过程中，遇到FP16算子，需要把算子输入和参数从FP32 cast成FP16进行计算；\n",
    "3. 将Loss层设置为FP32进行计算；\n",
    "4. 反向计算过程中，首先乘以Loss Scale值，避免反向梯度过小而产生下溢；\n",
    "5. FP16参数参与梯度计算，其结果将被cast回FP32；\n",
    "6. 除以Loss scale值，还原被放大的梯度；\n",
    "7. 判断梯度是否存在溢出，如果溢出则跳过更新，否则优化器以FP32对原始参数进行更新。\n",
    "\n",
    "本文通过自动混合精度和手动混合精度的样例来讲解计算流程。\n",
    "\n",
    "## 损失缩放原理\n",
    "\n",
    "损失缩放（Loss Scale）技术主要是作用于混合精度训练的过程当中。\n",
    "\n",
    "在混合精度训练的过程中，会使用FP16类型来替代FP32类型进行数据存储，从而达到减少内存和提高计算速度的效果。但是由于FP16类型要比FP32类型表示的范围小很多，所以当参数（如梯度）在训练过程中变得很小时，就会发生数据下溢的情况。而Loss Scale损失缩放，正是为了解决FP16类型数据下溢问题而提出的。\n",
    "\n",
    "其主要思想是在计算损失值loss的时候，将loss扩大一定的倍数。由于链式法则的存在，梯度也会相应扩大，然后在优化器更新权重时再缩小相应的倍数，从而避免了数据下溢的情况又不影响计算结果。\n",
    "\n",
    "MindSpore中提供了两种Loss Scale的实现方式， 用户既可以使用函数式编程的写法，在训练过程中手动调用`StaticLossScaler`或`DynamicLossScaler`的`scale`和`unscale`方法对损失或梯度进行缩放；也可以基于`Model`接口，在使用`Model`构建模型时，配置混合精度策略`amp_level`和Loss Scale方式`loss_scale_manager`为`FixedLossScaleManager`或`DynamicLossScaleManager`。\n",
    "\n",
    "首先来看看为什么需要混合精度。使用FP16训练神经网络，相对比使用FP32带来的优点有：\n",
    "\n",
    "- **减少内存占用**：FP16的位宽是FP32的一半，因此权重等参数所占用的内存也是原来的一半，节省下来的内存可以放更大的网络模型或者使用更多的数据进行训练。\n",
    "- **加快通讯效率**：针对分布式训练，特别是在大模型训练的过程中，通讯的开销制约了网络模型训练的整体性能，通讯的位宽少了意味着可以提升通讯性能，减少等待时间，加快数据的流通。\n",
    "- **计算效率更高**：在特殊的AI加速芯片如华为Ascend 910和310系列，或者NVIDIA VOLTA架构的Titan V and Tesla V100的GPU上，使用FP16的执行运算性能比FP32更加快。\n",
    "\n",
    "但是使用FP16同样会带来一些问题，其中最重要的是精度溢出和舍入误差，Loss Scale主要就是为了解决训练过程中梯度精度下溢而提出的。\n",
    "\n",
    "如图所示，如果仅仅使用FP32训练，模型收敛得比较好，但是如果用了混合精度训练，会存在网络模型无法收敛的情况。原因是梯度的值太小，使用FP16表示会造成了数据下溢出（Underflow）的问题，导致模型不收敛，如图中灰色的部分。于是需要引入损失缩放（Loss Scale）技术。\n",
    "\n",
    "![loss-scale1](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/experts/source_zh_cn/others/images/loss_scale1.png)\n",
    "\n",
    "下面是在网络模型训练阶段， 某一层的激活函数梯度分布式中，其中有68%的网络模型激活参数位0，另外有4%的精度在$2^{-32},2^{-20}$这个区间内，直接使用FP16对这里面的数据进行表示，会截断下溢的数据，所有的梯度值都会变为0。\n",
    "\n",
    "![loss-scale2](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/experts/source_zh_cn/others/images/loss_scale2.png)\n",
    "\n",
    "为了解决梯度过小数据下溢的问题，对前向计算出来的Loss值进行放大操作，也就是把FP32的参数乘以某一个因子系数后，把可能溢出的小数位数据往前移，平移到FP16能表示的数据范围内。根据链式求导法则，放大Loss后会作用在反向传播的每一层梯度，这样比在每一层梯度上进行放大更加高效。\n",
    "\n",
    "![loss-scale3](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/experts/source_zh_cn/others/images/loss_scale3.png)\n",
    "\n",
    "损失放大是需要结合混合精度实现的，其主要的主要思路是：\n",
    "\n",
    "- **Scale up阶段**：网络模型前向计算后在反向传播前，将得到的损失变化值DLoss增大$2^K$倍。\n",
    "- **Scale down阶段**：反向传播后，将权重梯度缩小$2^K$倍，恢复FP32值进行存储。\n",
    "\n",
    "**动态损失缩放（Dynamic Loss Scale）**：上面提到的损失缩放都是使用一个默认值对损失值进行缩放，为了充分利用FP16的动态范围，更好地缓解舍入误差，应尽量使用比较大的缩放因子。总结动态损失缩放算法，就是设定一个较大的损失缩放因子初始值scale value，并且间歇性地尝试对其进行放大，但当scale value过大，梯度发生上溢时应减少损失缩放规模，从而实现在不引起溢出的情况下使用最高损失缩放因子，更好地恢复精度。\n",
    "\n",
    "动态损失缩放的算法如下：\n",
    "\n",
    "1. 动态损失缩放的算法会从比较高的缩放因子开始（如$2^{24}$），然后开始进行训练，并在迭代中检查数是否会发生梯度上溢（Infs/Nans）；\n",
    "2. 如果没有梯度上溢，则不调整缩放因子，继续进行迭代；如果检测到梯度上溢，则缩放因子会减半，重新确认梯度更新情况，直到参数不出现在溢出的范围内；\n",
    "3. 在训练的后期，loss已经趋近收敛稳定，梯度更新的幅度往往小了，这个时候可以允许更高的损失缩放因子来再次防止数据下溢。\n",
    "4. 因此，动态损失缩放算法会尝试在每N（N=2000）次迭代将损失缩放增加F倍数，然后执行步骤2检查是否上溢。\n",
    "\n",
    "## MindSpore中混合精度与损失缩放的使用\n",
    "\n",
    "MindSpore提供了两种混合精度及损失缩放的使用方式：\n",
    "\n",
    "- 使用函数式编程：使用 `auto_mixed_precision` 实现自动混合精度，通过 `all_finite` 做溢出判断，使用 `StaticLossScaler` 和 `DynamicLossScaler` 手动执行梯度及损失的缩放。\n",
    "\n",
    "- 使用训练接口 `Model` ：配置入参 `amp_level` 设置混合精度的执行策略，配置入参 `loss_scale_manager` 为 `FixedLossScaleManager` 或 `DynamicLossScaleManager` 实现损失缩放；\n",
    "\n",
    "## 使用函数式编程方式实现混合精度与损失缩放\n",
    "\n",
    "MindSpore提供了函数式接口用于混合精度场景，用户可以使用 `auto_mixed_precision` 实现自动混合精度，训练过程中通过 `all_finite` 做溢出判断，使用 `StaticLossScaler` 和 `DynamicLossScaler` 手动执行梯度及损失的缩放。\n",
    "\n",
    "函数式下LossScaler的常见用法：\n",
    "\n",
    "首先导入相关的库，并定义一个LeNet5网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "from mindspore.train import Accuracy\n",
    "import mindspore as ms\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore import dataset as ds\n",
    "from mindspore.amp import auto_mixed_precision, DynamicLossScaler, all_finite\n",
    "from mindspore import jit, ops\n",
    "\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"\n",
    "    Lenet network\n",
    "\n",
    "    Args:\n",
    "        num_class (int): Number of classes. Default: 10.\n",
    "        num_channel (int): Number of channels. Default: 1.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.max_pool2d(self.relu(self.conv1(x)))\n",
    "        x = self.max_pool2d(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "对网络做自动混合精度处理。\n",
    "\n",
    "`auto_mixed_precision` 实现自动混合精度配置含义如下：\n",
    "\n",
    "- 'O0'：保持FP32；\n",
    "- 'O1'：按白名单cast为FP16；\n",
    "- 'O2'：按黑名单保留FP32，其余cast为FP16；\n",
    "- 'O3'：完全cast为FP16；\n",
    "\n",
    "> 当前黑白名单为Cell粒度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = LeNet5(10)\n",
    "auto_mixed_precision(net, 'O1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "实例化LossScaler，并在定义前向网络时，手动放大loss值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction='mean')\n",
    "opt = nn.Adam(net.trainable_params(), learning_rate=0.01)\n",
    "\n",
    "# Define LossScaler\n",
    "loss_scaler = DynamicLossScaler(scale_value=2**10, scale_factor=2, scale_window=50)\n",
    "\n",
    "def net_forward(data, label):\n",
    "    out = net(data)\n",
    "    loss_value = loss_fn(out, label)\n",
    "    # scale up the loss value\n",
    "    scaled_loss = loss_scaler.scale(loss_value)\n",
    "    return scaled_loss, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "反向获取梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grad_fn = ops.value_and_grad(net_forward, None, net.trainable_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "定义训练step：计算当前梯度值并恢复损失。使用 `all_finite` 判断是否出现梯度下溢问题，如果无溢出，恢复梯度并更新网络权重；如果溢出，跳过此step。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step(x, y):\n",
    "    (loss_value, _), grads = grad_fn(x, y)\n",
    "    loss_value = loss_scaler.unscale(loss_value)\n",
    "\n",
    "    is_finite = all_finite(grads)\n",
    "    if is_finite:\n",
    "        grads = loss_scaler.unscale(grads)\n",
    "        loss_value = ops.depend(loss_value, opt(grads))\n",
    "    loss_scaler.adjust(is_finite)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3a957",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接着创建一个虚拟的随机数据集，用于样例模型的数据输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "def get_data(num, img_size=(1, 32, 32), num_classes=10, is_onehot=True):\n",
    "    for _ in range(num):\n",
    "        img = np.random.randn(*img_size)\n",
    "        target = np.random.randint(0, num_classes)\n",
    "        target_ret = np.array([target]).astype(np.float32)\n",
    "        if is_onehot:\n",
    "            target_onehot = np.zeros(shape=(num_classes,))\n",
    "            target_onehot[target] = 1\n",
    "            target_ret = target_onehot.astype(np.float32)\n",
    "        yield img.astype(np.float32), target_ret\n",
    "\n",
    "def create_dataset(num_data=1024, batch_size=32, repeat_size=1):\n",
    "    input_data = ds.GeneratorDataset(list(get_data(num_data)), column_names=['data', 'label'])\n",
    "    input_data = input_data.batch(batch_size, drop_remainder=True)\n",
    "    input_data = input_data.repeat(repeat_size)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "执行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets = create_dataset()\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for data, label in datasets:\n",
    "        loss = train_step(data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 使用训练接口 `Model` 实现混合精度与损失缩放\n",
    "\n",
    "### 混合精度\n",
    "\n",
    "`Model` 接口提供了入参 `amp_level` 实现自动混合精度，用户也可以通过 `to_float(ms.float16)` 把Cell中涉及的算子设置成FP16，实现手动混合精度。\n",
    "\n",
    "#### 自动混合精度\n",
    "\n",
    "使用自动混合精度，需要调用`Model`接口，将待训练网络和优化器作为输入传入，该接口会根据设定策略把对应的网络模型的的算子转换成FP16算子。\n",
    "\n",
    "> 除`BatchNorm`算子和Loss涉及到的算子外因为精度问题，仍然使用FP32执行运算。\n",
    "\n",
    "使用`Model`接口具体的实现步骤为：\n",
    "\n",
    "1. 引入MindSpore的模型训练接口`Model`；\n",
    "\n",
    "2. 定义网络：该步骤和正常的网络定义相同(无需新增任何配置)；\n",
    "\n",
    "3. 创建数据集：该步骤可参考[数据处理](https://www.mindspore.cn/tutorials/zh-CN/master/advanced/dataset.html)；\n",
    "\n",
    "4. 使用`Model`接口封装网络模型、优化器和损失函数，设置`amp_level`参数，详情参考[MindSpore API](https://www.mindspore.cn/docs/zh-CN/master/api_python/train/mindspore.train.Model.html#mindspore.train.Model)。该步骤MindSpore会自动选择合适的算子自动进行FP32到FP16的类型转换。\n",
    "\n",
    "下面是基础的代码样例，首先导入必须的库和声明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "from mindspore.train import Accuracy, Model\n",
    "import mindspore as ms\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore import dataset as ds\n",
    "\n",
    "ms.set_context(mode=ms.GRAPH_MODE)\n",
    "ms.set_context(device_target=\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接着创建一个虚拟的随机数据集，用于样例模型的数据输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "def get_data(num, img_size=(1, 32, 32), num_classes=10, is_onehot=True):\n",
    "    for _ in range(num):\n",
    "        img = np.random.randn(*img_size)\n",
    "        target = np.random.randint(0, num_classes)\n",
    "        target_ret = np.array([target]).astype(np.float32)\n",
    "        if is_onehot:\n",
    "            target_onehot = np.zeros(shape=(num_classes,))\n",
    "            target_onehot[target] = 1\n",
    "            target_ret = target_onehot.astype(np.float32)\n",
    "        yield img.astype(np.float32), target_ret\n",
    "\n",
    "def create_dataset(num_data=1024, batch_size=32, repeat_size=1):\n",
    "    input_data = ds.GeneratorDataset(list(get_data(num_data)), column_names=['data', 'label'])\n",
    "    input_data = input_data.batch(batch_size, drop_remainder=True)\n",
    "    input_data = input_data.repeat(repeat_size)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以LeNet5网络为例，设置`amp_level`参数，使用`Model`接口封装网络模型、优化器和损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = create_dataset()\n",
    "\n",
    "# Initialize network\n",
    "network = LeNet5(10)\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(reduction=\"mean\")\n",
    "net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
    "# Set amp level\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()}, amp_level=\"O3\")\n",
    "\n",
    "# Run training\n",
    "model.train(epoch=10, train_dataset=ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 手动混合精度\n",
    "\n",
    "MindSpore目前还支持手动混合精度（一般不建议使用手动混合精度，除非自定义特殊网络和特性开发）。\n",
    "\n",
    "假定在网络中只有一个Conv Layer使用FP16计算，其他Layer都用FP32计算。\n",
    "\n",
    "> 混合精度配置以Cell为单位，Cell默认是FP32类型。\n",
    "\n",
    "以下是一个手动混合精度的实现步骤：\n",
    "\n",
    "1. 定义网络：该步骤与自动混合精度中的步骤2类似；\n",
    "\n",
    "2. 配置混合精度：通过`to_float(ms.float16)`把Cell中涉及的算子配置成FP16；\n",
    "\n",
    "3. 使用`TrainOneStepCell`封装网络模型和优化器。\n",
    "\n",
    "下面是基础的代码样例，首先导入必须的库和声明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mindspore.nn as nn\n",
    "from mindspore.train import Accuracy, Model\n",
    "import mindspore as ms\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore import dataset as ds\n",
    "import mindspore.ops as ops\n",
    "\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target=\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在初始化网络模型后，声明LeNet5中的Conv1层使用FP16进行计算，即`network.conv1.to_float(mstype.float16)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = create_dataset()\n",
    "network = LeNet5(10)\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(reduction=\"mean\")\n",
    "net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
    "network.conv1.to_float(ms.float16)\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()}, amp_level=\"O2\")\n",
    "model.train(epoch=2, train_dataset=ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> 使用混合精度时，只能由自动微分功能生成反向网络，不能由用户自定义生成反向网络，否则可能会导致MindSpore产生数据格式不匹配的异常信息。\n",
    "\n",
    "### 损失缩放\n",
    "\n",
    "下面将会分别介绍MindSpore中，配合 `Model` 接口使用损失缩放算法的主要两个API [FixedLossScaleManager](https://www.mindspore.cn/docs/zh-CN/master/api_python/amp/mindspore.amp.FixedLossScaleManager.html)和[DynamicLossScaleManager](https://www.mindspore.cn/docs/zh-CN/master/api_python/amp/mindspore.amp.DynamicLossScaleManager.html)。\n",
    "\n",
    "#### FixedLossScaleManager\n",
    "\n",
    "`FixedLossScaleManager`在进行缩放的时候，不会改变scale的大小，scale的值由入参loss_scale控制，可以由用户指定，不指定则取默认值。\n",
    "\n",
    "`FixedLossScaleManager`的另一个参数是`drop_overflow_update`，用来控制发生溢出时是否更新参数。\n",
    "\n",
    "一般情况下LossScale功能不需要和优化器配合使用，但使用`FixedLossScaleManager`时，如果`drop_overflow_update`为False，那么优化器需设置`loss_scale`的值，且`loss_scale`的值要与`FixedLossScaleManager`的相同。\n",
    "\n",
    "`FixedLossScaleManager`具体用法如下：\n",
    "\n",
    "import必要的库，并声明使用图模式下执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from mindspore import amp\n",
    "from mindspore.train import Accuracy, Model\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore import dataset as ds\n",
    "\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target=\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以LeNet5为例定义网络模型；定义数据集和训练流程中常用的接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = create_dataset()\n",
    "# Initialize network\n",
    "network = LeNet5(10)\n",
    "# Define Loss and Optimizer\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用Loss Scale的API接口，作用于优化器和模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Loss Scale, optimizer and model\n",
    "#1) Drop the parameter update if there is an overflow\n",
    "loss_scale_manager = amp.FixedLossScaleManager()\n",
    "net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()}, amp_level=\"O0\", loss_scale_manager=loss_scale_manager)\n",
    "\n",
    "#2) Execute parameter update even if overflow occurs\n",
    "loss_scale = 1024.0\n",
    "loss_scale_manager = amp.FixedLossScaleManager(loss_scale, False)\n",
    "net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9, loss_scale=loss_scale)\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()}, amp_level=\"O0\", loss_scale_manager=loss_scale_manager)\n",
    "\n",
    "# Run training\n",
    "model.train(epoch=10, train_dataset=ds_train, callbacks=[ms.LossMonitor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### LossScale与优化器\n",
    "\n",
    "前面提到了使用`FixedLossScaleManager`且`drop_overflow_update`为False时，优化器需要配合使用。\n",
    "\n",
    "这是由于采用此方式进行配置时，梯度与`loss_scale`系数之间的除法运算在优化器中进行。优化器设置与`FixedLossScaleManager`相同的`loss_scale`，训练结果才是正确的。\n",
    "\n",
    "> 后续MindSpore会优化不同场景下溢出检测功能的用法，并逐步移除优化器中的`loss_scale`参数，到时便无需配置优化器的`loss_scale`参数。\n",
    "\n",
    "需要注意的是，当前MindSpore提供的部分优化器如`AdamWeightDecay`，未提供`loss_scale`参数。如果使用`FixedLossScaleManager`，且`drop_overflow_update`配置为False，优化器中未能进行梯度与`loss_scale`之间的除法运算，此时需要自定义`TrainOneStepCell`，并在其中对梯度除`loss_scale`，以使最终的计算结果正确，定义方式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindspore.train import Model\n",
    "from mindspore import nn, ops\n",
    "\n",
    "grad_scale = ops.MultitypeFuncGraph(\"grad_scale\")\n",
    "\n",
    "@grad_scale.register(\"Tensor\", \"Tensor\")\n",
    "def gradient_scale(scale, grad):\n",
    "    return grad * ops.cast(scale, ops.dtype(grad))\n",
    "\n",
    "class CustomTrainOneStepCell(nn.TrainOneStepCell):\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(CustomTrainOneStepCell, self).__init__(network, optimizer, sens)\n",
    "        self.hyper_map = ops.HyperMap()\n",
    "        self.reciprocal_sense = ms.Tensor(1 / sens, ms.float32)\n",
    "\n",
    "    def scale_grad(self, gradients):\n",
    "        gradients = self.hyper_map(ops.partial(grad_scale, self.reciprocal_sense), gradients)\n",
    "        return gradients\n",
    "\n",
    "    def construct(self, *inputs):\n",
    "        loss = self.network(*inputs)\n",
    "        sens = ops.fill(loss.dtype, loss.shape, self.sens)\n",
    "        # calculate gradients, the sens will equal to the loss_scale\n",
    "        grads = self.grad(self.network, self.weights)(*inputs, sens)\n",
    "        # gradients / loss_scale\n",
    "        grads = self.scale_grad(grads)\n",
    "        # reduce gradients in distributed scenarios\n",
    "        grads = self.grad_reducer(grads)\n",
    "        loss = ops.depend(loss, self.optimizer(grads))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- network：参与训练的网络，该网络包含前向网络和损失函数的计算逻辑，输入数据和标签，输出损失函数值。\n",
    "- optimizer：所使用的优化器。\n",
    "- sens：参数用于接收用户指定的`loss_scale`，训练过程中梯度值会放大`loss_scale`倍。\n",
    "- scale_grad函数：用于梯度与`loss_scale`系数之间的除法运算，还原梯度。\n",
    "- construct函数：参照`nn.TrainOneStepCell`定义`construct`的计算逻辑，并在获取梯度后调用`scale_grad`。\n",
    "\n",
    "自定义`TrainOneStepCell`后，需要手动构建训练网络，如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mindspore import nn\n",
    "from mindspore import amp\n",
    "\n",
    "network = LeNet5(10)\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(reduction=\"mean\")\n",
    "net_opt = nn.AdamWeightDecay(network.trainable_params(), learning_rate=0.01)\n",
    "\n",
    "# Define LossScaleManager\n",
    "loss_scale = 1024.0\n",
    "loss_scale_manager = amp.FixedLossScaleManager(loss_scale, False)\n",
    "\n",
    "# Build train network\n",
    "net_with_loss = nn.WithLossCell(network, net_loss)\n",
    "net_with_train = CustomTrainOneStepCell(net_with_loss, net_opt, loss_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "构建训练网络后可以直接运行或通过Model运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "#1) Execute net_with_train\n",
    "ds_train = create_dataset()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for d in ds_train.create_dict_iterator():\n",
    "        result = net_with_train(d[\"data\"], d[\"label\"])\n",
    "\n",
    "#2) Define Model and run\n",
    "model = Model(net_with_train)\n",
    "\n",
    "ds_train = create_dataset()\n",
    "\n",
    "model.train(epoch=epochs, train_dataset=ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在此场景下使用`Model`进行训练时，`loss_scale_manager`和`amp_level`无需配置，因为`CustomTrainOneStepCell`中已经包含了混合精度的计算逻辑。\n",
    "\n",
    "#### DynamicLossScaleManager\n",
    "\n",
    "`DynamicLossScaleManager`在训练过程中可以动态改变scale的大小，在没有发生溢出的情况下，要尽可能保持较大的scale。\n",
    "\n",
    "`DynamicLossScaleManager`会首先将scale设置为一个初始值，该值由入参init_loss_scale控制。\n",
    "\n",
    "在训练过程中，如果不发生溢出，在更新scale_window次参数后，会尝试扩大scale的值，如果发生了溢出，则跳过参数更新，并缩小scale的值，入参scale_factor是控制扩大或缩小的步数，scale_window控制没有发生溢出时，最大的连续更新步数。\n",
    "\n",
    "具体用法如下，仅需将`FixedLossScaleManager`样例中定义LossScale，优化器和模型部分的代码改成如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Loss Scale, optimizer and model\n",
    "scale_factor = 4\n",
    "scale_window = 3000\n",
    "loss_scale_manager = amp.DynamicLossScaleManager(scale_factor, scale_window)\n",
    "net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()}, amp_level=\"O0\", loss_scale_manager=loss_scale_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> 图片引用自[automatic-mixed-precision](https://developer.nvidia.com/automatic-mixed-precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
