# 使用MindSpore Golden Stick中的感知量化算法

<a href="https://gitee.com/mindspore/docs/blob/master/docs/golden_stick/docs/source_zh_cn/quantization.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png"></a>
&nbsp;&nbsp;

## 背景

随着深度学习的发展，神经网络被广泛应用于各种领域，模型性能提高的同时也引入了巨大的参数量和计算量。越来越多的应用选择在移动设备或者边缘设备上使用深度学习技术。

以手机为例，为了提供人性化和智能化的服务，操作系统和APP应用都开始集成AI功能。而使用该功能，涉及训练或者推理，自然包含大量的网络模型及权重文件。以经典的AlexNet为例，原始权重文件已经超过了200MB，而最近出现的新模型正往结构更复杂、参数更多的方向发展。

## 概述

由于移动设备、边缘设备的硬件资源有限，需要对模型进行精简，而量化（Quantization）技术就是应对该类问题衍生出的技术之一。模型量化是一种将浮点计算转成低比特定点计算的技术，可以有效地降低模型计算强度、参数大小和内存消耗，但往往带来巨大的精度损失。

### 量化方法

模型量化即以较低的推理精度损失将连续取值（或者大量可能的离散取值）的浮点型模型权重或流经模型的张量数据定点近似为（通常为int8）有限多个（或较少的）离散值的过程，它是以更少位数的数据类型用于近似表示32位有限范围浮点型数据的过程，而模型的输入输出依然是浮点型，从而达到减少模型尺寸大小、减少模型内存消耗及加快模型推理速度等目标。

首先量化会损失精度，这相当于给网络引入了噪声，但是神经网络一般对噪声是不太敏感的，只要控制好量化的程度，对高级任务精度影响可以做到很小。

其次，传统的卷积操作都是使用FP32浮点，浮点运算时需要很多时间周期来完成，但是如果我们将权重参数和激活在输入各个层之前量化到INT8，减少了位数和乘法操作，而且此时做的卷积操作都是整型的乘加运算，比浮点快很多。

![](../images/quantization_aware_training1.png)

如上图所示，与FP32类型相比，FP16、INT8等低精度数据表达类型所占用空间更小。使用低精度数据表达类型替换高精度数据表达类型，可以大幅降低存储空间和传输时间。而低比特的计算性能也更高，INT8相对比FP32的加速比可达到3倍甚至更高，对于相同的计算，功耗上也有明显优势。

当前业界量化方案主要分为两种：**感知量化训练**（Quantization Aware Training）和**训练后量化**（Post-training Quantization）。

1）**感知量化训练**需要训练数据，在模型准确率上通常表现更好，适用于对模型压缩率和模型准确率要求较高的场景。目的是减少精度损失，其参与模型训练的前向推理过程令模型获得量化损失的差值，但梯度更新需要在浮点下进行，因而其并不参与反向传播过程。

2）**训练后量化**简单易用，只需少量校准数据，适用于追求高易用性和缺乏训练资源的场景。

3）**训练后校正量化**同样是训练后静态量化，也称为校正量化或者数据集量化。其原理是在端侧低比特推理时，生成一个校准表来量化模型。

### 伪量化节点

伪量化节点，是指感知量化训练中插入的节点，用以寻找网络数据分布，并反馈损失精度，具体作用如下：

- 找到网络数据的分布，即找到待量化参数的最大值和最小值；
- 模拟量化为低比特时的精度损失，把该损失作用到网络模型中，传递给损失函数，让优化器在训练过程中对该损失值进行优化。

### BatchNorm折叠

基本卷积Conv操作为：

$$y_{out}=w \cdot x+b$$

大部分网络模型中为了对卷积后的数据进行规约，会使用BatchNorm层。因此Conv和BatchNorm两个算子在正向传播的时候，根据公式可以融合为一个算子，该操作称为BN折叠：

$$y_{bn}=\operatorname{BN}\left(y_{cout}\right)=BN(w \cdot x+b)=\widehat{w} \cdot x+\widehat{b}$$

网络模型中经常使用BN层对数据进行规约，有效降低上下层之间的数据依赖性。然而大部分在推理框架当中都会对BN层和卷积操作进行融合，为了更好地模拟在推理时的算子融合操作，这里对BN层进行folding处理。MindSpore在进行感知量化训练的时候会默认进行BN折叠。

下面左边图是普通训练时模型，右边图是融合后推理时模型。

![](../images/quantization_aware_training2.png)

## 感知量化训练

MindSpore的感知量化训练是指在训练时使用伪量化节点来模拟量化操作，过程中仍然采用浮点数计算，并通过反向传播学习更新网络参数，使得网络参数更好地适应量化带来的损失。对于权值和数据的量化，MindSpore采用了参考文献[1]中的方案。

表1：感知量化训练规格

| 规格 | 规格说明 |
| --- | --- |
| 硬件支持 | GPU |
| 网络支持 | 已实现的网络包括LeNet、ResNet50等网络，具体请参见<https://gitee.com/mindspore/models/tree/master>。 |
| 算法支持 | 支持非对称和对称的量化算法；支持逐层和逐通道的量化算法。|
| 方案支持 | 支持8比特的量化方案。 |
| 数据类型支持 | GPU平台支持FP32。 |
| 运行模式支持 | Graph模式和PyNative模式 |

## 感知量化训练示例

### 应用量化算法生成量化网络

### 导出量化模型

## 参考文献

[1] Jacob B, Kligys S, Chen B, et al. Quantization and training of neural networks for efficient integer-arithmetic-only inference[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2704-2713.
