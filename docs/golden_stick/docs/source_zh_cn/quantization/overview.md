# 量化算法概述

<a href="https://gitee.com/mindspore/docs/blob/r1.8/docs/golden_stick/docs/source_zh_cn/quantization/overview.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source.png"></a>

本文是在介绍具体的量化算法之前，介绍一些量化算法的基本概念，帮助用户理解。如果已经对量化算法有较深的理解，可以直接跳转到[示例](#示例)小节。

## 背景

随着深度学习的发展，神经网络被广泛应用于各种领域，模型性能提高的同时也引入了巨大的参数量和计算量。越来越多的应用选择在移动设备或者边缘设备上使用深度学习技术。

以手机为例，为了提供人性化和智能化的服务，操作系统和APP应用都开始集成AI功能。而使用该功能，涉及训练或者推理，自然包含大量的网络模型及权重文件。以经典的AlexNet为例，原始权重文件已经超过了200MB，而最近出现的新模型正往结构更复杂、参数更多的方向发展。

由于移动设备、边缘设备的硬件资源有限，需要对模型进行精简，而量化（Quantization）技术就是应对该类问题衍生出的技术之一。模型量化是一种将浮点计算转成低比特定点计算的技术，可以有效地降低模型计算强度、参数大小和内存消耗，但往往带来巨大的精度损失。

## 量化方法

模型量化即以较低的推理精度损失将连续取值（或者大量可能的离散取值）的浮点型模型权重或流经模型的张量数据定点近似为（通常为int8）有限多个（或较少的）离散值的过程，它是以更少位数的数据类型用于近似表示32位有限范围浮点型数据的过程，而模型的输入输出依然是浮点型，从而达到减少模型尺寸大小、减少模型内存消耗及加快模型推理速度等目标。

首先量化会损失精度，这相当于给网络引入了噪声，但是神经网络一般对噪声是不太敏感的，只要控制好量化的程度，对高级任务精度影响可以做到很小。

其次，传统的卷积操作都是使用FP32浮点，浮点运算时需要很多时间周期来完成，但是如果我们将权重参数和激活在输入各个层之前量化到INT8，减少了位数和乘法操作，而且此时做的卷积操作都是整型的乘加运算，比浮点快很多。

![](../images/quantization/bit_define.png)

如上图所示，与FP32类型相比，FP16、INT8等低精度数据表达类型所占用空间更小。使用低精度数据表达类型替换高精度数据表达类型，可以大幅降低存储空间和传输时间。而低比特的计算性能也更高，INT8相对比FP32的加速比可达到3倍甚至更高，对于相同的计算，功耗上也有明显优势。

当前业界量化方案主要分为两种：**感知量化训练**（Quantization Aware Training）和**训练后量化**（Post-training Quantization）。

1）**感知量化训练**需要训练数据，在模型准确率上通常表现更好，适用于对模型压缩率和模型准确率要求较高的场景。目的是减少精度损失，其参与模型训练的前向推理过程令模型获得量化损失的差值，但梯度更新需要在浮点下进行，因而其并不参与反向传播过程。

2）**训练后量化**简单易用，只需少量校准数据，适用于追求高易用性和缺乏训练资源的场景。

3）**训练后校正量化**同样是训练后静态量化，也称为校正量化或者数据集量化。其原理是在端侧低比特推理时，生成一个校准表来量化模型。

## 示例

- [SimQAT算法示例](https://www.mindspore.cn/golden_stick/docs/zh-CN/r1.8/quantization/simqat.html)：一种基础的基于伪量化技术的感知量化算法
- [SLB量化算法示例](https://www.mindspore.cn/golden_stick/docs/zh-CN/r1.8/quantization/slb.html)：一种非线性的低比特感知量化算法

