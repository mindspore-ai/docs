# Copyright 2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

"""Distributed Operator Parallel Example"""

import os
import mindspore as ms
import mindspore.dataset as ds
import mindspore.runtime as rt
from mindspore import nn, ops
from mindspore.communication import init
from mindspore.common.initializer import initializer
from mindspore.nn.utils import no_init_parameters
from mindspore.parallel.auto_parallel import AutoParallel

ms.set_context(mode=ms.GRAPH_MODE)
rt.set_memory(max_size="28GB")
init()
ms.set_seed(1)

class Network(nn.Cell):
    """Network"""
    def __init__(self):
        super().__init__()
        self.flatten = ops.Flatten()
        self.fc1_weight = ms.Parameter(initializer("normal", [28*28, 512], ms.float32))
        self.fc2_weight = ms.Parameter(initializer("normal", [512, 512], ms.float32))
        self.fc3_weight = ms.Parameter(initializer("normal", [512, 10], ms.float32))
        self.matmul1 = ops.MatMul().shard(((2, 4), (4, 1)))
        self.relu1 = ops.ReLU().shard(((4, 1),))
        self.matmul2 = ops.MatMul().shard(((1, 8), (8, 1)))
        self.relu2 = ops.ReLU().shard(((8, 1),))
        self.matmul3 = ops.MatMul()

    def construct(self, x):
        x = self.flatten(x)
        x = self.matmul1(x, self.fc1_weight)
        x = self.relu1(x)
        x = self.matmul2(x, self.fc2_weight)
        x = self.relu2(x)
        logits = self.matmul3(x, self.fc3_weight)
        return logits



def create_dataset(batch_size):
    """create dataset"""
    dataset_path = os.getenv("DATA_PATH")
    dataset = ds.MnistDataset(dataset_path)
    image_transforms = [
        ds.vision.Rescale(1.0 / 255.0, 0),
        ds.vision.Normalize(mean=(0.1307,), std=(0.3081,)),
        ds.vision.HWC2CHW()
    ]
    label_transform = ds.transforms.TypeCast(ms.int32)
    dataset = dataset.map(image_transforms, 'image')
    dataset = dataset.map(label_transform, 'label')
    dataset = dataset.batch(batch_size)
    return dataset

def test_distributed_operator_parallel():
    """
    Test distributed operator parallel
    """
    data_set = create_dataset(32)

    with no_init_parameters():
        net = Network()
        optimizer = nn.SGD(net.trainable_params(), 1e-2)

    loss_fn = nn.CrossEntropyLoss()

    def forward_fn(data, target):
        """forward propagation"""
        logits = net(data)
        loss = loss_fn(logits, target)
        return loss, logits

    grad_fn = ms.value_and_grad(forward_fn, None, net.trainable_params(), has_aux=True)

    def train_step(inputs, targets):
        """train_step"""
        (loss_value, _), grads = grad_fn(inputs, targets)
        optimizer(grads)
        return loss_value

    parallel_net = AutoParallel(train_step, parallel_mode="semi_auto")

    for epoch in range(10):
        i = 0
        for image, label in data_set:
            loss_output = parallel_net(image, label)
            if i % 10 == 0:
                print("epoch: %s, step: %s, loss is %s" % (epoch, i, loss_output))
            i += 1
