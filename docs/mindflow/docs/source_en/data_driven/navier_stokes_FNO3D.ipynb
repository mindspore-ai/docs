{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "# FNO for 3D Navier-Stokes\n",
                "\n",
                "[![DownloadNotebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.5.0/resource/_static/logo_notebook_en.svg)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/master/mindflow/en/data_driven/mindspore_navier_stokes_FNO3D.ipynb) [![DownloadCode](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.5.0/resource/_static/logo_download_code_en.svg)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/master/mindflow/en/data_driven/mindspore_navier_stokes_FNO3D.py) [![View Source On Gitee](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.5.0/resource/_static/logo_source_en.svg)](https://gitee.com/mindspore/docs/blob/r2.5.0/docs/mindflow/docs/source_en/data_driven/navier_stokes_FNO3D.ipynb)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "## Overview\n",
                "\n",
                "Computational fluid dynamics is one of the most important techniques in the field of fluid mechanics in the 21st century. The flow analysis, prediction and control can be realized by solving the governing equations of fluid mechanics by numerical method. Traditional finite element method (FEM) and finite difference method (FDM) are inefficient because of the complex simulation process (physical modeling, meshing, numerical discretization, iterative solution, etc.) and high computing costs. Therefore, it is necessary to improve the efficiency of fluid simulation with AI.\n",
                "\n",
                "Machine learning methods provide a new paradigm for scientific computing by providing a fast solver similar to traditional methods. Classical neural networks learn mappings between finite dimensional spaces and can only learn solutions related to specific discretizations. Different from traditional neural networks, Fourier Neural Operator (FNO) is a new deep learning architecture that can learn mappings between infinite-dimensional function spaces. It directly learns mappings from arbitrary function parameters to solutions to solve a class of partial differential equations. Therefore, it has a stronger generalization capability. More information can be found in the paper, [Fourier Neural Operator for Parametric Partial Differential Equations](https://arxiv.org/abs/2010.08895).\n",
                "\n",
                "This tutorial describes how to solve the Navier-Stokes equation using 3-d Fourier neural operator.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "## Navier-Stokes equation\n",
                "\n",
                "Navier-Stokes equation is a classical equation in computational fluid dynamics. It is a set of partial differential equations describing the conservation of fluid momentum, called N-S equation for short. Its vorticity form in two-dimensional incompressible flows is as follows:\n",
                "\n",
                "$$ \\partial_t w(x, t)+u(x, t) \\cdot \\nabla w(x, t)=\\nu \\Delta w(x, t)+f(x), \\quad x \\in(0,1)^2, t \\in(0, T] $$\n",
                "\n",
                "$$ \\nabla \\cdot u(x, t)=0, \\quad x \\in(0,1)^2, t \\in[0, T] $$\n",
                "\n",
                "$$ w(x, 0)=w_0(x), \\quad x \\in(0,1)^2 $$\n",
                "\n",
                "where $u$ is the velocity field, $w=\\nabla \\times u$ is the vorticity, $w_0(x)$ is the initial vorticity, $\\nu$ is the viscosity coefficient, $f(x)$ is the forcing function.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Problem Description\n",
                "\n",
                "We aim to solve two-dimensional incompressible N-S equation by learning the operator mapping from each time step to the next time step:\n",
                "\n",
                "$$ w_t \\mapsto w(\\cdot, t+1) $$\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Technology Path\n",
                "\n",
                "MindSpore Flow solves the problem as follows:\n",
                "\n",
                "1. Training Dataset Construction.\n",
                "2. Model Construction.\n",
                "3. Optimizer and Loss Function.\n",
                "4. Model Training.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fourier Neural Operator\n",
                "\n",
                "The following figure shows the architecture of the Fourier Neural Operator model. In the figure, $w_0(x)$ represents the initial vorticity. The input vector is lifted to higher dimension channel space by the lifting layer. Then the mapping result is used as the input of the Fourier layer to perform nonlinear transformation of the frequency domain information. Finally, the decoding layer maps the transformation result to the final prediction result $w_1(x)$.\n",
                "\n",
                "The Fourier Neural Operator consists of the lifting Layer, Fourier Layers, and the decoding Layer.\n",
                "\n",
                "![Fourier Neural Operator model structure](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.5.0/docs/mindflow/docs/source_en/data_driven/images/FNO.png)\n",
                "\n",
                "Fourier layers: Start from input V. On top: apply the Fourier transform $\\mathcal{F}$; a linear transform R on the lower Fourier modes and filters out the higher modes; then apply the inverse Fourier transform $\\mathcal{F}^{-1}$. On the bottom: apply a local linear transform W. Finally, the Fourier Layer output vector is obtained through the activation function.\n",
                "\n",
                "![Fourier Layer structure](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.5.0/docs/mindflow/docs/source_en/data_driven/images/FNO-2.png)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "import numpy as np\n",
                "\n",
                "from mindspore import nn, ops, jit, data_sink, save_checkpoint, context, Tensor, ops\n",
                "from mindspore import set_seed\n",
                "from mindspore import dtype as mstype\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The following `src` pacakage can be downloaded in [applications/data_driven/navier_stokes/fno3d/src](https://gitee.com/mindspore/mindscience/tree/r0.7/MindFlow/applications/data_driven/navier_stokes/fno3d/src).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "from mindflow import get_warmup_cosine_annealing_lr, load_yaml_config\n",
                "from mindflow.cell.neural_operators.fno import FNO3D\n",
                "\n",
                "from src import LpLoss, UnitGaussianNormalizer, create_training_dataset\n",
                "\n",
                "set_seed(0)\n",
                "np.random.seed(0)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "# set context for training: using graph mode for high performance training with GPU acceleration\n",
                "context.set_context(mode=context.GRAPH_MODE, device_target='GPU', device_id=0)\n",
                "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
                "config = load_yaml_config('./configs/fno3d.yaml')\n",
                "data_params = config[\"data\"]\n",
                "model_params = config[\"model\"]\n",
                "optimizer_params = config[\"optimizer\"]\n",
                "\n",
                "sub = model_params[\"sub\"]\n",
                "grid_size = model_params[\"input_resolution\"] // sub\n",
                "input_timestep = model_params[\"input_timestep\"]\n",
                "output_timestep = model_params[\"output_timestep\"]\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "## Training Dataset Construction\n",
                "\n",
                "Download the training and test dataset: [data_driven/navier_stokes/dataset](https://download.mindspore.cn/mindscience/mindflow/dataset/applications/data_driven/navier_stokes/dataset/) .\n",
                "\n",
                "In this case, training data sets and test data sets are generated according to Zongyi Li's data set in [Fourier Neural Operator for Parametric Partial Differential Equations](https://arxiv.org/pdf/2010.08895.pdf) . The settings are as follows:\n",
                "\n",
                "The initial condition $w_0(x)$ is generated according to periodic boundary conditions:\n",
                "\n",
                "$$ w_0 \\sim \\mu, \\mu=\\mathcal{N}\\left(0,7^{3 / 2}(-\\Delta+49 I)^{-2.5}\\right) $$\n",
                "\n",
                "The forcing function is defined as:\n",
                "\n",
                "$$ f(x)=0.1\\left(\\sin \\left(2 \\pi\\left(x_1+x_2\\right)\\right)+\\right.\\cos(2 \\pi(x_1+x_2))) $$\n",
                "\n",
                "We use a time-step of 1e-4 for the Crank–Nicolson scheme in the data-generated process where we record the solution every t = 1 time units. All data are generated on a 256 × 256 grid and are downsampled to 64 × 64\\. In this case, the viscosity coefficient $\\nu=1e-4$, the number of samples in the training set is 1000, and the number of samples in the test set is 200.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data preparation finished\n"
                    ]
                }
            ],
            "source": [
                "train_a = Tensor(np.load(os.path.join(\n",
                "    data_params[\"path\"], \"train_a.npy\")), mstype.float32)\n",
                "train_u = Tensor(np.load(os.path.join(\n",
                "    data_params[\"path\"], \"train_u.npy\")), mstype.float32)\n",
                "test_a = Tensor(np.load(os.path.join(\n",
                "    data_params[\"path\"], \"test_a.npy\")), mstype.float32)\n",
                "test_u = Tensor(np.load(os.path.join(\n",
                "    data_params[\"path\"], \"test_u.npy\")), mstype.float32)\n",
                "train_loader = create_training_dataset(data_params,\n",
                "                                       shuffle=True)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "## Model Construction\n",
                "\n",
                "The network is composed of 1 lifting layer, multiple Fourier layers and 1 decoding layer:\n",
                "\n",
                "- The Lifting layer corresponds to the `FNO3D.fc0` in the case, and maps the output data $x$ to the high dimension;\n",
                "\n",
                "- Multi-layer Fourier Layer corresponds to the `FNO3D.fno_seq` in the case. Discrete Fourier transform is used to realize the conversion between time domain and frequency domain;\n",
                "\n",
                "- The Decoding layer corresponds to `FNO3D.fc1` and `FNO3D.fc2` in the case to obtain the final predictive value.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "if use_ascend:\n",
                "    compute_type = mstype.float16\n",
                "else:\n",
                "    compute_type = mstype.float32\n",
                "# prepare model\n",
                "model = FNO3D(in_channels=model_params[\"in_channels\"],\n",
                "              out_channels=model_params[\"out_channels\"],\n",
                "              n_modes=model_params[\"modes\"],\n",
                "              resolutions=[model_params[\"input_resolution\"],\n",
                "                           model_params[\"input_resolution\"], output_timestep],\n",
                "              hidden_channels=model_params[\"width\"],\n",
                "              n_layers=model_params[\"depth\"],\n",
                "              projection_channels=4*model_params[\"width\"],\n",
                "              fno_compute_dtype=compute_type\n",
                "              )\n",
                "\n",
                "model_params_list = []\n",
                "for k, v in model_params.items():\n",
                "    model_params_list.append(f\"{k}-{v}\")\n",
                "model_name = \"_\".join(model_params_list)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "## Optimizer and Loss Function\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "lr = get_warmup_cosine_annealing_lr(lr_init=optimizer_params[\"initial_lr\"],\n",
                "                                    last_epoch=optimizer_params[\"train_epochs\"],\n",
                "                                    steps_per_epoch=train_loader.get_dataset_size(),\n",
                "                                    warmup_epochs=optimizer_params[\"warmup_epochs\"])\n",
                "optimizer = nn.optim.Adam(model.trainable_params(),\n",
                "                          learning_rate=Tensor(lr), weight_decay=optimizer_params['weight_decay'])\n",
                "loss_fn = LpLoss()\n",
                "a_normalizer = UnitGaussianNormalizer(train_a)\n",
                "y_normalizer = UnitGaussianNormalizer(train_u)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_l2_error(model, inputs, labels):\n",
                "    \"\"\"\n",
                "    Evaluate the model respect to input data and label.\n",
                "    Args:\n",
                "        model (Cell): list of expressions node can by identified by mindspore.\n",
                "        inputs (Tensor): the input data of network.\n",
                "        labels (Tensor): the true output value of given inputs.\n",
                "    \"\"\"\n",
                "    print(\"================================Start Evaluation================================\")\n",
                "    time_beg = time.time()\n",
                "    rms_error = 0.0\n",
                "    for i in range(labels.shape[0]):\n",
                "        label = labels[i:i + 1]\n",
                "        test_batch = inputs[i:i + 1]\n",
                "        test_batch = a_normalizer.encode(test_batch)\n",
                "        label = y_normalizer.encode(label)\n",
                "        test_batch = test_batch.reshape(\n",
                "            1, grid_size, grid_size, 1, input_timestep).repeat(output_timestep, axis=3)\n",
                "        prediction = model(test_batch).reshape(\n",
                "            1, grid_size, grid_size, output_timestep)\n",
                "        prediction = y_normalizer.decode(prediction)\n",
                "        label = y_normalizer.decode(label)\n",
                "        rms_error_step = loss_fn(prediction.reshape(\n",
                "            1, -1), label.reshape(1, -1))\n",
                "        rms_error += rms_error_step\n",
                "\n",
                "    rms_error = rms_error / labels.shape[0]\n",
                "    print(\"mean rms_error:\", rms_error)\n",
                "    print(\"predict total time: {} s\".format(time.time() - time_beg))\n",
                "    print(\"=================================End Evaluation=================================\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Function\n",
                "\n",
                "With MindSpore>= 2.0.0, you can train neural networks using functional programming paradigms, and single-step training functions are decorated with jit. The data_sink function is used to transfer the step-by-step training function and training dataset.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def forward_fn(data, label):\n",
                "    bs = data.shape[0]\n",
                "    data = a_normalizer.encode(data)\n",
                "    label = y_normalizer.encode(label)\n",
                "    data = data.reshape(bs, grid_size, grid_size, 1, input_timestep).repeat(\n",
                "        output_timestep, axis=3)\n",
                "    logits = model(data).reshape(bs, grid_size, grid_size, output_timestep)\n",
                "    logits = y_normalizer.decode(logits)\n",
                "    label = y_normalizer.decode(label)\n",
                "    loss = loss_fn(logits.reshape(bs, -1), label.reshape(bs, -1))\n",
                "    return loss\n",
                "\n",
                "\n",
                "grad_fn = ops.value_and_grad(\n",
                "    forward_fn, None, optimizer.parameters, has_aux=False)\n",
                "\n",
                "\n",
                "@jit\n",
                "def train_step(data, label):\n",
                "    loss, grads = grad_fn(data, label)\n",
                "    loss = ops.depend(loss, optimizer(grads))\n",
                "    return loss\n",
                "\n",
                "\n",
                "sink_process = data_sink(train_step, train_loader, sink_size=100)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "## Model Training\n",
                "\n",
                "With **MindSpore version >= 2.0.0**, we can use the functional programming for training neural networks.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "def train():\n",
                "    def forward_fn(data, label):\n",
                "        bs = data.shape[0]\n",
                "        data = a_normalizer.encode(data)\n",
                "        label = y_normalizer.encode(label)\n",
                "        data = data.reshape(bs, grid_size, grid_size, 1, input_timestep).repeat(\n",
                "            output_timestep, axis=3)\n",
                "        logits = model(data).reshape(bs, grid_size, grid_size, output_timestep)\n",
                "        logits = y_normalizer.decode(logits)\n",
                "        label = y_normalizer.decode(label)\n",
                "        loss = loss_fn(logits.reshape(bs, -1), label.reshape(bs, -1))\n",
                "        return loss\n",
                "\n",
                "    grad_fn = ops.value_and_grad(\n",
                "        forward_fn, None, optimizer.parameters, has_aux=False)\n",
                "\n",
                "    @jit\n",
                "    def train_step(data, label):\n",
                "        loss, grads = grad_fn(data, label)\n",
                "        loss = ops.depend(loss, optimizer(grads))\n",
                "        return loss\n",
                "\n",
                "    sink_process = data_sink(train_step, train_loader, sink_size=100)\n",
                "    summary_dir = os.path.join(config[\"summary_dir\"], model_name)\n",
                "    ckpt_dir = os.path.join(summary_dir, \"ckpt\")\n",
                "    if not os.path.exists(ckpt_dir):\n",
                "        os.makedirs(ckpt_dir)\n",
                "    model.set_train()\n",
                "    for step in range(1, 1 + optimizer_params[\"train_epochs\"]):\n",
                "        local_time_beg = time.time()\n",
                "        cur_loss = sink_process()\n",
                "        print(\n",
                "            f\"epoch: {step} train loss: {cur_loss} epoch time: {time.time() - local_time_beg:.2f}s\")\n",
                "        if step % 10 == 0:\n",
                "            print(f\"loss: {cur_loss.asnumpy():>7f}\")\n",
                "            print(\"step: {}, time elapsed: {}ms\".format(\n",
                "                step, (time.time() - local_time_beg) * 1000))\n",
                "            calculate_l2_error(model, test_a, test_u)\n",
                "            save_checkpoint(model, os.path.join(\n",
                "                ckpt_dir, model_params[\"name\"]))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "pid: 1993\n",
                        "2023-02-01 12:14:12.2323\n",
                        "use_ascend: False\n",
                        "device_id: 2\n",
                        "Data preparation finished\n",
                        "steps_per_epoch:  1000\n",
                        "epoch: 1 train loss: 1.7631323 epoch time: 50.41s\n",
                        "epoch: 2 train loss: 1.9283392 epoch time: 36.59s\n",
                        "epoch: 3 train loss: 1.4265916 epoch time: 35.09s\n",
                        "epoch: 4 train loss: 1.8609437 epoch time: 34.41s\n",
                        "epoch: 5 train loss: 1.5222052 epoch time: 34.60s\n",
                        "epoch: 6 train loss: 1.3424721 epoch time: 33.85s\n",
                        "epoch: 7 train loss: 1.607729 epoch time: 33.11s\n",
                        "epoch: 8 train loss: 1.3308442 epoch time: 33.05s\n",
                        "epoch: 9 train loss: 1.3169765 epoch time: 33.90s\n",
                        "epoch: 10 train loss: 1.4149593 epoch time: 33.91s\n",
                        "...\n",
                        "predict total time: 15.179609298706055 s\n",
                        "epoch: 141 train loss: 0.777328 epoch time: 32.55s\n",
                        "epoch: 142 train loss: 0.7008966 epoch time: 32.52s\n",
                        "epoch: 143 train loss: 0.72377646 epoch time: 32.57s\n",
                        "epoch: 144 train loss: 0.72175145 epoch time: 32.44s\n",
                        "epoch: 145 train loss: 0.6235678 epoch time: 32.46s\n",
                        "epoch: 146 train loss: 0.9351083 epoch time: 32.45s\n",
                        "epoch: 147 train loss: 0.9283789 epoch time: 32.47s\n",
                        "epoch: 148 train loss: 0.7655642 epoch time: 32.60s\n",
                        "epoch: 149 train loss: 0.7233772 epoch time: 32.65s\n",
                        "epoch: 150 train loss: 0.86825275 epoch time: 32.59s\n",
                        "================================Start Evaluation================================\n",
                        "mean rel_rmse_error: 0.07437102290522307\n",
                        "=================================End Evaluation=================================\n",
                        "predict total time: 15.212349653244019 s\n"
                    ]
                }
            ],
            "source": [
                "train()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "vscode": {
            "interpreter": {
                "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
