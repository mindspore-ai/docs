{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINNs for Point Source Poisson\n",
    "\n",
    "[![DownloadNotebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_notebook_en.svg)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/master/mindflow/en/physics_driven/mindspore_poisson_point_source.ipynb)&emsp;[![DownloadCode](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_download_code_en.svg)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/master/mindflow/en/physics_driven/mindspore_poisson_point_source.py)&emsp;[![View Source On Gitee](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg)](https://gitee.com/mindspore/docs/blob/master/docs/mindflow/docs/source_en/physics_driven/poisson_point_source.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "This example demonstrates how to use the PINNs method to solve the Poisson equation with a point source in two dimensions. The equation is defined by\n",
    "\n",
    "$$\n",
    "\\Delta u = - \\delta(x-x_{src})\\delta(y-y_{src}),\n",
    "$$\n",
    "\n",
    "where $(x_{src}, y_{src})$  is the coordinate corresponding to the point source position. he point source can be represented mathematically using the Dirac $\\delta$ function\n",
    "\n",
    "$$\n",
    "\\delta(x) = \\begin{cases}\n",
    "+\\infty, & x = 0    \\\\\n",
    "0,       & x \\neq 0\n",
    "\\end{cases}\n",
    "\\qquad\n",
    "\\int_{-\\infty}^{+\\infty}\\delta(x)dx = 1.\n",
    "$$\n",
    "\n",
    "When the solution domain is $\\Omega=[0,\\pi]^2$, the analytical solution of this equation is\n",
    "\n",
    "$$\n",
    "u(x,y) = \\frac{4}{\\pi^2} \\sum_{i=1}^{\\infty} \\sum_{j=1}^{\\infty}\\frac{\\sin{(i x)}\\sin{(i x_{src})}\\sin{(j y)}\\sin{(j y_{src})}}{i^2 + j^2}\n",
    "$$\n",
    "\n",
    "The corresponding paper for this case is:\n",
    "[Xiang Huang, Hongsheng Liu, Beiji Shi, Zidong Wang, Kang Yang, Yang Li, Min Wang, Haotian Chu, Jing Zhou, Fan Yu, Bei Hua, Bin Dong, Lei Chen. “A Universal PINNs Method for Solving Partial Differential Equations with a Point Source”. Thirty-First International Joint Conference on Artificial Intelligence (IJCAI 2022), Vienna, Austria, Jul, 2022, Pages 3839-3846.](https://www.ijcai.org/proceedings/2022/0533.pdf)\n",
    "\n",
    "## Method\n",
    "\n",
    "The specific process of MindSpore Flow for solving the problem is as follows:\n",
    "\n",
    "1. Creating the dataset.\n",
    "2. Creating the neural network.\n",
    "3. PINNs' loss.\n",
    "4. Creating the optimizer.\n",
    "5. Model training.\n",
    "6. Model inference and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from mindspore import context, nn, ops, jit\n",
    "from mindflow import load_yaml_config\n",
    "\n",
    "from src.dataset import create_train_dataset, create_test_dataset\n",
    "from src.poisson import Poisson\n",
    "from src.utils import calculate_l2_error, visual\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target=\"GPU\")\n",
    "\n",
    "# Load config\n",
    "file_cfg = \"poisson_cfg.yaml\"\n",
    "config = load_yaml_config(file_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset\n",
    "\n",
    "In this example, random sampling is performed in the solution domain, boundaries, and point source region (a rectangular area centered on the point source position) to generate the training dataset. See [src/dataset.py](https://gitee.com/mindspore/mindscience/tree/master/MindFlow/applications/physics_driven/poisson/point_source/src) for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "ds_train = create_train_dataset(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neural Network\n",
    "\n",
    "This example uses a multiscale neural network combined with the sin activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindflow.cell import MultiScaleFCSequential\n",
    "\n",
    "# Create the model\n",
    "model = MultiScaleFCSequential(config['model']['in_channels'],\n",
    "                               config['model']['out_channels'],\n",
    "                               config['model']['layers'],\n",
    "                               config['model']['neurons'],\n",
    "                               residual=True,\n",
    "                               act=config['model']['activation'],\n",
    "                               num_scales=config['model']['num_scales'],\n",
    "                               amp_factor=1.0,\n",
    "                               scale_factor=2.0,\n",
    "                               input_scale=[10., 10.],\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINNs' Loss\n",
    "\n",
    "When using ``mindflow`` to solve PDEs, we need to write a subclass of ``mindflow.PDEWithLloss`` to define the loss function terms corresponding to the governing equation and boundary conditions (``loss_pde`` and ``loss_bc``, respectively). Since the point source region requires dense sampling points, we add an additional loss function term (``loss_src``).\n",
    "\n",
    "When the PINNs method uses the residual of the governing equation as a loss function term to constrain the neural network, the singularity of the Dirac delta function makes it impossible for neural network training to converge. Therefore, we use the probability density function of two-dimensional Laplace distribution to approximate the Dirac $\\delta$ function:\n",
    "\n",
    "$$\n",
    "\\eta_{\\alpha}(x, y) = \\frac{1}{4\\alpha^2} exp({-\\frac{|x-x_{src}|+|y-y_{src}|}{\\alpha}}) \\qquad \\underrightarrow{approx} \\qquad \\delta(x-x_{src})\\delta(y-y_{src})\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the kernel width. In theory, as long as the kernel width $\\alpha$ is small enough, the above probability density function can approximate the Dirac $\\delta$ function very well. However, in practice, the selection of kernel width $\\alpha$ has an important impact on the approximation effect. When $\\alpha$ is too large, the approximation error between probability density function $\\eta_{\\alpha}(x, y)$ and Dirac $\\delta$ function will increase. But if $\\alpha$ is too small, the training process may not converge or the accuracy after convergence may be poor. Therefore, $\\alpha$ needs to be manually tuned. Here we determine it as $\\alpha=0.01$.\n",
    "\n",
    "L2 loss is used for solution domain, boundaries and point source region. The ``MTLWeightedLoss`` multi-objective loss function of ``mindflow`` is used to combine these three loss function terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from mindspore import numpy as ms_np\n",
    "from mindflow import PDEWithLoss, MTLWeightedLoss, sympy_to_mindspore\n",
    "\n",
    "class Poisson(PDEWithLoss):\n",
    "    \"\"\"Define the loss of the Poisson equation.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.x, self.y = sympy.symbols(\"x y\")\n",
    "        self.u = sympy.Function(\"u\")(self.x, self.y)\n",
    "        self.in_vars = [self.x, self.y]\n",
    "        self.out_vars = [self.u,]\n",
    "        self.alpha = 0.01  # kernel width\n",
    "        super(Poisson, self).__init__(model, self.in_vars, self.out_vars)\n",
    "        self.bc_nodes = sympy_to_mindspore(self.bc(), self.in_vars, self.out_vars)\n",
    "        self.loss_fn = MTLWeightedLoss(num_losses=3)\n",
    "\n",
    "    def pde(self):\n",
    "        \"\"\"Define the gonvering equation.\"\"\"\n",
    "        uu_xx = sympy.diff(self.u, (self.x, 2))\n",
    "        uu_yy = sympy.diff(self.u, (self.y, 2))\n",
    "\n",
    "        # Use Laplace probability density function to approximate the Dirac \\delta function.\n",
    "        x_src = sympy.pi / 2\n",
    "        y_src = sympy.pi / 2\n",
    "        force_term = 0.25 / self.alpha**2 * sympy.exp(-(\n",
    "            sympy.Abs(self.x - x_src) + sympy.Abs(self.y - y_src)) / self.alpha)\n",
    "\n",
    "        poisson = uu_xx + uu_yy + force_term\n",
    "        equations = {\"poisson\": poisson}\n",
    "        return equations\n",
    "\n",
    "    def bc(self):\n",
    "        \"\"\"Define the boundary condition.\"\"\"\n",
    "        bc_eq = self.u\n",
    "\n",
    "        equations = {\"bc\": bc_eq}\n",
    "        return equations\n",
    "\n",
    "    def get_loss(self, pde_data, bc_data, src_data):\n",
    "        \"\"\"Define the loss function.\"\"\"\n",
    "        res_pde = self.parse_node(self.pde_nodes, inputs=pde_data)\n",
    "        res_bc = self.parse_node(self.bc_nodes, inputs=bc_data)\n",
    "        res_src = self.parse_node(self.pde_nodes, inputs=src_data)\n",
    "\n",
    "        loss_pde = ms_np.mean(ms_np.square(res_pde[0]))\n",
    "        loss_bc = ms_np.mean(ms_np.square(res_bc[0]))\n",
    "        loss_src = ms_np.mean(ms_np.square(res_src[0]))\n",
    "\n",
    "        return self.loss_fn((loss_pde, loss_bc, loss_src))\n",
    "\n",
    "# Create the problem and optimizer\n",
    "problem = Poisson(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Optimizer\n",
    "\n",
    "This example uses the ``Adam`` optimizer and the learning rate decays to 1/10, 1/100, and 1/1000 of the initial learning rate when training reaches 40%, 60%, and 80%, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "\n",
    "params = model.trainable_params() + problem.loss_fn.trainable_params()\n",
    "steps_per_epoch = ds_train.get_dataset_size()\n",
    "milestone = [int(steps_per_epoch * n_epochs * x) for x in [0.4, 0.6, 0.8]]\n",
    "lr_init = config[\"optimizer\"][\"initial_lr\"]\n",
    "learning_rates = [lr_init * (0.1**x) for x in [0, 1, 2]]\n",
    "lr_ = nn.piecewise_constant_lr(milestone, learning_rates)\n",
    "optimizer = nn.Adam(params, learning_rate=lr_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "With MindSpore version >= 2.0.0, we can use the functional programming for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    grad_fn = ops.value_and_grad(problem.get_loss, None, optimizer.parameters, has_aux=False)\n",
    "\n",
    "    @jit\n",
    "    def train_step(pde_data, bc_data, src_data):\n",
    "        loss, grads = grad_fn(pde_data, bc_data, src_data)\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.unscale(loss)\n",
    "            is_finite = all_finite(grads)\n",
    "            if is_finite:\n",
    "                grads = loss_scaler.unscale(grads)\n",
    "                loss = ops.depend(loss, optimizer(grads))\n",
    "            loss_scaler.adjust(is_finite)\n",
    "        else:\n",
    "            loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "    def train_epoch(model, dataset, i_epoch):\n",
    "        local_time_beg = time.time()\n",
    "\n",
    "        model.set_train()\n",
    "        for _, (pde_data, bc_data, src_data) in enumerate(dataset):\n",
    "            loss = train_step(pde_data, bc_data, src_data)\n",
    "\n",
    "        print(\n",
    "            f\"epoch: {i_epoch} train loss: {float(loss):.8f}\" +\n",
    "            f\" epoch time: {time.time() - local_time_beg:.2f}s\")\n",
    "\n",
    "    for i_epoch in range(1, 1 + n_epochs):\n",
    "        train_epoch(model, ds_train, i_epoch)\n",
    "\n",
    "\n",
    "time_beg = time.time()\n",
    "train()\n",
    "print(f\"End-to-End total time: {time.time() - time_beg:.1f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference and Visualization\n",
    "\n",
    "Calculate the relative L2 error and draw a comparison graph between the reference solution and the model prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import calculate_l2_error, visual\n",
    "\n",
    "# Create the dataset\n",
    "ds_test = create_test_dataset(config)\n",
    "\n",
    "# Evaluate the model\n",
    "calculate_l2_error(model, ds_test)\n",
    "\n",
    "# Visual comparison of label and prediction\n",
    "visual(model, ds_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('ms')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c22ff8496cdfc43b41d028d0afe27e7d77fc6967d8e63387d8409afb66bbd90b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
