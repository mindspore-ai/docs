{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 数据驱动(FNO2D和UNET2D两种backbone)下跨声速翼型复杂流场的多时间步预测\n",
    "\n",
    "[![下载Notebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_notebook.svg)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.2/mindflow/zh_cn/data_driven/mindspore_2D_unsteady.ipynb)&emsp;[![下载样例代码](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_download_code.svg)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.2/mindflow/zh_cn/data_driven/mindspore_2D_unsteady.py)&emsp;[![查看源文件](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source.svg)](https://gitee.com/mindspore/docs/blob/r2.2/docs/mindflow/docs/source_zh_cn/data_driven/2D_unsteady.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **概述**\n",
    "\n",
    "高精度非定常流动模拟是计算流体力学中的关键课题，有着广泛的应用场景和广阔的市场价值。然而，传统方法存在着算不快、算不准、算不稳等问题，通过AI方法探索流场演化规律为此提供了新的视角。\n",
    "\n",
    "本案例在二维跨声速翼型场景下提供了端到端的非定常复杂流场预测解决方案。案例中搭建了傅里叶神经算子(Fourier Neural Operator，FNO)和Unet两种网络结构。可以在保证一定精度的前提下，根据输入的*k*个时间步的流场，稳定预测出接续的*m*个时间步的流场。\n",
    "\n",
    "本案例中，流场来流马赫数达到了*Ma*=0.73。通过本案例可以验证深度学习方法在存在激波等复杂流动结构场景中，对多物理参数变化下非定常流场预测的有效性。\n",
    "\n",
    "![img_1.png](images/img_1.png)\n",
    "\n",
    "## 问题描述\n",
    "\n",
    "本案例利用*k*个时刻的流场学习接续的*m*个时刻的流场，实现二维可压缩非定常流场的预测：\n",
    "\n",
    "$$\n",
    "u_{[t_0\\sim t_{k-1}]} \\mapsto u_{[t_k\\sim t_{k+m}]}.\n",
    "$$\n",
    "\n",
    "## 技术路径\n",
    "\n",
    "求解该问题的具体流程如下：\n",
    "\n",
    "1. 构建数据集。\n",
    "2. 构建模型。\n",
    "3. 优化器与损失函数。\n",
    "4. 模型训练。\n",
    "\n",
    "## 准备环节\n",
    "\n",
    "实践前，确保已经正确安装合适版本的MindSpore。如果没有，可以通过：\n",
    "\n",
    "* [MindSpore安装页面](https://www.mindspore.cn/install) 安装MindSpore。\n",
    "\n",
    "## 二维翼型非定常流场预测的实现\n",
    "\n",
    "二维翼型非定常流场预测的实现分为以下7个步骤：\n",
    "\n",
    "1. 配置网络与训练参数\n",
    "2. 数据集的准备\n",
    "3. 模型构建\n",
    "4. 损失函数与优化器\n",
    "5. 训练函数\n",
    "6. 模型训练\n",
    "7. 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from mindspore import nn, Tensor, context, ops, jit, set_seed, data_sink, save_checkpoint\n",
    "from mindspore import dtype as mstype\n",
    "from mindflow.common import get_warmup_cosine_annealing_lr\n",
    "from mindflow.loss import RelativeRMSELoss\n",
    "from mindflow.utils import load_yaml_config, print_log\n",
    "\n",
    "from src import Trainer, init_dataset, init_model, plt_log, check_file_path, count_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE,\n",
    "                    save_graphs=False,\n",
    "                    device_target=\"Ascend\",\n",
    "                    device_id=0)\n",
    "use_ascend = context.get_context(\"device_target\") == \"Ascend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 配置网络与训练参数\n",
    "\n",
    "从配置文件中读取四类参数，分别为模型相关参数（model）、数据相关参数（data）、优化器相关参数（optimizer)和回调相关参数(callback)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = load_yaml_config(\"./config/2D_unsteady.yaml\")\n",
    "data_params = config[\"data\"]\n",
    "model_params = config[\"model\"]\n",
    "optimizer_params = config[\"optimizer\"]\n",
    "summary_params = config[\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 数据集的准备\n",
    "\n",
    "数据集下载地址：[data_driven/airfoil/2D_unsteady](https://download.mindspore.cn/mindscience/mindflow/dataset/applications/data_driven/airfoil/2D_unsteady/)\n",
    "\n",
    "数据为npz类型文件，其维度(*t*, *H*, *W*, *C*)为(9997, 128, 128, 3)。其中，*t*为时间步数，*H*和*W*为流场分辨率，*C*为通道数，3个通道分别为速度*U*、*V*和压力*P*。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size (3560, 8, 128, 128, 3)\n",
      "label size (3560, 32, 128, 128, 3)\n",
      "train_batch_size : 8\n",
      "train dataset size: 2967\n",
      "test dataset size: 593\n",
      "train batch dataset size: 370\n",
      "test batch dataset size: 74\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, means, stds = init_dataset(data_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建\n",
    "\n",
    "通过initial_model()函数调用，调用之前，需要先针对硬件定制loss_scaler和compute_dtype。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_dtype_of_FNO Float16\n"
     ]
    }
   ],
   "source": [
    "if use_ascend:\n",
    "    from mindspore.amp import DynamicLossScaler, all_finite, auto_mixed_precision\n",
    "    loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "    compute_dtype = mstype.float16\n",
    "    model = init_model(\"fno2d\", data_params, model_params, compute_dtype=compute_dtype)\n",
    "    auto_mixed_precision(model, optimizer_params[\"amp_level\"][\"fno2d\"])\n",
    "else:\n",
    "    context.set_context(enable_graph_kernel=True)\n",
    "    loss_scaler = None\n",
    "    compute_dtype = mstype.float32\n",
    "    model = init_model(\"fno2d\", data_params, model_params, compute_dtype=compute_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数与优化器\n",
    "\n",
    "当前案例中的损失函数采用了RelativeRMSELoss，优化器则选择了AdamWeightDecay，其中，学习率衰减采用了warmup_cosine_annealing_lr的策略。用户也可以根据需要定制适合的损失函数与优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameter count: 9464259\n",
      "learing rate: 0.001, T_in: 8, T_out: 32\n"
     ]
    }
   ],
   "source": [
    "loss_fn = RelativeRMSELoss()\n",
    "summary_dir = os.path.join(summary_params[\"summary_dir\"], \"Exp01\", \"fno2d\")\n",
    "ckpt_dir = os.path.join(summary_dir, \"ckpt_dir\")\n",
    "check_file_path(ckpt_dir)\n",
    "check_file_path(os.path.join(ckpt_dir, 'img'))\n",
    "print_log('model parameter count:', count_params(model.trainable_params()))\n",
    "print_log(\n",
    "    f'learing rate: {optimizer_params[\"lr\"][\"fno2d\"]}, T_in: {data_params[\"T_in\"]}, T_out: {data_params[\"T_out\"]}')\n",
    "steps_per_epoch = train_dataset.get_dataset_size()\n",
    "\n",
    "lr = get_warmup_cosine_annealing_lr(optimizer_params[\"lr\"][\"fno2d\"], steps_per_epoch,\n",
    "                                    optimizer_params[\"epochs\"], optimizer_params[\"warm_up_epochs\"])\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(),\n",
    "                               learning_rate=Tensor(lr),\n",
    "                               weight_decay=optimizer_params[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练函数\n",
    "\n",
    "使用**MindSpore>= 2.0.0**的版本，可以使用函数式编程范式训练神经网络，单步训练函数使用jit装饰。数据下沉函数data_sink，传入单步训练函数和训练数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, data_params, loss_fn, means, stds)\n",
    "\n",
    "def forward_fn(inputs, labels):\n",
    "    loss, _, _, _ = trainer.get_loss(inputs, labels)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.scale(loss)\n",
    "    return loss\n",
    "\n",
    "grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=False)\n",
    "\n",
    "@jit\n",
    "def train_step(inputs, labels):\n",
    "    loss, grads = grad_fn(inputs, labels)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.unscale(loss)\n",
    "        if all_finite(grads):\n",
    "            grads = loss_scaler.unscale(grads)\n",
    "    loss_new = ops.depend(loss, optimizer(grads))\n",
    "    return loss_new\n",
    "\n",
    "def test_step(inputs, labels):\n",
    "    return trainer.get_loss(inputs, labels)\n",
    "\n",
    "train_size = train_dataset.get_dataset_size()\n",
    "test_size = test_dataset.get_dataset_size()\n",
    "train_sink = data_sink(train_step, train_dataset, sink_size=1)\n",
    "test_sink = data_sink(test_step, test_dataset, sink_size=1)\n",
    "test_interval = summary_params[\"test_interval\"]\n",
    "save_ckpt_interval = summary_params[\"save_ckpt_interval\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "模型训练过程中边训练边推理。用户可以直接加载测试数据集，每训练test_interval个epoch后输出一次测试集上的推理精度并保存可视化结果。同时，还可以每隔save_checkpoint_interval保存一次checkpoint文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step time: 2.652332, loss: 0.733017\n",
      "epoch: 2, step time: 0.688175, loss: 0.203251\n",
      "epoch: 3, step time: 0.686817, loss: 0.128816\n",
      "epoch: 4, step time: 0.685909, loss: 0.109786\n",
      "epoch: 5, step time: 0.688545, loss: 0.093725\n",
      "epoch: 6, step time: 0.685986, loss: 0.076027\n",
      "epoch: 7, step time: 0.686459, loss: 0.069847\n",
      "epoch: 8, step time: 0.688228, loss: 0.058694\n",
      "epoch: 9, step time: 0.688053, loss: 0.060886\n",
      "epoch: 10, step time: 0.692221, loss: 0.065305\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 10, loss: 0.03798117920381923\n",
      "---------------------------end test---------------------------\n",
      "epoch: 11, step time: 0.691715, loss: 0.059103\n",
      "epoch: 12, step time: 0.689456, loss: 0.059963\n",
      "epoch: 13, step time: 0.688435, loss: 0.056177\n",
      "epoch: 14, step time: 0.688293, loss: 0.044799\n",
      "epoch: 15, step time: 0.688754, loss: 0.048757\n",
      "epoch: 16, step time: 0.687413, loss: 0.051108\n",
      "epoch: 17, step time: 0.689244, loss: 0.041168\n",
      "epoch: 18, step time: 0.687268, loss: 0.044156\n",
      "epoch: 19, step time: 0.686074, loss: 0.044050\n",
      "epoch: 20, step time: 0.687592, loss: 0.041634\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 20, loss: 0.03290090155693375\n",
      "---------------------------end test---------------------------\n",
      "epoch: 21, step time: 0.686854, loss: 0.038138\n",
      "epoch: 22, step time: 0.688540, loss: 0.040290\n",
      "epoch: 23, step time: 0.687311, loss: 0.037129\n",
      "epoch: 24, step time: 0.685613, loss: 0.042077\n",
      "epoch: 25, step time: 0.688631, loss: 0.035659\n",
      "epoch: 26, step time: 0.687999, loss: 0.031553\n",
      "epoch: 27, step time: 0.689534, loss: 0.036624\n",
      "epoch: 28, step time: 0.687576, loss: 0.040991\n",
      "epoch: 29, step time: 0.685203, loss: 0.034616\n",
      "epoch: 30, step time: 0.687388, loss: 0.029517\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 30, loss: 0.024450113343207062\n",
      "---------------------------end test---------------------------\n",
      "epoch: 31, step time: 0.687894, loss: 0.030733\n",
      "epoch: 32, step time: 0.686834, loss: 0.033166\n",
      "epoch: 33, step time: 0.684156, loss: 0.034593\n",
      "epoch: 34, step time: 0.687594, loss: 0.030224\n",
      "epoch: 35, step time: 0.685184, loss: 0.028527\n",
      "epoch: 36, step time: 0.687365, loss: 0.030338\n",
      "epoch: 37, step time: 0.686692, loss: 0.032679\n",
      "epoch: 38, step time: 0.684623, loss: 0.030559\n",
      "epoch: 39, step time: 0.684311, loss: 0.028198\n",
      "epoch: 40, step time: 0.686643, loss: 0.030848\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 40, loss: 0.024750267230194516\n",
      "---------------------------end test---------------------------\n",
      "epoch: 41, step time: 0.696444, loss: 0.027831\n",
      "epoch: 42, step time: 0.696912, loss: 0.029151\n",
      "epoch: 43, step time: 0.697324, loss: 0.032190\n",
      "epoch: 44, step time: 0.695226, loss: 0.026790\n",
      "epoch: 45, step time: 0.696124, loss: 0.026855\n",
      "epoch: 46, step time: 0.695851, loss: 0.026540\n",
      "epoch: 47, step time: 0.695193, loss: 0.027463\n",
      "epoch: 48, step time: 0.694509, loss: 0.032763\n",
      "epoch: 49, step time: 0.696785, loss: 0.027059\n",
      "epoch: 50, step time: 0.696424, loss: 0.024739\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 50, loss: 0.02536101617246262\n",
      "---------------------------end test---------------------------\n",
      "epoch: 51, step time: 0.691392, loss: 0.025051\n",
      "epoch: 52, step time: 0.691471, loss: 0.021609\n",
      "epoch: 53, step time: 0.686759, loss: 0.022961\n",
      "epoch: 54, step time: 0.684971, loss: 0.025058\n",
      "epoch: 55, step time: 0.687838, loss: 0.024519\n",
      "epoch: 56, step time: 0.688630, loss: 0.025127\n",
      "epoch: 57, step time: 0.684747, loss: 0.023062\n",
      "epoch: 58, step time: 0.685800, loss: 0.024089\n",
      "epoch: 59, step time: 0.687165, loss: 0.023332\n",
      "epoch: 60, step time: 0.687198, loss: 0.022027\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 60, loss: 0.012668337510911294\n",
      "---------------------------end test---------------------------\n",
      "epoch: 61, step time: 0.697993, loss: 0.023954\n",
      "epoch: 62, step time: 0.694950, loss: 0.022935\n",
      "epoch: 63, step time: 0.695095, loss: 0.019998\n",
      "epoch: 64, step time: 0.693050, loss: 0.021672\n",
      "epoch: 65, step time: 0.694423, loss: 0.020992\n",
      "epoch: 66, step time: 0.694051, loss: 0.024120\n",
      "epoch: 67, step time: 0.693227, loss: 0.021222\n",
      "epoch: 68, step time: 0.695719, loss: 0.018898\n",
      "epoch: 69, step time: 0.695193, loss: 0.020580\n",
      "epoch: 70, step time: 0.695925, loss: 0.021619\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 70, loss: 0.018595953204518033\n",
      "---------------------------end test---------------------------\n",
      "epoch: 71, step time: 0.699969, loss: 0.018871\n",
      "epoch: 72, step time: 0.693832, loss: 0.018930\n",
      "epoch: 73, step time: 0.696967, loss: 0.020276\n",
      "epoch: 74, step time: 0.698306, loss: 0.021508\n",
      "epoch: 75, step time: 0.696895, loss: 0.017177\n",
      "epoch: 76, step time: 0.698356, loss: 0.021475\n",
      "epoch: 77, step time: 0.694738, loss: 0.018566\n",
      "epoch: 78, step time: 0.691645, loss: 0.018249\n",
      "epoch: 79, step time: 0.693678, loss: 0.019016\n",
      "epoch: 80, step time: 0.693986, loss: 0.021684\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 80, loss: 0.03425668393233414\n",
      "---------------------------end test---------------------------\n",
      "epoch: 81, step time: 0.694019, loss: 0.018643\n",
      "epoch: 82, step time: 0.693350, loss: 0.019609\n",
      "epoch: 83, step time: 0.696632, loss: 0.017143\n",
      "epoch: 84, step time: 0.694124, loss: 0.016935\n",
      "epoch: 85, step time: 0.693287, loss: 0.017587\n",
      "epoch: 86, step time: 0.696237, loss: 0.016808\n",
      "epoch: 87, step time: 0.692938, loss: 0.016272\n",
      "epoch: 88, step time: 0.693298, loss: 0.016350\n",
      "epoch: 89, step time: 0.693079, loss: 0.016122\n",
      "epoch: 90, step time: 0.695640, loss: 0.016793\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 90, loss: 0.015049820608141631\n",
      "---------------------------end test---------------------------\n",
      "epoch: 91, step time: 0.695330, loss: 0.016076\n",
      "epoch: 92, step time: 0.694938, loss: 0.016224\n",
      "epoch: 93, step time: 0.696670, loss: 0.019537\n",
      "epoch: 94, step time: 0.694749, loss: 0.014278\n",
      "epoch: 95, step time: 0.696291, loss: 0.015967\n",
      "epoch: 96, step time: 0.695401, loss: 0.014452\n",
      "epoch: 97, step time: 0.692933, loss: 0.013790\n",
      "epoch: 98, step time: 0.693399, loss: 0.014776\n",
      "epoch: 99, step time: 0.691681, loss: 0.012814\n",
      "epoch: 100, step time: 0.694035, loss: 0.014525\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 100, loss: 0.014199618234596456\n",
      "---------------------------end test---------------------------\n",
      "epoch: 101, step time: 0.693424, loss: 0.015365\n",
      "epoch: 102, step time: 0.692964, loss: 0.013885\n",
      "epoch: 103, step time: 0.693850, loss: 0.015042\n",
      "epoch: 104, step time: 0.691925, loss: 0.013348\n",
      "epoch: 105, step time: 0.693731, loss: 0.012397\n",
      "epoch: 106, step time: 0.694072, loss: 0.015533\n",
      "epoch: 107, step time: 0.693095, loss: 0.013169\n",
      "epoch: 108, step time: 0.694365, loss: 0.013428\n",
      "epoch: 109, step time: 0.694493, loss: 0.012834\n",
      "epoch: 110, step time: 0.695011, loss: 0.011853\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 110, loss: 0.010528819402286506\n",
      "---------------------------end test---------------------------\n",
      "epoch: 111, step time: 0.699863, loss: 0.015232\n",
      "epoch: 112, step time: 0.695245, loss: 0.012432\n",
      "epoch: 113, step time: 0.696781, loss: 0.012335\n",
      "epoch: 114, step time: 0.696696, loss: 0.012129\n",
      "epoch: 115, step time: 0.696233, loss: 0.011410\n",
      "epoch: 116, step time: 0.691236, loss: 0.013234\n",
      "epoch: 117, step time: 0.694421, loss: 0.012024\n",
      "epoch: 118, step time: 0.694588, loss: 0.011595\n",
      "epoch: 119, step time: 0.695370, loss: 0.011778\n",
      "epoch: 120, step time: 0.695880, loss: 0.011173\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 120, loss: 0.009641800081275555\n",
      "---------------------------end test---------------------------\n",
      "epoch: 121, step time: 0.695209, loss: 0.011097\n",
      "epoch: 122, step time: 0.691141, loss: 0.010855\n",
      "epoch: 123, step time: 0.695012, loss: 0.011044\n",
      "epoch: 124, step time: 0.695229, loss: 0.011432\n",
      "epoch: 125, step time: 0.694662, loss: 0.010873\n",
      "epoch: 126, step time: 0.692417, loss: 0.010477\n",
      "epoch: 127, step time: 0.691926, loss: 0.010197\n",
      "epoch: 128, step time: 0.693930, loss: 0.011748\n",
      "epoch: 129, step time: 0.696376, loss: 0.009750\n",
      "epoch: 130, step time: 0.694301, loss: 0.009874\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 130, loss: 0.007853382740585084\n",
      "---------------------------end test---------------------------\n",
      "epoch: 131, step time: 0.696573, loss: 0.010120\n",
      "epoch: 132, step time: 0.693105, loss: 0.010578\n",
      "epoch: 133, step time: 0.696203, loss: 0.010406\n",
      "epoch: 134, step time: 0.692455, loss: 0.010169\n",
      "epoch: 135, step time: 0.695954, loss: 0.009985\n",
      "epoch: 136, step time: 0.691915, loss: 0.009575\n",
      "epoch: 137, step time: 0.691145, loss: 0.009403\n",
      "epoch: 138, step time: 0.693045, loss: 0.009086\n",
      "epoch: 139, step time: 0.695551, loss: 0.009042\n",
      "epoch: 140, step time: 0.694040, loss: 0.009282\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 140, loss: 0.009246890225472884\n",
      "---------------------------end test---------------------------\n",
      "epoch: 141, step time: 0.688479, loss: 0.008823\n",
      "epoch: 142, step time: 0.689157, loss: 0.009289\n",
      "epoch: 143, step time: 0.685248, loss: 0.009576\n",
      "epoch: 144, step time: 0.687585, loss: 0.009185\n",
      "epoch: 145, step time: 0.685840, loss: 0.009380\n",
      "epoch: 146, step time: 0.689897, loss: 0.008490\n",
      "epoch: 147, step time: 0.690710, loss: 0.008606\n",
      "epoch: 148, step time: 0.688072, loss: 0.008654\n",
      "epoch: 149, step time: 0.689746, loss: 0.008934\n",
      "epoch: 150, step time: 0.688304, loss: 0.008629\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 150, loss: 0.007401696321156468\n",
      "---------------------------end test---------------------------\n",
      "epoch: 151, step time: 0.697744, loss: 0.009447\n",
      "epoch: 152, step time: 0.699723, loss: 0.008901\n",
      "epoch: 153, step time: 0.699869, loss: 0.008984\n",
      "epoch: 154, step time: 0.699634, loss: 0.008882\n",
      "epoch: 155, step time: 0.697683, loss: 0.009762\n",
      "epoch: 156, step time: 0.696223, loss: 0.009919\n",
      "epoch: 157, step time: 0.690939, loss: 0.009107\n",
      "epoch: 158, step time: 0.691592, loss: 0.008638\n",
      "epoch: 159, step time: 0.693206, loss: 0.008478\n",
      "epoch: 160, step time: 0.692778, loss: 0.008224\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 160, loss: 0.007048012673810779\n",
      "---------------------------end test---------------------------\n",
      "epoch: 161, step time: 0.701309, loss: 0.009469\n",
      "epoch: 162, step time: 0.697829, loss: 0.008522\n",
      "epoch: 163, step time: 0.698344, loss: 0.008658\n",
      "epoch: 164, step time: 0.699316, loss: 0.008556\n",
      "epoch: 165, step time: 0.696708, loss: 0.008601\n",
      "epoch: 166, step time: 0.700000, loss: 0.009487\n",
      "epoch: 167, step time: 0.696938, loss: 0.009309\n",
      "epoch: 168, step time: 0.696608, loss: 0.009240\n",
      "epoch: 169, step time: 0.698408, loss: 0.008620\n",
      "epoch: 170, step time: 0.699614, loss: 0.008341\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 170, loss: 0.00696752735077737\n",
      "---------------------------end test---------------------------\n",
      "epoch: 171, step time: 0.700533, loss: 0.008454\n",
      "epoch: 172, step time: 0.701728, loss: 0.008413\n",
      "epoch: 173, step time: 0.700658, loss: 0.008100\n",
      "epoch: 174, step time: 0.697647, loss: 0.008213\n",
      "epoch: 175, step time: 0.700830, loss: 0.007987\n",
      "epoch: 176, step time: 0.697824, loss: 0.007898\n",
      "epoch: 177, step time: 0.698811, loss: 0.007717\n",
      "epoch: 178, step time: 0.698952, loss: 0.007696\n",
      "epoch: 179, step time: 0.695718, loss: 0.007801\n",
      "epoch: 180, step time: 0.694938, loss: 0.007583\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 180, loss: 0.006799178198569869\n",
      "---------------------------end test---------------------------\n",
      "epoch: 181, step time: 0.694381, loss: 0.007583\n",
      "epoch: 182, step time: 0.693612, loss: 0.007847\n",
      "epoch: 183, step time: 0.693077, loss: 0.007647\n",
      "epoch: 184, step time: 0.693908, loss: 0.007516\n",
      "epoch: 185, step time: 0.694870, loss: 0.007329\n",
      "epoch: 186, step time: 0.696124, loss: 0.007347\n",
      "epoch: 187, step time: 0.695269, loss: 0.007284\n",
      "epoch: 188, step time: 0.695620, loss: 0.007207\n",
      "epoch: 189, step time: 0.692392, loss: 0.007136\n",
      "epoch: 190, step time: 0.695566, loss: 0.007082\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 190, loss: 0.006618331926457924\n",
      "---------------------------end test---------------------------\n",
      "epoch: 191, step time: 0.693253, loss: 0.007012\n",
      "epoch: 192, step time: 0.691330, loss: 0.007043\n",
      "epoch: 193, step time: 0.692804, loss: 0.006986\n",
      "epoch: 194, step time: 0.690053, loss: 0.006973\n",
      "epoch: 195, step time: 0.692159, loss: 0.006967\n",
      "epoch: 196, step time: 0.690170, loss: 0.006944\n",
      "epoch: 197, step time: 0.690344, loss: 0.006930\n",
      "epoch: 198, step time: 0.690674, loss: 0.006911\n",
      "epoch: 199, step time: 0.690877, loss: 0.006904\n",
      "epoch: 200, step time: 0.689170, loss: 0.006892\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 200, loss: 0.005457837492891436\n",
      "---------------------------end test---------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, optimizer_params[\"epochs\"] + 1):\n",
    "    time_beg = time.time()\n",
    "    train_l2_step = 0.0\n",
    "    model.set_train()\n",
    "    for step in range(1, train_size + 1):\n",
    "        loss = train_sink()\n",
    "        train_l2_step += loss.asnumpy()\n",
    "    train_l2_step = train_l2_step / train_size / data_params[\"T_out\"]\n",
    "    print_log(\n",
    "        f\"epoch: {epoch}, step time: {(time.time() - time_beg) / steps_per_epoch:>7f}, loss: {train_l2_step:>7f}\")\n",
    "\n",
    "    if epoch % test_interval == 0:\n",
    "        model.set_train(False)\n",
    "        test_l2_by_step = [0.0 for _ in range(data_params[\"T_out\"])]\n",
    "        print_log(\"---------------------------start test-------------------------\")\n",
    "        for step in range(test_size):\n",
    "            _, pred, truth, step_losses = test_sink()\n",
    "            for i in range(data_params[\"T_out\"]):\n",
    "                test_l2_by_step[i] += step_losses[i].asnumpy()\n",
    "        test_l2_by_step = [error / test_size for error in test_l2_by_step]\n",
    "        test_l2_step = np.mean(test_l2_by_step)\n",
    "        print_log(f' test epoch: {epoch}, loss: {test_l2_step}')\n",
    "        print_log(\"---------------------------end test---------------------------\")\n",
    "\n",
    "        plt_log(predicts=pred.asnumpy(),\n",
    "                labels=truth.asnumpy(),\n",
    "                img_dir=os.path.join(ckpt_dir, 'img'),\n",
    "                epoch=epoch\n",
    "                )\n",
    "\n",
    "    if epoch % save_ckpt_interval == 0:\n",
    "        save_checkpoint(model, ckpt_file_name=os.path.join(ckpt_dir, 'airfoil2D_unsteady.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **结果可视化**\n",
    "\n",
    "UNET2D backbone下，不同时刻压力P的实际值、预测值和误差在流场中的分布如下图：\n",
    "\n",
    "![Unet_P.png](images/Unet_P.png)\n",
    "\n",
    "UNET2D backbone下，不同时刻速度U的实际值、预测值和误差在流场中的分布如下图：\n",
    "\n",
    "![Unet_U.png](images/Unet_U.png)\n",
    "\n",
    "UNET2D backbone下，不同时刻速度V的实际值、预测值和误差在流场中的分布如下图：\n",
    "\n",
    "![Unet_V.png](images/Unet_V.png)\n",
    "\n",
    "FNO2D backbone下，不同时刻压力P的实际值、预测值和误差在流场中的分布如下图：\n",
    "\n",
    "![FNO_P.png](images/FNO_P.png)\n",
    "\n",
    "FNO2D backbone下，不同时刻速度U的实际值、预测值和误差在流场中的分布如下图：\n",
    "\n",
    "![FNO_U.png](images/FNO_U.png)\n",
    "\n",
    "FNO2D backbone下，不同时刻速度V的实际值、预测值和误差在流场中的分布如下图：\n",
    "\n",
    "![FNO_V.png](images/FNO_V.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zys_test]",
   "language": "python",
   "name": "conda-env-zys_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9063439a3781aed32d6b0dd4804a0c8b51ecec7893a0f31b99846bc91ef39eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
