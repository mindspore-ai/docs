# Safetensors Weights

[![View Source On Gitee](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg)](https://gitee.com/mindspore/docs/blob/master/docs/mindformers/docs/source_en/function/safetensors.md)

## Overview

Safetensors is a reliable and portable machine learning model storage format from Huggingface for storing Tensors securely and with fast storage (zero copies). This article focuses on how MindSpore Transformers supports saving and loading of this file format to help users use weights better and faster.

## Safetensors Weights Samples

There are two main types of Safetensors files: complete weights files and distributed weights files. Below are examples of how they are obtained and the corresponding files.

### Complete Weights

Safetensors complete weights can be obtained in two ways:

1. Download directly from Huggingface.
2. After MindSpore Transformers distributed training, the weights are generated by [merge script](https://www.mindspore.cn/mindformers/docs/en/dev/function/transform_weight.html#safetensors-weight-merging).

Huggingface Safetensors example catalog structure is as follows:

```text
qwen2_7b
 └── hf_unified_safetenosrs
        ├── model-00001-of-00004.safetensors
        ├── model-00002-of-00004.safetensors
        ├── model-00003-of-00004.safetensors
        ├── model-00004-of-00004.safetensors
        └── model.safetensors.index.json        # Huggingface weight parameter and file storage relationship mapping json file
```

MindSpore Safetensors example catalog structure is as follows:

```text
qwen2_7b
 └── ms_unified_safetenosrs
        ├── model-00001-of-00004.safetensors
        ├── model-00002-of-00004.safetensors
        ├── model-00003-of-00004.safetensors
        ├── model-00004-of-00004.safetensors
        ├── hyper_param.safetensors            # Hyperparameter files for training task records
        └── param_name_map.json                # MindSpore weight parameter and file storage relationship mapping json file
```

### Distributed Weights

Safetensors distributed weights can be obtained in two ways:

1. Generated by distributed training with MindSpore Transformers.
2. Using [format conversion script](https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.ckpt_to_safetensors.html), the original distributed ckpt weights are changed to the Safetensors format.

Distributed Safetensors example catalog structure is as follows:

```text
qwen2_7b
 └── distributed_safetenosrs
        ├── rank_0
            └── qwen2_7b_rank_0.safetensors
        ├── rank_1
            └── qwen2_7b_rank_1.safetensors
        ...
        └── rank_x
            └── qwen2_7b_rank_x.safetensors
```

## Configuration Descriptions

Load the relevant configurations:

| Parameter names              | Descriptions                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ------------------- |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| load_checkpoint     | The path to the folder where the weights are preloaded.<br> - In case of full weights, fill in the path to the folder where the slices/individual weight files are located.<br/>Note: Huggingface safetensor weights loading is supported (currently only Llama series models are supported). During the online loading process, a copy of the converted MindSpore safetensor weights file is saved to `/output/ms_safetensors`.<br> - In case of distributed weights, they need to be stored in `model_dir/rank_x/xxx.safetensor` format, with the folder path filled in as `model_dir`.                                                                                                                                                                                                             |
| load_ckpt_format | The format of the loaded model weights, optionally `ckpt`, `safetensors`, defaults to `ckpt`.<br/>Loading weights in `safetensors` format needs to change this configuration to `safetensors`.                                                                                                                                                                                                                                                                                                                                                                  |
| auto_trans_ckpt | Whether to enable the online slicing function.<br/>- If loading weight is full weight:<br/>a. when `use_parallel: True`, it is judged as distributed loading, `auto_trans_ckpt: True` needs to be set synchronously to turn on online slicing. <br/>b. When `use_parallel: False`, it is judged as single card loading, you need to set `auto_trans_ckpt: False` synchronously to disable the online slicing function.<br/>- If loading weight is distributed weight:<br/>a. Without changing the original slicing strategy, you need to set `auto_trans_ckpt: False` to load directly according to the original slicing strategy.<br/>b. To change the original slicing strategy, set `auto_trans_ckpt: True` and configure `src_strategy_path_or_dir` to be the original slicing strategy file path.<br/>When the task is pulled up, the weights are merged online into full weights, which are sliced and loaded according to the parallelism strategy set in the configuration file. The online merged weights are saved in the current directory under the `/output/unified_checkpoint` file. |
| remove_redundancy | Whether the loaded weights are de-redundant, defaults to `False`.                                                                                                                                                                                                                                                                                                                                                                                                                                 |

Save the relevant configurations:

| Parameter names                    | Descriptions                                                         |
| :-------------------------- | ------------------------------------------------------------ |
| callbacks.checkpoint_format | The format of the saved model weights, defaults to `ckpt`. Options are `ckpt` and `safetensors`. |
| callbacks.remove_redundancy | Whether to enable de-redundancy saving when saving weights, defaults to `False`. Only `safetensors format` is supported. |

## Usage Example

### Examples of Pre-training Tasks

Taking Llama2-7B as an example, modify the configuration item [pretrain_llama2_7b.yaml](https://gitee.com/mindspore/mindformers/blob/dev/configs/llama2/pretrain_llama2_7b.yaml) to confirm the weight saving format:

```yaml
callbacks:
  - type: CheckpointMonitor
    checkpoint_format: safetensors                  # Save weights file format
    remove_redundancy: True                         # Turn on de-redundancy when saving weights
```

Execute the command when completed:

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config configs/llama2/pretrain_llama2_7b.yaml \
 --train_dataset_dir /{path}/wiki4096.mindrecord \
 --use_parallel True \
 --run_mode train" 8
```

After the task is executed, a checkpoint folder is generated in the mindformers/output directory, while the model files are saved in that folder.

For more details, please refer to: [Introduction to Pre-training](https://www.mindspore.cn/mindformers/docs/en/dev/usage/pre_training.html).

### Examples of Fine-tuning Tasks

If you use the full weighted multicard online fine-tuning, take the Qwen2-7B model as an example and modify the configuration item [finetune_qwen2_7b.yaml](https://gitee.com/mindspore/mindformers/blob/dev/research/qwen2/qwen2_7b/finetune_qwen2_7b.yaml):

```yaml
# Modified configuration
load_checkpoint: '/qwen2_7b/hf_unified_safetenosrs' # Load weights file path
load_ckpt_format: 'safetensors'                     # Load weights file format
auto_trans_ckpt: True                               # This configuration item needs to be turned on for complete weights to enable the online slicing feature
parallel_config:                                    # Configure the target distributed strategy
  data_parallel: 1
  model_parallel: 2
  pipeline_stage: 1
callbacks:
  - type: CheckpointMonitor
    checkpoint_format: safetensors                  # Save weights file format
```

If you use distributed weights multicard online fine-tuning, take the Qwen2-7B model as an example, modify the configuration item [finetune_qwen2_7b.yaml](https://gitee.com/mindspore/mindformers/blob/dev/research/qwen2/qwen2_7b/finetune_qwen2_7b.yaml):

```yaml
# Modified configuration
load_checkpoint: '/qwen2_7b/distributed_safetenosrs' # Load weights file path
load_ckpt_format: 'safetensors'                      # Load weights file format
parallel_config:                                     # Configure the target distributed strategy
  data_parallel: 1
  model_parallel: 2
  pipeline_stage: 1
callbacks:
  - type: CheckpointMonitor
    checkpoint_format: safetensors                  # Save weights file format
```

Execute the command when completed:

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config research/qwen2/qwen2_7b/finetune_qwen2_7b.yaml \
 --train_dataset_dir /{path}/alpaca-data.mindrecord \
 --register_path research/qwen2 \
 --use_parallel True \
 --run_mode finetune" 2
```

After the task is executed, a checkpoint folder is generated in the mindformers/output directory, while the model files are saved in that folder.

For more details, please refer to [Introduction to SFT fine-tuning](https://www.mindspore.cn/mindformers/docs/en/dev/usage/sft_tuning.html)

### Example of an Inference Task

If you use complete weighted multicard online inference, take the Qwen2-7B model as an example, and modify the configuration item [predict_qwen2_7b_instruct.yaml](https://gitee.com/mindspore/mindformers/blob/dev/research/qwen2/qwen2_7b/predict_qwen2_7b_instruct.yaml):

```yaml
# Modified configuration
load_checkpoint: '/qwen2_7b/hf_unified_safetenosrs' # Load weights file path
load_ckpt_format: 'safetensors'                     # Load weights file format
auto_trans_ckpt: True                               # This configuration item needs to be turned on for complete weights to enable the online slicing function
parallel_config:
  data_parallel: 1
  model_parallel: 2
  pipeline_stage: 1
```

If you use distributed weighted multicard online inference, take the Qwen2-7B model as an example, modify the configuration item [predict_qwen2_7b_instruct.yaml](https://gitee.com/mindspore/mindformers/blob/dev/research/qwen2/qwen2_7b/predict_qwen2_7b_instruct.yaml):

```yaml
# Modified configuration
load_checkpoint: '/qwen2_7b/distributed_safetenosrs' # Load weights file path
load_ckpt_format: 'safetensors'                      # Load weights file format
parallel_config:
  data_parallel: 1
  model_parallel: 2
  pipeline_stage: 1
```

Execute the command when completed:

```shell
bash scripts/msrun_launcher.sh "python run_mindformer.py \
--config research/qwen2/qwen2_7b/predict_qwen2_7b_instruct.yaml \
--run_mode predict \
--use_parallel True \
--register_path research/qwen2 \
--predict_data 'I love Beijing, because'" \
2
```

The results of executing the above single-card inference and multi-card inference commands are as follows:

```text
'text_generation_text': [I love Beijing, because it is a city with a long history and culture.......]
```

For more details, please refer to: [Introduction to Inference](https://www.mindspore.cn/mindformers/docs/en/dev/usage/inference.html)

### Examples of Resumable Training after Breakpoint Tasks

MindSpore Transformers supports step-level resumable training after breakpoint, which allows you to save a model's checkpoints during training and load the saved checkpoints to restore the previous state to continue training after a break in training.

If you use distributed weight multicard resumable training and do not change the slicing strategy, modify the configuration item and start the original training task:

```yaml
# Modified configuration
load_checkpoint: '/output/checkpoint'                # Load source distributed weights file path
load_ckpt_format: 'safetensors'                      # Load weights file format
resume_training: True                                # Resumable training after breakpoint switch
callbacks:
  - type: CheckpointMonitor
    checkpoint_format: safetensors                   # Save weights file format
```

If the distributed weight multi-card training is renewed and the slicing strategy is changed, it is necessary to pass in the path of the source slicing strategy file and start the original training task after modifying the configuration items:

```yaml
# Modified configuration
load_checkpoint: '/output/checkpoint'               # Load source distributed weights file path
src_strategy_path_or_dir: '/output/src_strategy'    # Load source strategy file for merging source distributed weights into full weights
load_ckpt_format: 'safetensors'                     # Load weights file format
auto_trans_ckpt: True                               # Enable online slicing
resume_training: True                               # Resumable training after breakpoint switch
parallel_config:                                    # Configure the target distributed strategy
  data_parallel: 2
  model_parallel: 4
  pipeline_stage: 1
callbacks:
  - type: CheckpointMonitor
    checkpoint_format: safetensors                  # Save weights file format
```

In large cluster scale scenarios, to avoid the online merging process taking too long to occupy the training resources, it is recommended to [merge the complete weights](https://www.mindspore.cn/mindformers/docs/en/dev/function/transform_weight.html#safetensors-weight-merging) with the original distributed weights file offline, and then pass it in. There is no need to pass in the path of the source slicing strategy file.

For more details, please refer to: [Resumable Training](https://www.mindspore.cn/mindformers/docs/en/dev/function/resume_training.html).

## Weight Saving

### Overview

In the training process of deep learning models, saving the model weights is a crucial step. The weight saving function allows us to store the model parameters at any stage of training, so that users can restore, continue training, evaluate or deploy after training is interrupted or completed.  At the same time, by saving weights, experimental results can be reproduced in different environments.

Currently, MindSpore TransFormer supports reading and saving weight files in the [safetensors](https://www.mindspore.cn/mindformers/docs/en/dev/function/safetensors.html) format.

### Directory Structure

During the training process, MindSpore Transformers will generate a weight saving folder: `checkpoint` in the output directory (same as training log, default is `./output`).

If the configuration item `save_network_params:True` is set in yaml file, an additional weight saving folder `checkpoint_network` will be generated.

| Folder             | Description                                                                                                                                                                              |
|--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| checkpoint         | Save model weights, optimizer state, step and epoch in safetensors files, which can be used to **restore training from breakpoints**.                                                    |
| checkpoint_network | Only the model weight parameters are saved in the safetensors file, which is suitable for subsequent fine-tuning, reasoning, and evaluation. It does not support breakpoint continuation. |

#### `checkpoint` directory structure

Take an 8-rank task as an example, the weight files in the `output` folder are saved in the following format:

```text
output
    ├── checkpoint
        ├── rank_0
            ├── meta.json
            └── {prefix}-{epoch}_{step}.ckpt
        ...
        └── rank_7
            ├── meta.json
            └── {prefix}-{epoch}_{step}.ckpt
    └──checkpoint_network
        ├── rank_0
            └── {prefix}-{epoch}_{step}.safetensors
        ...
        └── rank_7
            └── {prefix}-{epoch}_{step}.safetensors
```

##### Weight-related file description

| File                                | Description                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|-------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| meta.json                           | Records the `epoch`, `step` and weight name of the last saved weight. Each rank process maintains a `meta.json` file independently.                                                                                                                                                                                                                                                                                                                     |
| {prefix}-{epoch}_{step}.safetensors | The saved weight file, `prefix` contains rank_id information, and the format is `{prefix}-{epoch}_{step}.safetensors`. If a file with the same prefix already exists, the system will automatically increment the suffix. <br>When data sinking is enabled, the `epoch` position is calculated as $\frac{CurrentTotalStepNumber}{SinkSize} = \frac{((CurrentEpoch-1)*StepsPerEpoch+CurrentStepInEpoch)}{SinkSize}$, and `step` is fixed to `sink_size`. |

### Configuration and Usage

#### YAML Parameter Configuration

Users can control the weight saving behavior by modifying the configuration file. The following are the main parameters:

Users can modify the fields under `CheckpointMonitor` in the `yaml` configuration file to control the weight saving behavior.

Taking [`DeepSeek-V3` pre-training yaml](https://gitee.com/mindspore/mindformers/blob/dev/research/deepseek3/deepseek3_671b/pretrain_deepseek3_671b.yaml#L206) as an example, the following configuration can be made:

```yaml
# callbacks
callbacks:
  ...
  - type: CheckpointMonitor
    prefix: "deepseekv3"
    save_checkpoint_steps: 1000
    keep_checkpoint_max: 5
    integrated_save: False
    async_save: False
    checkpoint_format: "safetensors"
  ...
```

The meaning of this configuration is: save the safetensors weights every 1000 steps, store up to 5 weights at the same time,
do not merge and save the split Tensor in parallel scenarios, and do not use asynchronous method to save weight files.

##### Introduction to main configuration parameters

The main parameters for saving weight configuration are listed in the following table:

| Parameter             | Description                                                                                                                                                                       | Value Description                                                                                                                                                                                                                          |
|-----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| prefix                | The prefix of the model weight file, which can be used to refer to the model name.                                                                                                | (str, optional) - Default value: `"CKP"`.                                                                                                                                                                                                  |
| save_checkpoint_steps | The number of training steps to save the weights.                                                                                                                                 | (int, optional) - Default value: `1`. If not set, the model weights will not be saved.                                                                                                                                                     |
| keep_checkpoint_max   | The maximum number of weight files to save at the same time. When the upper limit is reached, the oldest weight file will be deleted when saving the weights.                     | (int, optional) - Default value: `5`. If not set, the number of weights in the folder will not be monitored and deleted.                                                                                                                   |
| integrated_save       | Whether to merge and save the split Tensor in parallel scenarios. The merge save function is only supported in automatic parallel scenarios and not in manual parallel scenarios. | (bool, optional) - Default value: `False`                                                                                                                                                                                                  |
| async_save            | Whether to save safetensors files asynchronously.                                                                                                                                 | (bool, optional) - `True` uses asynchronous threads by default, default value: `False`.                                                                                                                                                    |
| checkpoint_format     | The format of the output file, which needs to be configured as `safetensors`.                                                                                                     | (str, optional) - The format in which the model weights are saved. Supports `"ckpt"` and `"safetensors"`. Default value: `ckpt`. (Note: The ckpt format will be sunset in subsequent versions, and the safetensors format is recommended.) |
| remove_redundancy     | Whether to remove redundancy when saving model weights.                                                                                                                           | (bool, optional) - Default value: `False`.                                                                                                                                                                                                 |
| save_network_params   | Whether to save only network parameters additionally.                                                                                                                             | (bool, optional) - Whether to save only network parameters additionally. Default value: `False`.                                                                                                                                           |

If you want to learn more about CheckpointMonitor, you can follow the [CheckpointMonitor API Documentation](https://www.mindspore.cn/mindformers/docs/en/dev/core/mindformers.core.CheckpointMonitor.html).
