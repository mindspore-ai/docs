# åˆ†è¯å™¨

<a href="https://gitee.com/mindspore/docs/blob/r1.1/docs/programming_guide/source_zh_cn/tokenizer.md" target="_blank"><img src="./_static/logo_source.png"></a>
&nbsp;&nbsp;
<a href="https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r1.1/programming_guide/mindspore_tokenizer.ipynb"><img src="./_static/logo_notebook.png"></a>
&nbsp;&nbsp;
<a href="https://console.huaweicloud.com/modelarts/?region=cn-north-4#/notebook/loading?share-url-b64=aHR0cHM6Ly9vYnMuZHVhbHN0YWNrLmNuLW5vcnRoLTQubXlodWF3ZWljbG91ZC5jb20vbWluZHNwb3JlLXdlYnNpdGUvbm90ZWJvb2svbW9kZWxhcnRzL3Byb2dyYW1taW5nX2d1aWRlL21pbmRzcG9yZV90b2tlbml6ZXIuaXB5bmI=&image_id=65f636a0-56cf-49df-b941-7d2a07ba8c8c" target="_blank"><img src="./_static/logo_modelarts.png"></a>

## æ¦‚è¿°

åˆ†è¯å°±æ˜¯å°†è¿ç»­çš„å­—åºåˆ—æŒ‰ç…§ä¸€å®šçš„è§„èŒƒé‡æ–°ç»„åˆæˆè¯åºåˆ—çš„è¿‡ç¨‹ï¼Œåˆç†çš„è¿›è¡Œåˆ†è¯æœ‰åŠ©äºè¯­ä¹‰çš„ç†è§£ã€‚

MindSporeæä¾›äº†å¤šç§ç”¨é€”çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·é«˜æ€§èƒ½åœ°å¤„ç†æ–‡æœ¬ï¼Œç”¨æˆ·å¯ä»¥æ„å»ºè‡ªå·±çš„å­—å…¸ï¼Œä½¿ç”¨é€‚å½“çš„æ ‡è®°å™¨å°†å¥å­æ‹†åˆ†ä¸ºä¸åŒçš„æ ‡è®°ï¼Œå¹¶é€šè¿‡æŸ¥æ‰¾æ“ä½œè·å–å­—å…¸ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚

MindSporeç›®å‰æä¾›çš„åˆ†è¯å™¨å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚æ­¤å¤–ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å®ç°è‡ªå®šä¹‰çš„åˆ†è¯å™¨ã€‚

| åˆ†è¯å™¨ | åˆ†è¯å™¨è¯´æ˜ |
| -- | -- |
| BasicTokenizer | æ ¹æ®æŒ‡å®šè§„åˆ™å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |
| BertTokenizer | ç”¨äºå¤„ç†Bertæ–‡æœ¬æ•°æ®çš„åˆ†è¯å™¨ã€‚ |
| JiebaTokenizer | åŸºäºå­—å…¸çš„ä¸­æ–‡å­—ç¬¦ä¸²åˆ†è¯å™¨ã€‚ |
| RegexTokenizer | æ ¹æ®æŒ‡å®šæ­£åˆ™è¡¨è¾¾å¼å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |
| SentencePieceTokenizer | åŸºäºSentencePieceå¼€æºå·¥å…·åŒ…è¿›è¡Œåˆ†è¯ã€‚ |
| UnicodeCharTokenizer | å°†æ ‡é‡æ–‡æœ¬æ•°æ®åˆ†è¯ä¸ºUnicodeå­—ç¬¦ã€‚ |
| UnicodeScriptTokenizer | æ ¹æ®Unicodeè¾¹ç•Œå¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |
| WhitespaceTokenizer | æ ¹æ®ç©ºæ ¼ç¬¦å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |
| WordpieceTokenizer | æ ¹æ®å•è¯é›†å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |

æ›´å¤šåˆ†è¯å™¨çš„è¯¦ç»†è¯´æ˜ï¼Œå¯ä»¥å‚è§[APIæ–‡æ¡£](https://www.mindspore.cn/doc/api_python/zh-CN/r1.1/mindspore/mindspore.dataset.text.html)ã€‚

## MindSporeåˆ†è¯å™¨

ä¸‹é¢ä»‹ç»å‡ ç§å¸¸ç”¨åˆ†è¯å™¨çš„ä½¿ç”¨æ–¹æ³•ã€‚

### BertTokenizer

`BertTokenizer`æ˜¯é€šè¿‡è°ƒç”¨`BasicTokenizer`å’Œ`WordpieceTokenizer`æ¥è¿›è¡Œåˆ†è¯çš„ã€‚

ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†å’Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç„¶åé€šè¿‡`BertTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["åºŠå‰æ˜æœˆå…‰", "ç–‘æ˜¯åœ°ä¸Šéœœ", "ä¸¾å¤´æœ›æ˜æœˆ", "ä½å¤´æ€æ•…ä¹¡", "I am making small mistakes during working hours",
                "ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»", "ç¹é«”å­—"]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

vocab_list = [
  "åºŠ", "å‰", "æ˜", "æœˆ", "å…‰", "ç–‘", "æ˜¯", "åœ°", "ä¸Š", "éœœ", "ä¸¾", "å¤´", "æœ›", "ä½", "æ€", "æ•…", "ä¹¡",
  "ç¹", "é«”", "å­—", "å˜¿", "å“ˆ", "å¤§", "ç¬‘", "å˜»", "i", "am", "mak", "make", "small", "mistake",
  "##s", "during", "work", "##ing", "hour", "ğŸ˜€", "ğŸ˜ƒ", "ğŸ˜„", "ğŸ˜", "+", "/", "-", "=", "12",
  "28", "40", "16", " ", "I", "[CLS]", "[SEP]", "[UNK]", "[PAD]", "[MASK]", "[unused1]", "[unused10]"]

vocab = text.Vocab.from_list(vocab_list)
tokenizer_op = text.BertTokenizer(vocab=vocab)
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']))
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```text
------------------------before tokenization----------------------------
åºŠå‰æ˜æœˆå…‰
ç–‘æ˜¯åœ°ä¸Šéœœ
ä¸¾å¤´æœ›æ˜æœˆ
ä½å¤´æ€æ•…ä¹¡
I am making small mistakes during working hours
ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»
ç¹é«”å­—
------------------------after tokenization-----------------------------
['åºŠ' 'å‰' 'æ˜' 'æœˆ' 'å…‰']
['ç–‘' 'æ˜¯' 'åœ°' 'ä¸Š' 'éœœ']
['ä¸¾' 'å¤´' 'æœ›' 'æ˜' 'æœˆ']
['ä½' 'å¤´' 'æ€' 'æ•…' 'ä¹¡']
['I' 'am' 'mak' '##ing' 'small' 'mistake' '##s' 'during' 'work' '##ing'
 'hour' '##s']
['ğŸ˜€' 'å˜¿' 'å˜¿' 'ğŸ˜ƒ' 'å“ˆ' 'å“ˆ' 'ğŸ˜„' 'å¤§' 'ç¬‘' 'ğŸ˜' 'å˜»' 'å˜»']
['ç¹' 'é«”' 'å­—']
```

### JiebaTokenizer

`JiebaTokenizer`æ˜¯åŸºäºjiebaçš„ä¸­æ–‡åˆ†è¯ã€‚

ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨HMMä¸MPå­—å…¸æ–‡ä»¶åˆ›å»º`JiebaTokenizer`å¯¹è±¡ï¼Œå¹¶å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œæœ€åå±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§"]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

# files from open source repository https://github.com/yanyiwu/cppjieba/tree/master/dict
HMM_FILE = "hmm_model.utf8"
MP_FILE = "jieba.dict.utf8"
jieba_op = text.JiebaTokenizer(HMM_FILE, MP_FILE)
dataset = dataset.map(operations=jieba_op, input_columns=["text"], num_parallel_workers=1)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']))
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```text
------------------------before tokenization----------------------------
ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§
------------------------after tokenization-----------------------------
['ä»Šå¤©å¤©æ°”' 'å¤ªå¥½äº†' 'æˆ‘ä»¬' 'ä¸€èµ·' 'å»' 'å¤–é¢' 'ç©å§']
```

### SentencePieceTokenizer

`SentencePieceTokenizer`æ˜¯åŸºäº[SentencePiece](https://github.com/google/sentencepiece)è¿™ä¸ªå¼€æºçš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ã€‚

ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä»`vocab_file`æ–‡ä»¶ä¸­æ„å»ºä¸€ä¸ª`vocab`å¯¹è±¡ï¼Œå†é€šè¿‡`SentencePieceTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text
from mindspore.dataset.text import SentencePieceModel, SPieceTokenizerOutType

input_list = ["I saw a girl with a telescope."]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

# file from MindSpore repository https://gitee.com/mindspore/mindspore/blob/r1.1/tests/ut/data/dataset/test_sentencepiece/botchan.txt
vocab_file = "botchan.txt"
vocab = text.SentencePieceVocab.from_file([vocab_file], 5000, 0.9995, SentencePieceModel.UNIGRAM, {})
tokenizer_op = text.SentencePieceTokenizer(vocab, out_type=SPieceTokenizerOutType.STRING)
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']))
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```text
------------------------before tokenization----------------------------
I saw a girl with a telescope.
------------------------after tokenization-----------------------------
['â–I' 'â–sa' 'w' 'â–a' 'â–girl' 'â–with' 'â–a' 'â–te' 'les' 'co' 'pe' '.']
```

### UnicodeCharTokenizer

`UnicodeCharTokenizer`æ˜¯æ ¹æ®Unicodeå­—ç¬¦é›†æ¥åˆ†è¯çš„ã€‚

ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åé€šè¿‡`UnicodeCharTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["Welcome to Beijing!", "åŒ—äº¬æ¬¢è¿æ‚¨ï¼", "æˆ‘å–œæ¬¢English!"]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

tokenizer_op = text.UnicodeCharTokenizer()
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']).tolist())
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```text
------------------------before tokenization----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenization-----------------------------
['W', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'B', 'e', 'i', 'j', 'i', 'n', 'g', '!']
['åŒ—', 'äº¬', 'æ¬¢', 'è¿', 'æ‚¨', 'ï¼']
['æˆ‘', 'å–œ', 'æ¬¢', 'E', 'n', 'g', 'l', 'i', 's', 'h', '!']
```

### WhitespaceTokenizer

`WhitespaceTokenizer`æ˜¯æ ¹æ®ç©ºæ ¼æ¥è¿›è¡Œåˆ†è¯çš„ã€‚

ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åé€šè¿‡`WhitespaceTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["Welcome to Beijing!", "åŒ—äº¬æ¬¢è¿æ‚¨ï¼", "æˆ‘å–œæ¬¢English!"]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

tokenizer_op = text.WhitespaceTokenizer()
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']).tolist())
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```text
------------------------before tokenization----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenization-----------------------------
['Welcome', 'to', 'Beijing!']
['åŒ—äº¬æ¬¢è¿æ‚¨ï¼']
['æˆ‘å–œæ¬¢English!']
```

### WordpieceTokenizer

`WordpieceTokenizer`æ˜¯åŸºäºå•è¯é›†æ¥è¿›è¡Œåˆ’åˆ†çš„ï¼Œåˆ’åˆ†ä¾æ®å¯ä»¥æ˜¯å•è¯é›†ä¸­çš„å•ä¸ªå•è¯ï¼Œæˆ–è€…å¤šä¸ªå•è¯çš„ç»„åˆå½¢å¼ã€‚

ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä»å•è¯åˆ—è¡¨ä¸­æ„å»º`vocab`å¯¹è±¡ï¼Œé€šè¿‡`WordpieceTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["my", "favorite", "book", "is", "love", "during", "the", "cholera", "era", "what",
    "æˆ‘", "æœ€", "å–œ", "æ¬¢", "çš„", "ä¹¦", "æ˜¯", "éœ", "ä¹±", "æ—¶", "æœŸ", "çš„", "çˆ±", "æƒ…", "æ‚¨"]
vocab_english = ["book", "cholera", "era", "favor", "##ite", "my", "is", "love", "dur", "##ing", "the"]
vocab_chinese = ["æˆ‘", 'æœ€', 'å–œ', 'æ¬¢', 'çš„', 'ä¹¦', 'æ˜¯', 'éœ', 'ä¹±', 'æ—¶', 'æœŸ', 'çˆ±', 'æƒ…']

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

vocab = text.Vocab.from_list(vocab_english+vocab_chinese)
tokenizer_op = text.WordpieceTokenizer(vocab=vocab)
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']))
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```text
------------------------before tokenization----------------------------
my
favorite
book
is
love
during
the
cholera
era
what
æˆ‘
æœ€
å–œ
æ¬¢
çš„
ä¹¦
æ˜¯
éœ
ä¹±
æ—¶
æœŸ
çš„
çˆ±
æƒ…
æ‚¨
------------------------after tokenization-----------------------------
['my']
['favor' '##ite']
['book']
['is']
['love']
['dur' '##ing']
['the']
['cholera']
['era']
['[UNK]']
['æˆ‘']
['æœ€']
['å–œ']
['æ¬¢']
['çš„']
['ä¹¦']
['æ˜¯']
['éœ']
['ä¹±']
['æ—¶']
['æœŸ']
['çš„']
['çˆ±']
['æƒ…']
['[UNK]']
```
