准备训练脚本
------------

收集神经网络性能数据有两种方式，可以使用以下任意一种方式使能Profiler。

方式一：修改训练脚本
~~~~~~~~~~~~~~~~~~~~

在训练脚本中添加MindSpore Profiler相关接口。

- 在训练开始前，初始化MindSpore ``Profiler``\ 对象，Profiler开启收集性能数据。

  .. note::
     Profiler支持的参数可以参考：
     `Profiler API <https://www.mindspore.cn/docs/zh-CN/master/api_python/mindspore/mindspore.Profiler.html#mindspore.Profiler>`_ 。
     Profiler初始化之前需要确定device_id。

- 在训练结束后，调用\ ``Profiler.analyse()``\ 停止性能数据收集并生成性能分析结果。

**按条件开启样例：**

用户可以通过设置初始化参数start_profile为False来决定暂时不开启Profiler，然后通过调用start函数来在适当的时机开启Profiler，再调用stop函数停止收集数据，最后调用analyse解析数据。
可以是基于epoch或者step开启和关闭Profiler，只收集指定step区间或者epoch区间的数据。基于step或者基于epoch性能数据的收集有两种方式，一种是用户自定义训练，另一种是借助Callback基于step或者epoch开启关闭Profiler。

- 自定义训练：

  MindSpore函数式编程用例使用Profiler进行自定义训练，可以在指定的step区间或者epoch区间开启或者关闭收集Profiler性能数据。`基于step开启Profiler完整代码样例 <https://gitee.com/mindspore/docs/blob/master/docs/sample_code/mindinsight/profiler/profiling_step.py>`_。

  .. code:: python

     profiler = ms.Profiler(start_profile=False)
     data_loader = ds.create_dict_iterator()

     for i, data in enumerate(data_loader):
         train()
         if i==100:
             profiler.start()
         if i==200:
             profiler.stop()

     profiler.analyse()

- 自定义Callback

  - 对于数据非下沉模式，只有在每个step结束后才有机会告知CANN开启和停止，因此需要基于step开启和关闭。
    `自定义Callback基于step开启Profiler完整代码样例 <https://gitee.com/mindspore/docs/blob/master/docs/sample_code/mindinsight/profiler/profiling_feed_step.py>`_。

    .. code:: python

       import os
       import mindspore as ms
       from mindspore.communication import get_rank

       def get_real_rank():
           """get rank id"""
           try:
               return get_rank()
           except RuntimeError:
               return int(os.getenv("RANK_ID", "0"))

       class StopAtStep(ms.Callback):
           def __init__(self, start_step, stop_step):
               super(StopAtStep, self).__init__()
               self.start_step = start_step
               self.stop_step = stop_step
               # 按照rank_id设置性能数据落盘路径
               rank_id = get_real_rank()
               output_path = os.path.join("profiler_data", f"rank_{rank_id}")
               self.profiler = ms.Profiler(start_profile=False, output_path=output_path)

           def on_train_step_begin(self, run_context):
               cb_params = run_context.original_args()
               step_num = cb_params.cur_step_num
               if step_num == self.start_step:
                   self.profiler.start()

           def on_train_step_end(self, run_context):
               cb_params = run_context.original_args()
               step_num = cb_params.cur_step_num
               if step_num == self.stop_step:
                   self.profiler.stop()
                   self.profiler.analyse()

  - 对于数据下沉模式，只有在每个epoch结束后才有机会告知CANN开启和停止，因此需要基于epoch开启和关闭。可根据自定义Callback基于step开启Profiler样例代码修改训练脚本。

    .. code:: python

       class StopAtEpoch(ms.Callback):
           def __init__(self, start_epoch, stop_epoch):
               super(StopAtEpoch, self).__init__()
               self.start_epoch = start_epoch
               self.stop_epoch = stop_epoch
               # 按照rank_id设置性能数据落盘路径
               rank_id = get_real_rank()
               output_path = os.path.join("profiler_data", f"rank_{rank_id}")
               self.profiler = ms.Profiler(start_profile=False, output_path=output_path)

           def on_train_epoch_begin(self, run_context):
               cb_params = run_context.original_args()
               epoch_num = cb_params.cur_epoch_num
               if epoch_num == self.start_epoch:
                   self.profiler.start()

           def on_train_epoch_end(self, run_context):
               cb_params = run_context.original_args()
               epoch_num = cb_params.cur_epoch_num
               if epoch_num == self.stop_epoch:
                   self.profiler.stop()
                   self.profiler.analyse()

**非条件开启样例：**

- 样例一：MindSpore函数式编程用例中使用Profiler收集性能数据，部分样例代码如下所示。`完整代码样例 <https://gitee.com/mindspore/docs/blob/master/docs/sample_code/mindinsight/profiler/profiling_sample.py>`_ 。

  .. code:: python

     # Init Profiler.
     # Note that the Profiler should be initialized before model training.
     profiler = Profiler(output_path="profiler_data")

     def forward_fn(data, label):
         logits = model(data)
         loss = loss_fn(logits, label)
         return loss, logits


     # Get gradient function
     grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)


     @ms.jit
     def train_step(data, label):
         """Define function of one-step training"""
         (loss, _), grads = grad_fn(data, label)
         optimizer(grads)
         return loss


     for t in range(epochs):
         train_loop(model, train_dataset, loss_fn, optimizer)

     profiler.analyse()


- 样例二：使用model.train进行网络训练，完整代码如下所示。

  .. code:: python

     import numpy as np
     from mindspore import nn
     from mindspore.train import Model
     import mindspore as ms
     import mindspore.dataset as ds

     class Net(nn.Cell):
         def __init__(self):
             super(Net, self).__init__()
             self.fc = nn.Dense(2, 2)

         def construct(self, x):
             return self.fc(x)


     def generator():
         for i in range(2):
             yield (np.ones([2, 2]).astype(np.float32), np.ones([2]).astype(np.int32))


     def train(net):
         optimizer = nn.Momentum(net.trainable_params(), 1, 0.9)
         loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)
         data = ds.GeneratorDataset(generator, ["data", "label"])
         model = Model(net, loss, optimizer)
         model.train(1, data)


     if __name__ == '__main__':
         ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend")

         # Init Profiler
         # Note that the Profiler should be initialized before model.train
         profiler = ms.Profiler(output_path='./profiler_data')

         # Train Model
         net = Net()
         train(net)

         # Profiler end
         profiler.analyse()

方式二：环境变量使能
~~~~~~~~~~~~~~~~~~~~

在运行网络脚本前，配置Profiler相关配置项。

说明：

- 使用环境变量使能方式，请在脚本开始执行之前通过环境变量设置好device id。禁止在脚本中通过set_context函数设置device id。

.. code-block:: shell

   export MS_PROFILER_OPTIONS='{"start": true, "output_path": "/XXX", "profile_memory": false, "profile_communication": false, "aicore_metrics": 0, "l2_cache": false}'

- `start` (bool，必选) - 设置为true，表示使能Profiler；设置成false，表示关闭性能数据收集，默认值：false。

- `output_path` (str, 可选) - 表示输出数据的路径（绝对路径）。默认值："./data"。

- `op_time` (bool, 可选) - 表示是否收集算子性能数据，默认值：true。

- `profile_memory` (bool，可选) - 表示是否收集Tensor内存数据。当值为true时，收集这些数据。使用此参数时，`op_time` 必须设置成true。默认值：false。

- `profile_communication` (bool, 可选) - 表示是否在多设备训练中收集通信性能数据。当值为true时，收集这些数据。在单台设备训练中，该参数的设置无效。使用此参数时，`op_time` 必须设置成true。默认值：false。

- `aicore_metrics` (int, 可选) - 设置AI Core指标类型，使用此参数时，`op_time` 必须设置成true。默认值：0。

- `l2_cache` (bool, 可选) - 设置是否收集l2缓存数据，默认值：false。

- `timeline_limit` (int, 可选) - 设置限制timeline文件存储上限大小（单位M），使用此参数时，`op_time` 必须设置成true。默认值：500。

- `data_process` (bool, 可选) - 表示是否收集数据准备性能数据，默认值：true。

- `parallel_strategy` (bool, 可选) - 表示是否收集并行策略性能数据，默认值：true。

- `profile_framework` (str, 可选) - 是否需要收集Host侧的内存和时间，可选参数为["all", "time", null]。默认值："all"。

方式三：动态profile使能
~~~~~~~~~~~~~~~~~~~~~~~

mindspore.profiler.DynamicProfilerMonitor提供用户动态修改Profiler配置参数的能力，修改配置时无需中断训练流程，初始化生成的JSON配置文件示例如下。

.. code:: python

   {
      "start_step": -1,
      "stop_step": -1,
      "aicore_metrics": -1,
      "profiler_level": -1,
      "profile_framework": -1,
      "analyse_mode": -1,
      "profile_communication": false,
      "parallel_strategy": false,
      "with_stack": false,
      "data_simplification": true
   }

- `start_step` (int, 必选) - 设置Profiler开始采集的步数，为相对值，训练的第一步为1。默认值-1，表示在整个训练流程不会开始采集。

- `stop_step` (int, 必选) - 设置Profiler开始停止的步数，为相对值，训练的第一步为1，需要满足stop_step大于等于start_step。默认值-1，表示在整个训练流程不会开始采集。

- `aicore_metrics` (int, 可选) - 设置采集AI Core指标数据，取值范围与Profiler一致。默认值-1，表示不采集AI Core指标。

- `profiler_level` (int, 可选) - 设置采集性能数据级别，0代表ProfilerLevel.Level0，1代表ProfilerLevel.Level1，2代表ProfilerLevel.Level2。默认值-1，表示不控制性能数据采集级别。

- `profile_framework` (int, 可选) - 设置收集的host信息类别，0代表"all"，1代表"time"。默认值-1，表示不采集host信息。

- `analyse_mode` (int, 可选) - 设置在线解析的模式，对应mindspore.Profiler.analyse接口的analyse_mode参数，0代表"sync"，1代表"async"。默认值-1，表示不使用在线解析。

- `profile_communication` (bool, 可选) - 设置是否在多设备训练中采集通信性能数据，true代表采集，false代表不采集。默认值false，表示不采集集通信性能数据。

- `parallel_strategy` (bool, 可选) - 设置是否采集并行策略性能数据，true代表采集，false代表不采集。默认值false，表示不采集并行策略性能数据。

- `with_stack` (bool, 可选) - 设置是否采集调用栈信息，true代表采集，false代表不采集。默认值false，表示不采集调用栈。

- `data_simplification` (bool, 可选) - 设置开启数据精简，true代表开启，false代表不开启。默认值true，表示开启数据精简。

- 样例一：使用model.train进行网络训练，将DynamicProfilerMonitor注册到model.train。

  - 步骤一：在训练代码中添加DynamicProfilerMonitor，将其注册到训练流程。

    .. code:: python

       import numpy as np
       from mindspore import nn
       from mindspore.train import Model
       import mindspore as ms
       import mindspore.dataset as ds
       from mindspore.profiler import DynamicProfilerMonitor

       class Net(nn.Cell):
           def __init__(self):
               super(Net, self).__init__()
               self.fc = nn.Dense(2, 2)

           def construct(self, x):
               return self.fc(x)


       def generator():
           for i in range(2):
               yield (np.ones([2, 2]).astype(np.float32), np.ones([2]).astype(np.int32))


       def train(net):
           optimizer = nn.Momentum(net.trainable_params(), 1, 0.9)
           loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)
           data = ds.GeneratorDataset(generator, ["data", "label"])

           # cfg_path参数为共享配置文件的文件夹路径，多机场景下需要满足此路径所有节点都能访问到
           # output_path参数为动态profile数据保存路径
           profile_callback = DynamicProfilerMonitor(cfg_path="./dyn_cfg", output_path="./dynprof_data")
           model = Model(net, loss, optimizer)
           model.train(10, data, callbacks=[profile_callback])


       if __name__ == '__main__':
           ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend")

           # Train Mode
           net = Net()
           train(net)

  - 步骤二：拉起训练流程，动态修改配置文件实现动态采集性能数据。拉起训练后，DynamicProfilerMonitor会在指定的cfg_path路径下生成配置文件profiler_config.json，用户可以动态编辑该配置文件，比如修改为下面的配置，表示DynamicProfilerMonitor将会在训练的第10个step开始采集，第10个step停止采集后在线解析。

    .. code:: python

       {
         "start_step": 10,
         "stop_step": 10,
         "aicore_metrics": -1,
         "profiler_level": -1,
         "profile_framework": -1,
         "analyse_mode": 0,
         "profile_communication": false,
         "parallel_strategy": true,
         "with_stack": true,
         "data_simplification": false
       }

- 样例二：MindFormers中使用DynamicProfilerMonitor。

  - 步骤一：在MindFormers中添加DynamicProfilerMonitor，将其注册到训练流程。修改mindformers/trainer/trainer.py中的_build_profile_cb函数，将其默认的ProfileMonitor修改为DynamicProfilerMonitor，修改示例如下。

    .. code:: python

      def _build_profile_cb(self):
        """build profile callback from config."""
        if self.config.profile:
            sink_size = self.config.runner_config.sink_size
            sink_mode = self.config.runner_config.sink_mode
            if sink_mode:
                if self.config.profile_start_step % sink_size != 0:
                    self.config.profile_start_step -= self.config.profile_start_step % sink_size
                    self.config.profile_start_step = max(self.config.profile_start_step, sink_size)
                    logger.warning("profile_start_step should divided by sink_size, \
                        set profile_start_step to %s", self.config.profile_start_step)
                if self.config.profile_stop_step % sink_size != 0:
                    self.config.profile_stop_step += self.config.profile_stop_step % sink_size
                    self.config.profile_stop_step = max(self.config.profile_stop_step, \
                        self.config.profile_start_step + sink_size)
                    logger.warning("profile_stop_step should divided by sink_size, \
                        set profile_stop_step to %s", self.config.profile_stop_step)

            start_profile = self.config.init_start_profile
            profile_communication = self.config.profile_communication

            # 添加DynamicProfilerMonitor，替换原有的ProfileMonitor
            from mindspore.profiler import DynamicProfilerMonitor

            # cfg_path参数为共享配置文件的文件夹路径，多机场景下需要满足此路径所有节点都能访问到
            # output_path参数为动态profile数据保存路径
            profile_cb = DynamicProfilerMonitor(cfg_path="./dyn_cfg", output_path="./dynprof_data")

            # 原始的ProfileMonitor不再使用
            # profile_cb = ProfileMonitor(
            #     start_step=self.config.profile_start_step,
            #     stop_step=self.config.profile_stop_step,
            #     start_profile=start_profile,
            #     profile_communication=profile_communication,
            #     profile_memory=self.config.profile_memory,
            #     output_path=self.config.profile_output,
            #     config=self.config)
            logger.warning(
                "Please reduce the data sample size with 'num_samples' in MindSpore data format according to "
                "https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling_ascend.html.")
            logger.warning("In profiler mode, auto-tune will be turned off.")
            self.config.auto_tune = False
            self.config.profile_cb = profile_cb

  - 步骤二：在模型的yaml配置文件中开启profile功能后拉起训练，拉起训练后，DynamicProfilerMonitor会在指定的cfg_path路径下生成配置文件profiler_config.json，用户可以动态编辑该配置文件，比如修改为下面的配置，表示DynamicProfilerMonitor将会在训练的第10个step开始采集，第10个step停止采集后在线解析。

    .. code:: python

       {
         "start_step": 10,
         "stop_step": 10,
         "aicore_metrics": -1,
         "profiler_level": -1,
         "profile_framework": -1,
         "analyse_mode": 0,
         "profile_communication": false,
         "parallel_strategy": true,
         "with_stack": true,
         "data_simplification": false
       }