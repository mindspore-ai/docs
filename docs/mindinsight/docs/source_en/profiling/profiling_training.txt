Preparing the Training Script
-----------------------------

There are two ways to collect neural network performance data. You can enable Profiler in either of the following ways.

Method 1: Modify the training script
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Add the MindSpore Profiler interface to the training script.

- Before training, initialize the MindSpore Profiler object, and profiler enables collection of performance data.

  .. note::
     The parameters of Profiler are as follows:
     `Profiler API <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.Profiler.html#mindspore.Profiler>`_ .
     Before initializing the Profiler, you need to determine the device_id.

- At the end of the training, ``Profiler.analyse()`` should be called to finish profiling and generate the performance analyse results.

** Conditional open example: **

The user decides not to start the Profiler by setting the initialization parameter start_profile to False, then starts the Profiler at the right time by calling Start, stops collecting data, and finally calls analyse to phase the data.
You can open and close the Profiler based on the epoch or step, and the data within the specified step or epoch interval is collected. There are two ways to collect performance data based on step or epoch, one is through user-defined training,
the other is through Callback based on step or epoch to open and close Profiler.

- Custom training:

  The MindSpore functional programming use case uses profilers for custom training by turning Profiler performance data on or off during a specified step interval or epoch interval. `Enable Profiler's complete code sample based on step <https://gitee.com/mindspore/docs/blob/master/docs/sample_code/mindinsight/profiler/profiling_step.py>`_.

  .. code:: python

     profiler = ms.Profiler(start_profile=False)
     data_loader = ds.create_dict_iterator()

     for i, data in enumerate(data_loader):
         train()
         if i==100:
             profiler.start()
         if i==200:
             profiler.stop()

     profiler.analyse()

- User-defined Callback

  - For data non-sink mode, there is only an opportunity to turn on and off CANN at the end of each step, so whether the CANN is turned on or off is based on the step. A custom Callback opens the Profiler's `complete code sample based on step <https://gitee.com/mindspore/docs/blob/master/docs/sample_code/mindinsight/profiler/profiling_feed_step.py>`_ .

    .. code:: python

       import os
       import mindspore as ms
       from mindspore.communication import get_rank

       def get_real_rank():
           """get rank id"""
           try:
               return get_rank()
           except RuntimeError:
               return int(os.getenv("RANK_ID", "0"))

       class StopAtStep(ms.Callback):
           def __init__(self, start_step, stop_step):
               super(StopAtStep, self).__init__()
               self.start_step = start_step
               self.stop_step = stop_step
               # Set the path for performance data to disks as rank_id
               rank_id = get_real_rank()
               output_path = os.path.join("profiler_data", f"rank_{rank_id}")
               self.profiler = ms.Profiler(start_profile=False, output_path=output_path)

           def on_train_step_begin(self, run_context):
               cb_params = run_context.original_args()
               step_num = cb_params.cur_step_num
               if step_num == self.start_step:
                   self.profiler.start()

           def on_train_step_end(self, run_context):
               cb_params = run_context.original_args()
               step_num = cb_params.cur_step_num
               if step_num == self.stop_step:
                   self.profiler.stop()
                   self.profiler.analyse()

- For data sink mode, CANN is told to start and stop only after the end of each epoch, so it needs to start and stop based on the epoch. The Profiler sample code modification training script can be opened based on step according to a custom Callback.

  .. code:: python

      class StopAtEpoch(ms.Callback):
          def __init__(self, start_epoch, stop_epoch):
              super(StopAtEpoch, self).__init__()
              self.start_epoch = start_epoch
              self.stop_epoch = stop_epoch
               # Set the path for performance data to disks as rank_id
               rank_id = get_real_rank()
               output_path = os.path.join("profiler_data", f"rank_{rank_id}")
               self.profiler = ms.Profiler(start_profile=False, output_path=output_path)

          def on_train_epoch_begin(self, run_context):
              cb_params = run_context.original_args()
              epoch_num = cb_params.cur_epoch_num
              if epoch_num == self.start_epoch:
                  self.profiler.start()

          def on_train_epoch_end(self, run_context):
              cb_params = run_context.original_args()
              epoch_num = cb_params.cur_epoch_num
              if epoch_num == self.stop_epoch:
                  self.profiler.stop()
                  self.profiler.analyse()

** Unconditional Open example: **

- Example 1: In the MindSpore functional Programming use case, Profiler is used to collect performance data. Part of the sample code is shown below.
  `Complete code sample <https://gitee.com/mindspore/docs/blob/master/docs/sample_code/mindinsight/profiler/profiling_sample.py>`_ .

  .. code:: python

     # Init Profiler.
     # Note that the Profiler should be initialized before model training.
     profiler = Profiler(output_path="profiler_data")

     def forward_fn(data, label):
         logits = model(data)
         loss = loss_fn(logits, label)
         return loss, logits


     # Get gradient function
     grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)


     @ms.jit
     def train_step(data, label):
         """Define function of one-step training"""
         (loss, _), grads = grad_fn(data, label)
         optimizer(grads)
         return loss


     for t in range(epochs):
         train_loop(model, train_dataset, loss_fn, optimizer)

     profiler.analyse()


- Example 2: model.train is used for network training. The complete code is as follows:

  .. code:: python

     import numpy as np
     from mindspore import nn
     from mindspore.train import Model
     import mindspore as ms
     import mindspore.dataset as ds

     class Net(nn.Cell):
         def __init__(self):
             super(Net, self).__init__()
             self.fc = nn.Dense(2, 2)

         def construct(self, x):
             return self.fc(x)


     def generator():
         for i in range(2):
             yield (np.ones([2, 2]).astype(np.float32), np.ones([2]).astype(np.int32))


     def train(net):
         optimizer = nn.Momentum(net.trainable_params(), 1, 0.9)
         loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)
         data = ds.GeneratorDataset(generator, ["data", "label"])
         model = Model(net, loss, optimizer)
         model.train(1, data)


     if __name__ == '__main__':
         ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend")

         # Init Profiler
         # Note that the Profiler should be initialized before model.train
         profiler = ms.Profiler(output_path='./profiler_data')

         # Train Model
         net = Net()
         train(net)

         # Profiler end
         profiler.analyse()

Method 2: Enable environment variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before running the network script, configure Profiler configuration items.

Note：

- To enable using environment variables, please set the device ID through the environment variables before the script starts executing. Prohibit using the set_context function to set the device ID in the script.

.. code-block:: shell

   export MS_PROFILER_OPTIONS='{"start": true, "output_path": "/XXX", "profile_memory": false, "profile_communication": false, "aicore_metrics": 0, "l2_cache": false}'

- `start` (bool, mandatory) - Set to true to enable Profiler. Set false to disable performance data collection. Default value: false.

- `output_path` (str, optional) - Represents the path (absolute path) of the output data. Default value： "./data".

- `op_time` (bool, optional) - Indicates whether to collect operators performance data. Default values: true.

- `profile_memory` (bool, optional) - Tensor data will be collected. This data is collected when the value is true. When using this parameter, `op_time` must be set to true. Default value: false.

- `profile_communication` (bool, optional) - Indicates whether to collect communication performance data in multi-device training. This data is collected when the value is true. In single-device training, this parameter is not set correctly. When using this parameter, `op_time` must be set to true. Default value: false.

- `aicore_metrics` (int, optional) - Set the indicator type of AI Core. When using this parameter, `op_time` must be set to true. Default value: 0.

- `l2_cache` (bool, optional) - Set whether to collect l2 cache data. Default value: false.

- `timeline_limit` (int, optional) - Set the maximum storage size of the timeline file (unit M). When using this parameter, `op_time` must be set to true. Default value: 500.

- `data_process` (bool, optional) - Indicates whether to collect data to prepare performance data. Default value: true.

- `parallel_strategy` (bool, optional) - Indicates whether to collect parallel policy performance data. Default value: true.

- `profile_framework` (str, optional) - Whether to collect host time, it must be one of ["all", "time", null]. Default: null.

Method 3：Enable Dynamic Profile
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

mindspore.profiler.DynamicProfilerMonitor provides the ability to dynamically modify the configuration parameters of the profiler without interrupting the training process. The following is an example of the generated JSON configuration file for initialization.

.. code:: python

   {
      "start_step": -1,
      "stop_step": -1,
      "aicore_metrics": -1,
      "profiler_level": -1,
      "profile_framework": -1,
      "analyse_mode": -1,
      "profile_communication": false,
      "parallel_strategy": false,
      "with_stack": false,
      "data_simplification": true
   }

- `start_step` (int, mandatory) - Sets the number of steps at which the Profiler will start acquisition, as a relative value, with the first step of training being 1. The default value of -1 means that acquisition will not start throughout the training process.

- `stop_step` (int, mandatory) - Sets the number of steps at which the Profiler starts and stops, as a relative value. The first step of training is 1, which needs to be satisfied that stop_step is greater than or equal to start_step. The default value of -1 means that acquisition will not be started throughout the training process.

- `aicore_metrics` (int, optional) - Set to collect AI Core metrics data, the value range is consistent with Profiler. The default value of -1 means no AI Core metrics are collected.

- `profiler_level` (int, optional) - Sets the performance data collection level, 0 for ProfilerLevel.Level0, 1 for ProfilerLevel.Level1, 2 for ProfilerLevel.Level2. The default value is -1, indicating that the performance data collection level is not controlled.

- `profile_framework` (int, optional) - Set the category of host information to be collected, 0 for “all”, 1 for “time”. Default value is -1, which means no host information will be collected.

- `analyse_mode` (int, optional) - Set the mode of online parsing, it corresponds to the analyse_mode parameter of mindspore.Profiler.analyse interface, 0 means “sync”, 1 means “async”. The default value is -1, which means online parsing is not used.

- `profile_communication` (bool, optional) - Set whether to collect communication performance data in multi-device training. True indicates that communication performance data is collected, while false, the default value, indicates that communication performance data is not collected.

- `parallel_strategy` (bool, optional) - Set whether to collect parallel strategy performance data. True indicates that parallel strategy performance data is collected, while false, the default value, indicates that parallel strategy performance data is not collected.

- `with_stack` (bool, optional) - Set whether to capture call stack information. True indicates that the call stack information is collected, and false, the default value, indicates that the call stack information is not collected.

- `data_simplification` (bool, optional) - Set to enable data simplification, true means on, false means off. The default value is true, which means data simplification is enabled.

- Example 1: Use model.train for network training, register DynamicProfilerMonitor to model.train.

   - Step 1: Add DynamicProfilerMonitor to the training code to register it to the training flow.

     .. code:: python

         import numpy as np
         from mindspore import nn
         from mindspore.train import Model
         import mindspore as ms
         import mindspore.dataset as ds
         from mindspore.profiler import DynamicProfilerMonitor

         class Net(nn.Cell):
             def __init__(self):
                 super(Net, self).__init__()
                 self.fc = nn.Dense(2, 2)

             def construct(self, x):
                 return self.fc(x)


         def generator():
             for i in range(2):
                 yield (np.ones([2, 2]).astype(np.float32), np.ones([2]).astype(np.int32))


         def train(net):
             optimizer = nn.Momentum(net.trainable_params(), 1, 0.9)
             loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)
             data = ds.GeneratorDataset(generator, ["data", "label"])

             # The cfg_path parameter is the path to the folder where the configuration file is to be shared, which needs to be accessible by all nodes in a cluster scenario
             # The output_path parameter is the path where the dynamic profile data is saved.
             profile_callback = DynamicProfilerMonitor(cfg_path="./dyn_cfg", output_path="./dynprof_data")
             model = Model(net, loss, optimizer)
             model.train(10, data, callbacks=[profile_callback])


         if __name__ == '__main__':
             ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend")

             # Train Mode
             net = Net()
             train(net)

   - Step 2: Pull up the training process and dynamically modify the configuration file to realize dynamic collection of performance data. After pulling up the training, DynamicProfilerMonitor will generate the configuration file profiler_config.json under the specified cfg_path, and the user can dynamically edit the configuration file, such as modifying it to the following configuration, which means that DynamicProfilerMonitor will start collecting performance data at the 10th step of training, and the online parsing will stop at the 10th step.

     .. code:: python

        {
          "start_step": 10,
          "stop_step": 10,
          "aicore_metrics": -1,
          "profiler_level": -1,
          "profile_framework": -1,
          "analyse_mode": 0,
          "profile_communication": false,
          "parallel_strategy": true,
          "with_stack": true,
          "data_simplification": false
        }

- Example 2：DynamicProfilerMonitor is used in MindFormers.

    - Step 1: Add DynamicProfilerMonitor to MindFormers to register it to the training process. Modify the _build_profile_cb function in mindformers/trainer/trainer.py to change its default ProfileMonitor to DynamicProfilerMonitor, and the modification example is as follows.

      .. code:: python

          def _build_profile_cb(self):
            """build profile callback from config."""
            if self.config.profile:
                sink_size = self.config.runner_config.sink_size
                sink_mode = self.config.runner_config.sink_mode
                if sink_mode:
                    if self.config.profile_start_step % sink_size != 0:
                        self.config.profile_start_step -= self.config.profile_start_step % sink_size
                        self.config.profile_start_step = max(self.config.profile_start_step, sink_size)
                        logger.warning("profile_start_step should divided by sink_size, \
                            set profile_start_step to %s", self.config.profile_start_step)
                    if self.config.profile_stop_step % sink_size != 0:
                        self.config.profile_stop_step += self.config.profile_stop_step % sink_size
                        self.config.profile_stop_step = max(self.config.profile_stop_step, \
                            self.config.profile_start_step + sink_size)
                        logger.warning("profile_stop_step should divided by sink_size, \
                            set profile_stop_step to %s", self.config.profile_stop_step)

                start_profile = self.config.init_start_profile
                profile_communication = self.config.profile_communication

                # Add DynamicProfilerMonitor to replace the existing ProfileMonitor.
                from mindspore.profiler import DynamicProfilerMonitor

                # The cfg_path parameter is the path to the folder where the configuration file is to be shared, which needs to be accessible by all nodes in a cluster scenario
                # The output_path parameter is the path where the dynamic profile data is saved.
                profile_cb = DynamicProfilerMonitor(cfg_path="./dyn_cfg", output_path="./dynprof_data")

                # The original ProfileMonitor is no longer used.
                # profile_cb = ProfileMonitor(
                #     start_step=self.config.profile_start_step,
                #     stop_step=self.config.profile_stop_step,
                #     start_profile=start_profile,
                #     profile_communication=profile_communication,
                #     profile_memory=self.config.profile_memory,
                #     output_path=self.config.profile_output,
                #     config=self.config)
                logger.warning(
                    "Please reduce the data sample size with 'num_samples' in MindSpore data format according to "
                    "https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling_ascend.html.")
                logger.warning("In profiler mode, auto-tune will be turned off.")
                self.config.auto_tune = False
                self.config.profile_cb = profile_cb

    - Step 2: After enabling the profile function in the model's yaml configuration file and pulling up the training, DynamicProfilerMonitor will generate the configuration file profiler_config.json under the specified cfg_path path after pulling up the training, and the user can dynamically edit the configuration file, for example, modify it to the following configuration, which means that DynamicProfilerMonitor will start to collect at the 10th step of training and stop collecting at the 10th step to parse online.

      .. code:: python

          {
             "start_step": 10,
             "stop_step": 10,
             "aicore_metrics": -1,
             "profiler_level": -1,
             "profile_framework": -1,
             "analyse_mode": 0,
             "profile_communication": false,
             "parallel_strategy": true,
             "with_stack": true,
             "data_simplification": false
          }