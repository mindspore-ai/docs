Preparing the Training Script
-----------------------------

There are two ways to collect neural network performance data. You can enable Profiler in either of the following ways.

- Method 1: Enable environment variables

  Before running the network script, configure Profiler configuration items.

  .. code-block:: shell
     export MS_PROFILER_OPTIONS='{"start": true, "output_path": "/XXX", "memory": false, "hccl": false, "aicore_metrics": 0, "l2_cache": false}'

  - `start`(bool): Set to "true" to enable Profiler. Set to "false" to disable performance data collection. Default value: "false".

  - `output_path`(str, optional): Represents the path (absolute path) of the output data. Default valueï¼š "./data".

  - `memory`(bool, optional): Tensor data will be collected. This data is collected when the value is "true". Default value: "false".

  - `hccl`(bool, optional): Indicates whether to collect communication performance data in multi-device training. This data is collected when the value is "true". In single-device training, this parameter is not set correctly. Default value: "false".

  - `aicore_metrics`(int, optional): Set the indicator type of AI Core. Default value: 0.

  - `l2_cache`(bool, optional): Set whether to collect l2 cache data. Default value: "false".

- Method 2: Modify the training script

  Add the MindSpore Profiler interface to the training script.

  - Before training starts, the MindSpore ``Profiler`` object needs to be
    initialized.

    .. note::
      The parameters of Profiler are as follows:
      `Profiler API <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.Profiler.html#mindspore.Profiler>`_ .

  - At the end of the training, ``Profiler.analyse()`` should be called
    to finish profiling and generate the perforamnce analyse results.

  Profiler can control whether performance data collection is turned on or
  off based on step (epoch) with the start_profile parameter. For the data
  sinking mode of graph mode, CANN can only be told to turn on and off
  after each epoch, so for the data sinking mode, it needs to turn on and
  off based on the epoch.

  The code for a normal scenario is as follows:

  .. code:: python

     import numpy as np
     from mindspore import nn
     import mindspore as ms
     import mindspore.dataset as ds

     class Net(nn.Cell):
         def __init__(self):
             super(Net, self).__init__()
             self.fc = nn.Dense(2, 2)

         def construct(self, x):
             return self.fc(x)


     def generator():
         for i in range(2):
             yield (np.ones([2, 2]).astype(np.float32), np.ones([2]).astype(np.int32))


     def train(net):
         optimizer = nn.Momentum(net.trainable_params(), 1, 0.9)
         loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)
         data = ds.GeneratorDataset(generator, ["data", "label"])
         model = ms.Model(net, loss, optimizer)
         model.train(1, data)


     if __name__ == '__main__':
         ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend")

         # Init Profiler
         # Note that the Profiler should be initialized before model.train
         profiler = ms.Profiler(output_path = './profiler_data')

         # Train Model
         net = Net()
         train(net)

         # Profiler end
         profiler.analyse()

  Graph mode:

  - When dataset_sink_mode is set to False, it needs to be enabled based
    on step.

    .. code:: python

        import mindspore as ms
        class StopAtStep(ms.Callback):
            def __init__(self, start_step, stop_step):
                super(StopAtStep, self).__init__()
                self.start_step = start_step
                self.stop_step = stop_step
                self.profiler = ms.Profiler(start_profile=False)
            def step_begin(self, run_context):
                cb_params = run_context.original_args()
                step_num = cb_params.cur_step_num
                if step_num == self.start_step:
                    self.profiler.start()
            def step_end(self, run_context):
                cb_params = run_context.original_args()
                step_num = cb_params.cur_step_num
                if step_num == self.stop_step:
                    self.profiler.stop()
            def end(self, run_context):
                self.profiler.analyse()

  - When dataset_sink_mode is set to True, it needs to be enabled based
    on epoch.

    .. code:: python

        class StopAtEpoch(ms.Callback):
            def __init__(self, start_epoch, stop_epoch):
                super(StopAtEpoch, self).__init__()
                self.start_epoch = start_epoch
                self.stop_epoch = stop_epoch
                self.profiler = ms.Profiler(start_profile=False)
            def epoch_begin(self, run_context):
                cb_params = run_context.original_args()
                epoch_num = cb_params.cur_epoch_num
                if epoch_num == self.start_epoch:
                    self.profiler.start()
            def epoch_end(self, run_context):
                cb_params = run_context.original_args()
                epoch_num = cb_params.cur_epoch_num
                if epoch_num == self.stop_epoch:
                    self.profiler.stop()
            def end(self, run_context):
                self.profiler.analyse()

  Custom training:

  .. code:: python

     profiler = ms.Profiler(start_profile=False)
     data_loader = ds.create_dict_iterator()

     for i, data in enumerate(data_loader):
         train()
         if i==100:
             profiler.start()
         if i==200:
             profiler.stop()

     profiler.analyse()
