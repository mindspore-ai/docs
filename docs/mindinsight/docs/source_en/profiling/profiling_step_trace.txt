Step Trace Analysis
~~~~~~~~~~~~~~~~~~~

| The Step Trace Component is used to show the general performance of
  the stages in the training. Step Trace will divide the training into
  several stages:
| Step Gap (The time between the end of one step and the computation of
  next step), Forward/Backward Propagation, All Reduce and Parameter
  Update. It will show the execution time for each stage, and help to
  find the bottleneck stage quickly.

.. note::
   Step Trace does not support heterogeneous training currently.

.. figure:: ./images/step_trace.png
   :alt: step_trace.png

*Figure:Step Trace Analysis*

Figure above displays the Step Trace page. The Step Trace detail will show
the start/finish time for each stage. By default, it shows the average
time for all the steps. Users can also choose a specific step to see its
step trace statistics.

The graphs at the bottom of the page show the execution time of Step
Interval, Forward/Backward Propagation and Step Tail (The time between
the end of Backward Propagation and the end of Parameter Update) changes
according to different steps, it will help to decide whether we can
optimize the performance of some stages. Here are more details:

- **Step Interval** is the duration for reading data from data queues.
  If this part takes long time, it is advised to check the data
  preparation for further analysis.
- **Forward and Backward Propagation** is the duration for executing
  the forward and backward operations on the network, which handle the
  main calculation work of a step. If this part takes long time, it is
  advised to check the statistics of operators or timeline for further
  analysis.
- **Step Tail** is the duration for performing parameter aggregation
  and update operations in parallel training. If the operation takes
  long time, it is advised to check the statistics of communication
  operators and the status of parallelism.

In order to divide the stages, the Step Trace Component need to figure
out the forward propagation start operator and the backward propagation
end operator. MindSpore will automatically figure out the two operators
to reduce the profiler configuration work. The first operator after
``get_next`` will be selected as the forward start operator and the
operator before the last all reduce will be selected as the backward end
operator. **However, Profiler do not guarantee that the automatically
selected operators will meet the userâ€™s expectation in all cases.**
Users can set the two operators manually as follows:

- Set environment variable ``PROFILING_FP_START`` to configure the
  forward start operator, for example,
  ``export PROFILING_FP_START=fp32_vars/conv2d/BatchNorm``.
- Set environment variable ``PROFILING_BP_END`` to configure the
  backward end operator, for example,
  ``export PROFILING_BP_END=loss_scale/gradients/AddN_70``.
