{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重更新与依赖控制\n",
    "\n",
    "[![](https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source.png)](https://gitee.com/mindspore/docs/blob/r1.5/docs/mindspore/programming_guide/source_zh_cn/parameter.ipynb)&emsp;[![](https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_notebook.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r1.5/programming_guide/zh_cn/mindspore_parameter.ipynb)&emsp;[![](https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_modelarts.png)](https://authoring-modelarts-cnnorth4.huaweicloud.com/console/lab?share-url-b64=aHR0cHM6Ly9vYnMuZHVhbHN0YWNrLmNuLW5vcnRoLTQubXlodWF3ZWljbG91ZC5jb20vbWluZHNwb3JlLXdlYnNpdGUvbm90ZWJvb2svbW9kZWxhcnRzL3Byb2dyYW1taW5nX2d1aWRlL21pbmRzcG9yZV9wYXJhbWV0ZXIuaXB5bmI=&imageid=65f636a0-56cf-49df-b941-7d2a07ba8c8c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "`Parameter`是变量张量，代表在训练网络时，需要被更新的参数。本章主要介绍了`Parameter`的初始化以及属性和方法的使用，同时介绍了`ParameterTuple`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 属性\n",
    "\n",
    "- `inited_param`：返回保存了实际数据的`Parameter`。\n",
    "\n",
    "- `name`：实例化`Parameter`时，为其指定的名字。\n",
    "\n",
    "- `sliced`：用在自动并行场景下，表示`Parameter`里保存的数据是否是分片数据。\n",
    "\n",
    "    如果是，就不再对其进行切分，如果不是，需要根据网络并行策略确认是否对其进行切分。\n",
    "    \n",
    "\n",
    "- `is_init`：`Parameter`的初始化状态。在GE后端，`Parameter`需要一个`init graph`来从主机同步数据到设备侧，该标志表示数据是否已同步到设备。 此标志仅在GE后端起作用，其他后端将被设置为False。\n",
    "\n",
    "- `layerwise_parallel`：`Parameter`是否支持layerwise并行。如果支持，参数就不会进行广播和梯度聚合，反之则需要。\n",
    "\n",
    "- `requires_grad`：是否需要计算参数梯度。如果参数需要被训练，则需要计算参数梯度，否则不需要。\n",
    "\n",
    "- `data`： `Parameter`本身。\n",
    "\n",
    "下例通过`Tensor`初始化一个`Parameter`，获取了`Parameter`的相关属性。如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  x \n",
      " sliced:  False \n",
      " is_init:  False \n",
      " inited_param:  None \n",
      " requires_grad:  True \n",
      " layerwise_parallel:  False \n",
      " data:  Parameter (name=x, shape=(2, 3), dtype=Int64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from mindspore import Tensor, Parameter\n",
    "\n",
    "x = Parameter(default_input=Tensor(np.arange(2*3).reshape((2, 3))), name=\"x\")\n",
    "\n",
    "print(\"name: \", x.name, \"\\n\",\n",
    "      \"sliced: \", x.sliced, \"\\n\",\n",
    "      \"is_init: \", x.is_init, \"\\n\",\n",
    "      \"inited_param: \", x.inited_param, \"\\n\",\n",
    "      \"requires_grad: \", x.requires_grad, \"\\n\",\n",
    "      \"layerwise_parallel: \", x.layerwise_parallel, \"\\n\",\n",
    "      \"data: \", x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法\n",
    "\n",
    "- `init_data`：在网络采用半自动或者全自动并行策略的场景下， 当初始化`Parameter`传入的数据是`Initializer`时，可调用该接口将`Parameter`保存的数据转换为`Tensor`。\n",
    "\n",
    "- `set_data`：设置`Parameter`保存的数据，支持传入`Tensor`、`Initializer`、`int`和`float`进行设置， 将方法的入参`slice_shape`设置为True时，可改变`Parameter`的shape，反之，设置的数据shape必须与`Parameter`原来的shape保持一致。\n",
    "\n",
    "- `set_param_ps`：控制训练参数是否通过[Parameter Server](https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.5/apply_parameter_server_training.html)进行训练。\n",
    "\n",
    "- `clone`：克隆`Parameter`，克隆完成后可以给新Parameter指定新的名字。\n",
    "\n",
    "下例通过`Initializer`来初始化`Tensor`，调用了`Parameter`的相关方法。如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter (name=Parameter, shape=(1, 2, 3), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=x_clone, shape=(1, 2, 3), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=Parameter, shape=(1, 2, 3), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=Parameter, shape=(1, 2, 3), dtype=Float32, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mindspore import Tensor, Parameter\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore.common.initializer import initializer\n",
    "\n",
    "x = Parameter(default_input=initializer('ones', [1, 2, 3], mstype.float32))\n",
    "\n",
    "print(x)\n",
    "x_clone = x.clone()\n",
    "x_clone.name = \"x_clone\"\n",
    "print(x_clone)\n",
    "\n",
    "print(x.init_data())\n",
    "print(x.set_data(data=Tensor(np.arange(2*3).reshape((1, 2, 3)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParameterTuple\n",
    "\n",
    "继承于`tuple`，用于保存多个`Parameter`，通过`__new__(cls, iterable)`传入一个存放`Parameter`的迭代器进行构造，提供`clone`接口进行克隆。\n",
    "\n",
    "下例构造了一个`ParameterTuple`对象，并进行了克隆。如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Parameter (name=x, shape=(2, 3), dtype=Int64, requires_grad=True), Parameter (name=y, shape=(1, 2, 3), dtype=Float32, requires_grad=True), Parameter (name=z, shape=(), dtype=Float32, requires_grad=True)) \n",
      "\n",
      "(Parameter (name=params_copy.x, shape=(2, 3), dtype=Int64, requires_grad=True), Parameter (name=params_copy.y, shape=(1, 2, 3), dtype=Float32, requires_grad=True), Parameter (name=params_copy.z, shape=(), dtype=Float32, requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mindspore import Tensor, Parameter, ParameterTuple\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore.common.initializer import initializer\n",
    "\n",
    "x = Parameter(default_input=Tensor(np.arange(2*3).reshape((2, 3))), name=\"x\")\n",
    "y = Parameter(default_input=initializer('ones', [1, 2, 3], mstype.float32), name='y')\n",
    "z = Parameter(default_input=2.0, name='z')\n",
    "params = ParameterTuple((x, y, z))\n",
    "params_copy = params.clone(\"params_copy\")\n",
    "print(params, \"\\n\")\n",
    "print(params_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 依赖控制\n",
    "如果函数的运行结果依赖或影响外部状态，我们认为该函数具有副作用，比如函数会改变外部全局变量、函数的结果依赖全局变量的值。如果操作符会改变输入参数的值或者操作符的输出依赖全局参数的值，我们认为这是带副作用的操作符。\n",
    "\n",
    "根据内存属性和IO状态，将副作用划分为内存副作用和IO副作用。当前内存副作用主要有Assign、优化器算子等等，IO副作用主要有Print算子。详细可以查看算子定义，内存副作用算子在定义中有side_effect_mem属性，IO副作用算子在定义中有side_effect_io属性。\n",
    "\n",
    "Depend用于处理依赖项操作。\n",
    "在大多数情况下，如果操作符有IO副作用或内存副作用，则将根据用户的语义执行它们，不需要另外使用Depend算子来保证执行顺序。在某些情况下，如果两个运算符A和B没有顺序依赖关系，并且A必须在B之前执行，我们建议使用Depend指定它们的执行顺序。使用方法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "a = A(x)                --->        a = A(x)\n",
    "b = B(y)                --->        y = Depend(y, a)\n",
    "                        --->        b = B(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得说明的是，用于浮点数溢出状态检测的一组特殊算子它们存在隐含副作用，但又不属于IO副作用或内存副作用。此外，使用时还有严格的顺序要求，即：在使用NPUClearFloatStatus算子前需要保证NPUAllocFloatStatus已经执行，使用NPUGetFloatStatus算子前需要保证NPUClearFloatStatus已经执行。因为这些算子使用较少，目前的方案是保持它们的定义为无副作用形式，以Depend确保执行顺序。如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "import mindspore.ops as ops\n",
    "\n",
    "self.alloc_status = ops.operations.NPUAllocFloatStatus()\n",
    "self.get_status = ops.operations.NPUGetFloatStatus()\n",
    "self.clear_status = ops.operations.NPUClearFloatStatus()\n",
    "...\n",
    "init = self.alloc_status()\n",
    "init = ops.functional.Depend(init, input)\n",
    "clear_status = self.clear_status(init)\n",
    "input = ops.functional.Depend(input, clear_status)\n",
    "output = Compute(input)\n",
    "init = ops.functional.Depend(init, output)\n",
    "get_status = self.get_status(init)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体使用可参考溢出检测逻辑中[start_overflow_check函数](https://gitee.com/mindspore/mindspore/blob/r1.5/mindspore/nn/wrap/loss_scale.py)的实现。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
