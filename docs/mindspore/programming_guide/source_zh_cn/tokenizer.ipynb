{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# æ–‡æœ¬å¤„ç†ä¸å¢å¼º\n",
    "\n",
    "`Ascend` `GPU` `CPU` `æ•°æ®å‡†å¤‡`\n",
    "\n",
    "[![](https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source.png)](https://gitee.com/mindspore/docs/blob/r1.5/docs/mindspore/programming_guide/source_zh_cn/tokenizer.ipynb)&emsp;[![](https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_notebook.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r1.5/programming_guide/zh_cn/mindspore_tokenizer.ipynb)&emsp;[![](https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_modelarts.png)](https://authoring-modelarts-cnnorth4.huaweicloud.com/console/lab?share-url-b64=aHR0cHM6Ly9vYnMuZHVhbHN0YWNrLmNuLW5vcnRoLTQubXlodWF3ZWljbG91ZC5jb20vbWluZHNwb3JlLXdlYnNpdGUvbm90ZWJvb2svbW9kZWxhcnRzL3Byb2dyYW1taW5nX2d1aWRlL21pbmRzcG9yZV90b2tlbml6ZXIuaXB5bmI=&imageid=65f636a0-56cf-49df-b941-7d2a07ba8c8c)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## æ¦‚è¿°\n",
    "\n",
    "åˆ†è¯å°±æ˜¯å°†è¿ç»­çš„å­—åºåˆ—æŒ‰ç…§ä¸€å®šçš„è§„èŒƒé‡æ–°ç»„åˆæˆè¯åºåˆ—çš„è¿‡ç¨‹ï¼Œåˆç†çš„è¿›è¡Œåˆ†è¯æœ‰åŠ©äºè¯­ä¹‰çš„ç†è§£ã€‚\n",
    "\n",
    "MindSporeæä¾›äº†å¤šç§ç”¨é€”çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·é«˜æ€§èƒ½åœ°å¤„ç†æ–‡æœ¬ï¼Œç”¨æˆ·å¯ä»¥æ„å»ºè‡ªå·±çš„å­—å…¸ï¼Œä½¿ç”¨é€‚å½“çš„æ ‡è®°å™¨å°†å¥å­æ‹†åˆ†ä¸ºä¸åŒçš„æ ‡è®°ï¼Œå¹¶é€šè¿‡æŸ¥æ‰¾æ“ä½œè·å–å­—å…¸ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚\n",
    "\n",
    "MindSporeç›®å‰æä¾›çš„åˆ†è¯å™¨å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚æ­¤å¤–ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å®ç°è‡ªå®šä¹‰çš„åˆ†è¯å™¨ã€‚\n",
    "\n",
    "| åˆ†è¯å™¨ | åˆ†è¯å™¨è¯´æ˜ |\n",
    "| :-- | :-- |\n",
    "| BasicTokenizer | æ ¹æ®æŒ‡å®šè§„åˆ™å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |\n",
    "| BertTokenizer | ç”¨äºå¤„ç†Bertæ–‡æœ¬æ•°æ®çš„åˆ†è¯å™¨ã€‚ |\n",
    "| JiebaTokenizer | åŸºäºå­—å…¸çš„ä¸­æ–‡å­—ç¬¦ä¸²åˆ†è¯å™¨ã€‚ |\n",
    "| RegexTokenizer | æ ¹æ®æŒ‡å®šæ­£åˆ™è¡¨è¾¾å¼å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |\n",
    "| SentencePieceTokenizer | åŸºäºSentencePieceå¼€æºå·¥å…·åŒ…è¿›è¡Œåˆ†è¯ã€‚ |\n",
    "| UnicodeCharTokenizer | å°†æ ‡é‡æ–‡æœ¬æ•°æ®åˆ†è¯ä¸ºUnicodeå­—ç¬¦ã€‚ |\n",
    "| UnicodeScriptTokenizer | æ ¹æ®Unicodeè¾¹ç•Œå¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |\n",
    "| WhitespaceTokenizer | æ ¹æ®ç©ºæ ¼ç¬¦å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |\n",
    "| WordpieceTokenizer | æ ¹æ®å•è¯é›†å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ |\n",
    "\n",
    "æ›´å¤šåˆ†è¯å™¨çš„è¯¦ç»†è¯´æ˜ï¼Œå¯ä»¥å‚è§[APIæ–‡æ¡£](https://www.mindspore.cn/docs/api/zh-CN/r1.5/api_python/mindspore.dataset.text.html)ã€‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MindSporeåˆ†è¯å™¨\n",
    "\n",
    "ä¸‹é¢ä»‹ç»å‡ ç§å¸¸ç”¨åˆ†è¯å™¨çš„ä½¿ç”¨æ–¹æ³•ã€‚\n",
    "\n",
    "### BertTokenizer\n",
    "\n",
    "`BertTokenizer`æ˜¯é€šè¿‡è°ƒç”¨`BasicTokenizer`å’Œ`WordpieceTokenizer`æ¥è¿›è¡Œåˆ†è¯çš„ã€‚\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†å’Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç„¶åé€šè¿‡`BertTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "input_list = [\"åºŠå‰æ˜æœˆå…‰\", \"ç–‘æ˜¯åœ°ä¸Šéœœ\", \"ä¸¾å¤´æœ›æ˜æœˆ\", \"ä½å¤´æ€æ•…ä¹¡\", \"I am making small mistakes during working hours\",\n",
    "                \"ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»\", \"ç¹é«”å­—\"]\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))\n",
    "\n",
    "vocab_list = [\n",
    "  \"åºŠ\", \"å‰\", \"æ˜\", \"æœˆ\", \"å…‰\", \"ç–‘\", \"æ˜¯\", \"åœ°\", \"ä¸Š\", \"éœœ\", \"ä¸¾\", \"å¤´\", \"æœ›\", \"ä½\", \"æ€\", \"æ•…\", \"ä¹¡\",\n",
    "  \"ç¹\", \"é«”\", \"å­—\", \"å˜¿\", \"å“ˆ\", \"å¤§\", \"ç¬‘\", \"å˜»\", \"i\", \"am\", \"mak\", \"make\", \"small\", \"mistake\",\n",
    "  \"##s\", \"during\", \"work\", \"##ing\", \"hour\", \"ğŸ˜€\", \"ğŸ˜ƒ\", \"ğŸ˜„\", \"ğŸ˜\", \"+\", \"/\", \"-\", \"=\", \"12\",\n",
    "  \"28\", \"40\", \"16\", \" \", \"I\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[PAD]\", \"[MASK]\", \"[unused1]\", \"[unused10]\"]\n",
    "\n",
    "vocab = text.Vocab.from_list(vocab_list)\n",
    "tokenizer_op = text.BertTokenizer(vocab=vocab)\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "åºŠå‰æ˜æœˆå…‰\n",
      "ç–‘æ˜¯åœ°ä¸Šéœœ\n",
      "ä¸¾å¤´æœ›æ˜æœˆ\n",
      "ä½å¤´æ€æ•…ä¹¡\n",
      "I am making small mistakes during working hours\n",
      "ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»\n",
      "ç¹é«”å­—\n",
      "------------------------after tokenization-----------------------------\n",
      "['åºŠ' 'å‰' 'æ˜' 'æœˆ' 'å…‰']\n",
      "['ç–‘' 'æ˜¯' 'åœ°' 'ä¸Š' 'éœœ']\n",
      "['ä¸¾' 'å¤´' 'æœ›' 'æ˜' 'æœˆ']\n",
      "['ä½' 'å¤´' 'æ€' 'æ•…' 'ä¹¡']\n",
      "['I' 'am' 'mak' '##ing' 'small' 'mistake' '##s' 'during' 'work' '##ing'\n",
      " 'hour' '##s']\n",
      "['ğŸ˜€' 'å˜¿' 'å˜¿' 'ğŸ˜ƒ' 'å“ˆ' 'å“ˆ' 'ğŸ˜„' 'å¤§' 'ç¬‘' 'ğŸ˜' 'å˜»' 'å˜»']\n",
      "['ç¹' 'é«”' 'å­—']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### JiebaTokenizer\n",
    "\n",
    "`JiebaTokenizer`æ˜¯åŸºäºjiebaçš„ä¸­æ–‡åˆ†è¯ã€‚\n",
    "\n",
    "ä¸‹è½½å­—å…¸æ–‡ä»¶`hmm_model.utf8`å’Œ`jieba.dict.utf8`ï¼Œå¹¶å°†å…¶æ”¾åˆ°æŒ‡å®šä½ç½®ï¼Œåœ¨Jupyter Notebookä¸­æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ã€‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget -N https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/hmm_model.utf8 --no-check-certificate\n",
    "!wget -N https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/jieba.dict.utf8 --no-check-certificate\n",
    "!mkdir -p ./datasets/tokenizer/\n",
    "!mv hmm_model.utf8 jieba.dict.utf8 -t ./datasets/tokenizer/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ä¸‹è½½çš„æ–‡ä»¶æ”¾ç½®çš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\n",
    "\n",
    "```text\n",
    "./datasets/tokenizer/\n",
    "â”œâ”€â”€ hmm_model.utf8\n",
    "â””â”€â”€ jieba.dict.utf8\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨HMMä¸MPå­—å…¸æ–‡ä»¶åˆ›å»º`JiebaTokenizer`å¯¹è±¡ï¼Œå¹¶å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œæœ€åå±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "input_list = [\"ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§\"]\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))\n",
    "\n",
    "# files from open source repository https://github.com/yanyiwu/cppjieba/tree/master/dict\n",
    "HMM_FILE = \"./datasets/tokenizer/hmm_model.utf8\"\n",
    "MP_FILE = \"./datasets/tokenizer/jieba.dict.utf8\"\n",
    "jieba_op = text.JiebaTokenizer(HMM_FILE, MP_FILE)\n",
    "dataset = dataset.map(operations=jieba_op, input_columns=[\"text\"], num_parallel_workers=1)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§\n",
      "------------------------after tokenization-----------------------------\n",
      "['ä»Šå¤©å¤©æ°”' 'å¤ªå¥½äº†' 'æˆ‘ä»¬' 'ä¸€èµ·' 'å»' 'å¤–é¢' 'ç©å§']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SentencePieceTokenizer\n",
    "\n",
    "`SentencePieceTokenizer`æ˜¯åŸºäº[SentencePiece](https://github.com/google/sentencepiece)è¿™ä¸ªå¼€æºçš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ã€‚\n",
    "\n",
    "ä¸‹è½½æ–‡æœ¬æ•°æ®é›†æ–‡ä»¶`botchan.txt`ï¼Œå¹¶å°†å…¶æ”¾ç½®åˆ°æŒ‡å®šä½ç½®ï¼Œåœ¨Jupyter Notebookä¸­æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ã€‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget -N https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/botchan.txt --no-check-certificate\n",
    "!mkdir -p ./datasets/tokenizer/\n",
    "!mv botchan.txt ./datasets/tokenizer/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ä¸‹è½½çš„æ–‡ä»¶æ”¾ç½®çš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\n",
    "\n",
    "```text\n",
    "./datasets/tokenizer/\n",
    "â””â”€â”€ botchan.txt\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä»`vocab_file`æ–‡ä»¶ä¸­æ„å»ºä¸€ä¸ª`vocab`å¯¹è±¡ï¼Œå†é€šè¿‡`SentencePieceTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "from mindspore.dataset.text import SentencePieceModel, SPieceTokenizerOutType\n",
    "\n",
    "input_list = [\"I saw a girl with a telescope.\"]\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))\n",
    "\n",
    "# file from MindSpore repository https://gitee.com/mindspore/mindspore/blob/r1.5/tests/ut/data/dataset/test_sentencepiece/botchan.txt\n",
    "vocab_file = \"./datasets/tokenizer/botchan.txt\"\n",
    "vocab = text.SentencePieceVocab.from_file([vocab_file], 5000, 0.9995, SentencePieceModel.UNIGRAM, {})\n",
    "tokenizer_op = text.SentencePieceTokenizer(vocab, out_type=SPieceTokenizerOutType.STRING)\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "I saw a girl with a telescope.\n",
      "------------------------after tokenization-----------------------------\n",
      "['â–I' 'â–sa' 'w' 'â–a' 'â–girl' 'â–with' 'â–a' 'â–te' 'les' 'co' 'pe' '.']\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### UnicodeCharTokenizer\n",
    "\n",
    "`UnicodeCharTokenizer`æ˜¯æ ¹æ®Unicodeå­—ç¬¦é›†æ¥åˆ†è¯çš„ã€‚\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åé€šè¿‡`UnicodeCharTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "input_list = [\"Welcome to Beijing!\", \"åŒ—äº¬æ¬¢è¿æ‚¨ï¼\", \"æˆ‘å–œæ¬¢English!\"]\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))\n",
    "\n",
    "tokenizer_op = text.UnicodeCharTokenizer()\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']).tolist())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "Welcome to Beijing!\n",
      "åŒ—äº¬æ¬¢è¿æ‚¨ï¼\n",
      "æˆ‘å–œæ¬¢English!\n",
      "------------------------after tokenization-----------------------------\n",
      "['W', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'B', 'e', 'i', 'j', 'i', 'n', 'g', '!']\n",
      "['åŒ—', 'äº¬', 'æ¬¢', 'è¿', 'æ‚¨', 'ï¼']\n",
      "['æˆ‘', 'å–œ', 'æ¬¢', 'E', 'n', 'g', 'l', 'i', 's', 'h', '!']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WhitespaceTokenizer\n",
    "\n",
    "`WhitespaceTokenizer`æ˜¯æ ¹æ®ç©ºæ ¼æ¥è¿›è¡Œåˆ†è¯çš„ã€‚\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åé€šè¿‡`WhitespaceTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "input_list = [\"Welcome to Beijing!\", \"åŒ—äº¬æ¬¢è¿æ‚¨ï¼\", \"æˆ‘å–œæ¬¢English!\"]\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))\n",
    "\n",
    "tokenizer_op = text.WhitespaceTokenizer()\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']).tolist())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "Welcome to Beijing!\n",
      "åŒ—äº¬æ¬¢è¿æ‚¨ï¼\n",
      "æˆ‘å–œæ¬¢English!\n",
      "------------------------after tokenization-----------------------------\n",
      "['Welcome', 'to', 'Beijing!']\n",
      "['åŒ—äº¬æ¬¢è¿æ‚¨ï¼']\n",
      "['æˆ‘å–œæ¬¢English!']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordpieceTokenizer\n",
    "\n",
    "`WordpieceTokenizer`æ˜¯åŸºäºå•è¯é›†æ¥è¿›è¡Œåˆ’åˆ†çš„ï¼Œåˆ’åˆ†ä¾æ®å¯ä»¥æ˜¯å•è¯é›†ä¸­çš„å•ä¸ªå•è¯ï¼Œæˆ–è€…å¤šä¸ªå•è¯çš„ç»„åˆå½¢å¼ã€‚\n",
    "\n",
    "ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä»å•è¯åˆ—è¡¨ä¸­æ„å»º`vocab`å¯¹è±¡ï¼Œé€šè¿‡`WordpieceTokenizer`å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.text as text\n",
    "\n",
    "input_list = [\"my\", \"favorite\", \"book\", \"is\", \"love\", \"during\", \"the\", \"cholera\", \"era\", \"what\",\n",
    "    \"æˆ‘\", \"æœ€\", \"å–œ\", \"æ¬¢\", \"çš„\", \"ä¹¦\", \"æ˜¯\", \"éœ\", \"ä¹±\", \"æ—¶\", \"æœŸ\", \"çš„\", \"çˆ±\", \"æƒ…\", \"æ‚¨\"]\n",
    "vocab_english = [\"book\", \"cholera\", \"era\", \"favor\", \"##ite\", \"my\", \"is\", \"love\", \"dur\", \"##ing\", \"the\"]\n",
    "vocab_chinese = [\"æˆ‘\", 'æœ€', 'å–œ', 'æ¬¢', 'çš„', 'ä¹¦', 'æ˜¯', 'éœ', 'ä¹±', 'æ—¶', 'æœŸ', 'çˆ±', 'æƒ…']\n",
    "\n",
    "dataset = ds.NumpySlicesDataset(input_list, column_names=[\"text\"], shuffle=False)\n",
    "\n",
    "print(\"------------------------before tokenization----------------------------\")\n",
    "\n",
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))\n",
    "\n",
    "vocab = text.Vocab.from_list(vocab_english+vocab_chinese)\n",
    "tokenizer_op = text.WordpieceTokenizer(vocab=vocab)\n",
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------before tokenization----------------------------\n",
      "my\n",
      "favorite\n",
      "book\n",
      "is\n",
      "love\n",
      "during\n",
      "the\n",
      "cholera\n",
      "era\n",
      "what\n",
      "æˆ‘\n",
      "æœ€\n",
      "å–œ\n",
      "æ¬¢\n",
      "çš„\n",
      "ä¹¦\n",
      "æ˜¯\n",
      "éœ\n",
      "ä¹±\n",
      "æ—¶\n",
      "æœŸ\n",
      "çš„\n",
      "çˆ±\n",
      "æƒ…\n",
      "æ‚¨\n",
      "------------------------after tokenization-----------------------------\n",
      "['my']\n",
      "['favor' '##ite']\n",
      "['book']\n",
      "['is']\n",
      "['love']\n",
      "['dur' '##ing']\n",
      "['the']\n",
      "['cholera']\n",
      "['era']\n",
      "['[UNK]']\n",
      "['æˆ‘']\n",
      "['æœ€']\n",
      "['å–œ']\n",
      "['æ¬¢']\n",
      "['çš„']\n",
      "['ä¹¦']\n",
      "['æ˜¯']\n",
      "['éœ']\n",
      "['ä¹±']\n",
      "['æ—¶']\n",
      "['æœŸ']\n",
      "['çš„']\n",
      "['çˆ±']\n",
      "['æƒ…']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}