{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Parameter\r\n",
    "\r\n",
    "`Ascend` `GPU` `CPU` `Beginner`\r\n",
    "\r\n",
    "[![](https://gitee.com/mindspore/docs/raw/master/resource/_static/logo_source_en.png)](https://gitee.com/mindspore/docs/blob/master/docs/mindspore/programming_guide/source_zh_cn/parameter_introduction.ipynb)&emsp;[![](https://gitee.com/mindspore/docs/raw/master/resource/_static/logo_notebook_en.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/master/programming_guide/zh_cn/mindspore_parameter_introduction.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Parameter` is a variable tensor, indicating the parameters that need to be updated during network training. Typically, it consists of `weight` and `bias`. \r\n",
    "\r\n",
    "For example, the `weight` and `bias` for `nn.Conv2d` is a tensor with the shape of `[out_channel, in_channel, kernel_size, kernel_size]` and a scalar with the shape of `[out_channel]`, respectively. For `nn.Dense`, the shapes are `[out_channel, in_channel]` and `[out_channel]`.\r\n",
    "\r\n",
    "```python\r\n",
    "import numpy as np\r\n",
    "from mindspore import Tensor\r\n",
    "from mindspore import nn\r\n",
    "\r\n",
    "conv_net = nn.Conv2d(in_channels=120, out_channels=240, kernel_size=4, has_bias=True, weight_init='normal')\r\n",
    "dense_net = nn.Dense(in_channels=3, out_channels=4, weight_init='normal', bias_init='zeros', has_bias=True)\r\n",
    "\r\n",
    "print(\"conv_net, weight shape:\", conv_net.weight.shape, \", bias shape:\", conv_net.bias.shape)\r\n",
    "print(\"dense_net, weight_shapeï¼š\", dense_net.weight.shape, \", bias shape:\", dense_net.bias.shape)\r\n",
    "```\r\n",
    "\r\n",
    "```python\r\n",
    "conv_net, weight shape: (240, 120, 4, 4) , bias shape:  (240,)\r\n",
    "dense_net, weight_shape: (4, 3) , bias shape:  (4,)\r\n",
    "```\r\n",
    "\r\n",
    "Usually, weight and bias are integrated in a `nn` operator, users can specify a kind of initialization method to initialize these parameters.\r\n",
    "\r\n",
    "During training, the optimizer will continually update the parameters according to the backward propagating gradients. The optimal parameters will be finally saved for evaluation usage. \r\n",
    "\r\n",
    "`Parameter` does not support sparse Tensor, such as [mindspore.RowTensor](https://www.mindspore.cn/docs/api/en/master/api_python/mindspore/mindspore.RowTensor.html) and [mindspore.SparseTensor](https://www.mindspore.cn/docs/api/en/master/api_python/mindspore/mindspore.SparseTensor.html). More information about `Parameter` initializing and updating can refer to [Parameter initialize](https://www.mindspore.cn/docs/programming_guide/en/master/initializer.html) and [Parameter update](https://www.mindspore.cn/docs/programming_guide/en/master/parameter.html).\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}