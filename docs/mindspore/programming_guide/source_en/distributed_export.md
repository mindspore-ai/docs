# Distributed Export MindIR File With Multi Devices

When the super-large-scale neural network model has too many parameters, the MindIR format model cannot be completely loaded into a single card for reasoning. Multi-card should be used for distributed reasoning. In this case, multiple MindIRs need to be exported before the reasoning task. file.
For Multi-card training and distributed reasoning, it is necessary to export MindIR files in a distributed manner. The specific methods are as follows:

First, you need to prepare checkpoint files and training strategy files.

The checkpoint file is generated during the training process. For specific usage of checkpoint, please refer to: [checkpoint usage](https://www.mindspore.cn/docs/programming_guide/en/r1.3/save_model.html#checkpoint).

The training strategy file needs to be generated by setting the context during training. The context configuration items are as follows:
`context.set_auto_parallel_context(strategy_ckpt_save_file='train_strategy.ckpt')`

In this way, after training, a training strategy file named `train_strategy.ckpt` will be generated in the set directory.

Before exporting the MindIR file, it is necessary to load the checkpoint file, the distributed training checkpoint file, the training strategy and the reasoning strategy need to be combined, so the reasoning strategy file can be generated.
The code to generate the inference strategy is as follows:
`predict_strategy = model.infer_predict_layout(predict_data)`

Then, use the method of loading distributed checkpoints to load the previously trained parameters to the network.
code show as below:
`load_distributed_checkpoint(model, ckpt_file_list, predict_strategy)`

For the specific usage of `load_distributed_checkpoint`, please refer to: [Distributed Inference](https://www.mindspore.cn/docs/programming_guide/en/r1.3/multi_platform_inference_ascend_910.html#distributed-inference-with-multi-devices).

Finally, you can export the MindIR file in the distributed reasoning scenario.

The core code is as follows:

```python
# Configure the strategy file generated during the training process in the context
context.set_auto_parallel_context(strategy_ckpt_load_file='train_strategy.ckpt')
# Define network structure
network = Net()
model = Model(network)
# Get the reasoning strategy file
predict_strategy = model.infer_predict_layout(predict_data)
# Create checkpoint list
ckpt_file_list = create_ckpt_file_list()
# Load distributed parameters
load_distributed_checkpoint(model, ckpt_file_list, predict_strategy)
# Export distributed MindIR file
export(net, Tensor(input), file_name='net', file_format='MINDIR')
```

In the case of multi-card training and single-card inference, the usage of exporting MindIR is the same as that of single machine. For the usage of loading checkpoint, please refer to: [Distributed Inference](https://www.mindspore.cn/docs/programming_guide/en/r1.3/multi_platform_inference_ascend_910.html#ascend-910-ai).

> Distributed scene export MindIR file sample code:
>
> <https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/distributed_export>
