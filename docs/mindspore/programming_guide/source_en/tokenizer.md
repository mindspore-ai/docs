# Text Data Processing and Enhancement

`Ascend` `GPU` `CPU` `Data Preparation`

<!-- TOC -->

- [Text Data Processing and Enhancement](#text-data-processing-and-enhancement)
    - [Overview](#overview)
    - [MindSpore Tokenizers](#mindspore-tokenizers)
        - [BertTokenizer](#berttokenizer)
        - [JiebaTokenizer](#jiebatokenizer)
        - [SentencePieceTokenizer](#sentencepiecetokenizer)
        - [UnicodeCharTokenizer](#unicodechartokenizer)
        - [WhitespaceTokenizer](#whitespacetokenizer)
        - [WordpieceTokenizer](#wordpiecetokenizer)

<!-- /TOC -->

<a href="https://gitee.com/mindspore/docs/blob/master/docs/mindspore/programming_guide/source_en/tokenizer.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/master/resource/_static/logo_source_en.png"></a>

## Overview

Tokenization is a process of re-combining continuous character sequences into word sequences according to certain specifications. Reasonable tokenization is helpful for semantic comprehension.

MindSpore provides a tokenizer for multiple purposes to help you process text with high performance. You can build your own dictionaries, use appropriate tokenizers to split sentences into different tokens, and search for indexes of the tokens in the dictionaries.

MindSpore provides the following tokenizers. In addition, you can customize tokenizers as required.

| Tokenizer | Description |
| -- | -- |
| BasicTokenizer | Performs tokenization on scalar text data based on specified rules.  |
| BertTokenizer | Processes BERT text data.  |
| JiebaTokenizer | Dictionary-based Chinese character string tokenizer.  |
| RegexTokenizer | Performs tokenization on scalar text data based on a specified regular expression.  |
| SentencePieceTokenizer | Performs tokenization based on the open-source tool package SentencePiece.  |
| UnicodeCharTokenizer | Tokenizes scalar text data into Unicode characters.  |
| UnicodeScriptTokenizer | Performs tokenization on scalar text data based on Unicode boundaries.  |
| WhitespaceTokenizer | Performs tokenization on scalar text data based on spaces.  |
| WordpieceTokenizer | Performs tokenization on scalar text data based on the word set.  |

For details about tokenizers, see [MindSpore API](https://www.mindspore.cn/docs/api/en/master/api_python/mindspore.dataset.text.html).

## MindSpore Tokenizers

The following describes how to use common tokenizers.

### BertTokenizer

`BertTokenizer` performs tokenization by calling `BasicTokenizer` and `WordpieceTokenizer`.

The following example builds a text dataset and a character string list, uses `BertTokenizer` to perform tokenization on the dataset, and displays the text results before and after tokenization.

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["åºŠå‰æ˜æœˆå…‰", "ç–‘æ˜¯åœ°ä¸Šéœœ", "ä¸¾å¤´æœ›æ˜æœˆ", "ä½å¤´æ€æ•…ä¹¡", "I am making small mistakes during working hours",
                "ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»", "ç¹é«”å­—"]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

vocab_list = [
  "åºŠ", "å‰", "æ˜", "æœˆ", "å…‰", "ç–‘", "æ˜¯", "åœ°", "ä¸Š", "éœœ", "ä¸¾", "å¤´", "æœ›", "ä½", "æ€", "æ•…", "ä¹¡",
  "ç¹", "é«”", "å­—", "å˜¿", "å“ˆ", "å¤§", "ç¬‘", "å˜»", "i", "am", "mak", "make", "small", "mistake",
  "##s", "during", "work", "##ing", "hour", "ğŸ˜€", "ğŸ˜ƒ", "ğŸ˜„", "ğŸ˜", "+", "/", "-", "=", "12",
  "28", "40", "16", " ", "I", "[CLS]", "[SEP]", "[UNK]", "[PAD]", "[MASK]", "[unused1]", "[unused10]"]

vocab = text.Vocab.from_list(vocab_list)
tokenizer_op = text.BertTokenizer(vocab=vocab)
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']))
```

The output is as follows:

```text
------------------------before tokenization----------------------------
åºŠå‰æ˜æœˆå…‰
ç–‘æ˜¯åœ°ä¸Šéœœ
ä¸¾å¤´æœ›æ˜æœˆ
ä½å¤´æ€æ•…ä¹¡
I am making small mistakes during working hours
ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»
ç¹é«”å­—
------------------------after tokenization-----------------------------
['åºŠ' 'å‰' 'æ˜' 'æœˆ' 'å…‰']
['ç–‘' 'æ˜¯' 'åœ°' 'ä¸Š' 'éœœ']
['ä¸¾' 'å¤´' 'æœ›' 'æ˜' 'æœˆ']
['ä½' 'å¤´' 'æ€' 'æ•…' 'ä¹¡']
['I' 'am' 'mak' '##ing' 'small' 'mistake' '##s' 'during' 'work' '##ing'
 'hour' '##s']
['ğŸ˜€' 'å˜¿' 'å˜¿' 'ğŸ˜ƒ' 'å“ˆ' 'å“ˆ' 'ğŸ˜„' 'å¤§' 'ç¬‘' 'ğŸ˜' 'å˜»' 'å˜»']
['ç¹' 'é«”' 'å­—']
```

### JiebaTokenizer

`JiebaTokenizer` performs Chinese tokenization based on Jieba.

Download the dictionary files `hmm_model.utf8` and `jieba.dict.utf8` and put them in the specified location.

```python
import os
import requests

def download_dataset(dataset_url, path):
    filename = dataset_url.split("/")[-1]
    if not os.path.exists(path):
        os.makedirs(path)
    res = requests.get(dataset_url, stream=True)
    save_path = os.path.join(path, filename)
    with open(save_path, "wb") as f:
        for chunk in res.iter_content(chunk_size=512):
            if chunk:
                f.write(chunk)

download_dataset("https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/hmm_model.utf8", "./datasets/tokenizer/")
download_dataset("https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/jieba.dict.utf8", "./datasets/tokenizer/")
```

```text
./datasets/tokenizer/
â”œâ”€â”€ hmm_model.utf8
â””â”€â”€ jieba.dict.utf8

0 directories, 2 files
```

The following example builds a text dataset, uses the HMM and MP dictionary files to create a `JiebaTokenizer` object, performs tokenization on the dataset, and displays the text results before and after tokenization.

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§"]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

# files from open source repository https://github.com/yanyiwu/cppjieba/tree/master/dict
HMM_FILE = "./datasets/tokenizer/hmm_model.utf8"
MP_FILE = "./datasets/tokenizer/jieba.dict.utf8"
jieba_op = text.JiebaTokenizer(HMM_FILE, MP_FILE)
dataset = dataset.map(operations=jieba_op, input_columns=["text"], num_parallel_workers=1)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']))
```

The output is as follows:

```text
------------------------before tokenization----------------------------
ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§
------------------------after tokenization-----------------------------
['ä»Šå¤©å¤©æ°”' 'å¤ªå¥½äº†' 'æˆ‘ä»¬' 'ä¸€èµ·' 'å»' 'å¤–é¢' 'ç©å§']
```

### SentencePieceTokenizer

`SentencePieceTokenizer` performs tokenization based on an open-source natural language processing tool package [SentencePiece](https://github.com/google/sentencepiece).

Download the text dataset file `botchan.txt` and place it in the specified location.

```python
download_dataset("https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/botchan.txt", "./datasets/tokenizer/")
```

```text
./datasets/tokenizer/
â””â”€â”€ botchan.txt

0 directories, 1 files
```

The following example builds a text dataset, creates a `vocab` object from the `vocab_file` file, uses `SentencePieceTokenizer` to perform tokenization on the dataset, and displays the text results before and after tokenization.

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text
from mindspore.dataset.text import SentencePieceModel, SPieceTokenizerOutType

input_list = ["I saw a girl with a telescope."]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

# file from MindSpore repository https://gitee.com/mindspore/mindspore/blob/master/tests/ut/data/dataset/test_sentencepiece/botchan.txt
vocab_file = "./datasets/tokenizer/botchan.txt"
vocab = text.SentencePieceVocab.from_file([vocab_file], 5000, 0.9995, SentencePieceModel.UNIGRAM, {})
tokenizer_op = text.SentencePieceTokenizer(vocab, out_type=SPieceTokenizerOutType.STRING)
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']))
```

The output is as follows:

```text
------------------------before tokenization----------------------------
I saw a girl with a telescope.
------------------------after tokenization-----------------------------
['â–I' 'â–sa' 'w' 'â–a' 'â–girl' 'â–with' 'â–a' 'â–te' 'les' 'co' 'pe' '.']
```

### UnicodeCharTokenizer

`UnicodeCharTokenizer` performs tokenization based on the Unicode character set.

The following example builds a text dataset, uses `UnicodeCharTokenizer` to perform tokenization on the dataset, and displays the text results before and after tokenization.

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["Welcome to Beijing!", "åŒ—äº¬æ¬¢è¿æ‚¨ï¼", "æˆ‘å–œæ¬¢English!"]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

tokenizer_op = text.UnicodeCharTokenizer()
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']).tolist())
```

The output is as follows:

```text
------------------------before tokenization----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenization-----------------------------
['W', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'B', 'e', 'i', 'j', 'i', 'n', 'g', '!']
['åŒ—', 'äº¬', 'æ¬¢', 'è¿', 'æ‚¨', 'ï¼']
['æˆ‘', 'å–œ', 'æ¬¢', 'E', 'n', 'g', 'l', 'i', 's', 'h', '!']
```

### WhitespaceTokenizer

`WhitespaceTokenizer` performs tokenization based on spaces.

The following example builds a text dataset, uses `WhitespaceTokenizer` to perform tokenization on the dataset, and displays the text results before and after tokenization.

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["Welcome to Beijing!", "åŒ—äº¬æ¬¢è¿æ‚¨ï¼", "æˆ‘å–œæ¬¢English!"]
dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

tokenizer_op = text.WhitespaceTokenizer()
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']).tolist())
```

The output is as follows:

```text
------------------------before tokenization----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenization-----------------------------
['Welcome', 'to', 'Beijing!']
['åŒ—äº¬æ¬¢è¿æ‚¨ï¼']
['æˆ‘å–œæ¬¢English!']
```

### WordpieceTokenizer

`WordpieceTokenizer` performs tokenization based on the word set. A token can be a single word in the word set or a combination of words.

The following example builds a text dataset, creates a `vocab` object from the word list, uses `WordpieceTokenizer` to perform tokenization on the dataset, and displays the text results before and after tokenization.

```python
import mindspore.dataset as ds
import mindspore.dataset.text as text

input_list = ["my", "favorite", "book", "is", "love", "during", "the", "cholera", "era", "what",
    "æˆ‘", "æœ€", "å–œ", "æ¬¢", "çš„", "ä¹¦", "æ˜¯", "éœ", "ä¹±", "æ—¶", "æœŸ", "çš„", "çˆ±", "æƒ…", "æ‚¨"]
vocab_english = ["book", "cholera", "era", "favor", "##ite", "my", "is", "love", "dur", "##ing", "the"]
vocab_chinese = ["æˆ‘", 'æœ€', 'å–œ', 'æ¬¢', 'çš„', 'ä¹¦', 'æ˜¯', 'éœ', 'ä¹±', 'æ—¶', 'æœŸ', 'çˆ±', 'æƒ…']

dataset = ds.NumpySlicesDataset(input_list, column_names=["text"], shuffle=False)

print("------------------------before tokenization----------------------------")

for data in dataset.create_dict_iterator(output_numpy=True):
    print(text.to_str(data['text']))

vocab = text.Vocab.from_list(vocab_english+vocab_chinese)
tokenizer_op = text.WordpieceTokenizer(vocab=vocab)
dataset = dataset.map(operations=tokenizer_op)

print("------------------------after tokenization-----------------------------")

for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):
    print(text.to_str(i['text']))
```

The output is as follows:

```text
------------------------before tokenization----------------------------
my
favorite
book
is
love
during
the
cholera
era
what
æˆ‘
æœ€
å–œ
æ¬¢
çš„
ä¹¦
æ˜¯
éœ
ä¹±
æ—¶
æœŸ
çš„
çˆ±
æƒ…
æ‚¨
------------------------after tokenization-----------------------------
['my']
['favor' '##ite']
['book']
['is']
['love']
['dur' '##ing']
['the']
['cholera']
['era']
['[UNK]']
['æˆ‘']
['æœ€']
['å–œ']
['æ¬¢']
['çš„']
['ä¹¦']
['æ˜¯']
['éœ']
['ä¹±']
['æ—¶']
['æœŸ']
['çš„']
['çˆ±']
['æƒ…']
['[UNK]']
```
