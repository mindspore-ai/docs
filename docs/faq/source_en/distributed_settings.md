# Distributed Settings

`Ascend` `GPU` `Distributed Training` `Beginner` `Intermediate` `Expert`

<a href="https://gitee.com/mindspore/docs/blob/master/docs/faq/source_en/distributed_settings.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/master/resource/_static/logo_source.png"></a>

<font size=3>**Q：The communication profile file needs to be configured on the Ascend environment, how should it be configured?**</font>

A：Please refer to the [Configuring Distributed Environment Variables](https://mindspore.cn/tutorial/training/en/master/advanced_use/distributed_training_ascend.html#configuring-distributed-environment-variables) section of Ascend-based distributed training in the MindSpore tutorial.

<br/>

<font size=3>**Q：How to perform distributed multi-machine multi-card training?**</font>

A：For Ascend environment, please refer to the [Multi-machine Training](https://mindspore.cn/tutorial/training/en/master/advanced_use/distributed_training_ascend.html#multi-machine-training) section of the MindSpore tutorial "distributed_training_ascend".
For GPU-based environments, please refer to the [Run Multi-Host Script](https://mindspore.cn/tutorial/training/en/master/advanced_use/distributed_training_gpu.html#running-the-multi-host-script) section of the MindSpore tutorial "distributed_training_gpu".

<br/>


